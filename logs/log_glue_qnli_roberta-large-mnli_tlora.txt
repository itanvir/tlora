Running experiment with data: glue_qnli, model: FacebookAI/roberta-large-mnli, LoRA type: tlora
RobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 1024, padding_idx=1)
      (position_embeddings): Embedding(514, 1024, padding_idx=1)
      (token_type_embeddings): Embedding(1, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-23): 24 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSdpaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHead(
    (dense): Linear(in_features=1024, out_features=1024, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=1024, out_features=3, bias=True)
  )
)
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        51,471,360
│    │    └─Embedding: 3-2                                        526,336
│    │    └─Embedding: 3-3                                        1,024
│    │    └─LayerNorm: 3-4                                        2,048
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       302,309,376
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                1,049,600
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                3,075
==========================================================================================
Total params: 355,362,819
Trainable params: 355,362,819
Non-trainable params: 0
==========================================================================================
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        51,471,360
│    │    └─Embedding: 3-2                                        526,336
│    │    └─Embedding: 3-3                                        1,024
│    │    └─LayerNorm: 3-4                                        2,048
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       302,309,376
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                1,049,600
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                3,075
==========================================================================================
Total params: 355,362,819
Trainable params: 355,362,819
Non-trainable params: 0
==========================================================================================
RobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 1024, padding_idx=1)
      (position_embeddings): Embedding(514, 1024, padding_idx=1)
      (token_type_embeddings): Embedding(1, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-23): 24 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSdpaSelfAttention(
              (query): TLoRALayer(
                (linear): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.5, inplace=False)
              )
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): TLoRALayer(
                (linear): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.5, inplace=False)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHead(
    (dense): Linear(in_features=1024, out_features=1024, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=1024, out_features=3, bias=True)
  )
)
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        (51,471,360)
│    │    └─Embedding: 3-2                                        (526,336)
│    │    └─Embedding: 3-3                                        (1,024)
│    │    └─LayerNorm: 3-4                                        (2,048)
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       305,504,304
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                (1,049,600)
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                (3,075)
==========================================================================================
Total params: 358,557,747
Trainable params: 49,200
Non-trainable params: 358,508,547
==========================================================================================
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        (51,471,360)
│    │    └─Embedding: 3-2                                        (526,336)
│    │    └─Embedding: 3-3                                        (1,024)
│    │    └─LayerNorm: 3-4                                        (2,048)
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       305,504,304
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                (1,049,600)
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                (3,075)
==========================================================================================
Total params: 358,557,747
Trainable params: 49,200
Non-trainable params: 358,508,547
==========================================================================================
Epoch 1/30
Batch 654/3274 - Loss: 0.6575 - Accuracy: 0.6250
Batch 1308/3274 - Loss: 0.4998 - Accuracy: 0.7188
Batch 1962/3274 - Loss: 0.3404 - Accuracy: 0.8750
Batch 2616/3274 - Loss: 0.3439 - Accuracy: 0.9062
Batch 3270/3274 - Loss: 0.3766 - Accuracy: 0.8438
Scalings = [(0.46, 0.51, 1.04, 1.06), (0.44, 0.54, 1.01, 1.05), (0.52, 0.55, 1.04, 1.06), (0.62, 0.57, 1.09, 1.06), (0.67, 0.55, 1.11, 1.03), (0.65, 0.55, 1.07, 1.03), (0.88, 0.54, 1.17, 1.03), (0.88, 0.72, 1.15, 1.05), (0.88, 0.78, 1.23, 1.04), (0.97, 0.88, 1.24, 1.09), (1.18, 0.99, 1.28, 1.09), (1.14, 0.95, 1.24, 1.07), (1.22, 1.16, 1.24, 1.14), (1.45, 1.24, 1.31, 1.16), (1.57, 1.19, 1.34, 1.13), (1.76, 1.39, 1.36, 1.18), (1.59, 1.43, 1.25, 1.13), (1.98, 1.49, 1.34, 1.14), (1.61, 1.42, 1.22, 1.14), (1.7, 1.37, 1.37, 1.15), (1.52, 1.47, 1.31, 1.18), (1.38, 1.22, 1.35, 1.15), (1.81, 1.3, 1.39, 1.13), (1.3, 1.03, 1.27, 1.09)]
Epoch 1 - Train Loss: 0.6373 - Train Acc: 0.7153 - Val Loss: 0.3633 - Val Acc: 0.8612 - Val MCC: 0.74
Epoch 2/30
Batch 654/3274 - Loss: 0.5914 - Accuracy: 0.7812
Batch 1308/3274 - Loss: 0.3661 - Accuracy: 0.8750
Batch 1962/3274 - Loss: 0.4688 - Accuracy: 0.7500
Batch 2616/3274 - Loss: 0.2563 - Accuracy: 0.9062
Batch 3270/3274 - Loss: 0.3752 - Accuracy: 0.8438
Scalings = [(1.46, 1.54, 1.01, 1.07), (1.37, 1.15, 0.94, 0.88), (1.44, 1.19, 1.03, 0.86), (1.5, 1.08, 1.09, 0.79), (1.62, 1.2, 1.11, 0.87), (1.67, 1.03, 1.08, 0.72), (1.64, 1.21, 1.12, 0.73), (1.79, 1.47, 1.1, 0.85), (1.78, 1.74, 1.19, 0.87), (1.98, 1.71, 1.26, 0.94), (2.12, 1.86, 1.29, 0.95), (2.21, 1.77, 1.26, 0.9), (2.02, 2.18, 1.18, 1.1), (2.67, 2.23, 1.4, 1.12), (2.71, 2.23, 1.4, 1.06), (2.94, 2.64, 1.46, 1.26), (2.79, 2.57, 1.36, 1.15), (2.98, 2.8, 1.39, 1.27), (3.21, 2.9, 1.38, 1.26), (3.27, 2.59, 1.61, 1.23), (2.66, 3.23, 1.42, 1.32), (2.96, 2.76, 1.66, 1.31), (3.24, 3.12, 1.57, 1.46), (2.34, 2.5, 1.32, 1.18)]
Epoch 2 - Train Loss: 0.3601 - Train Acc: 0.8514 - Val Loss: 0.2839 - Val Acc: 0.8946 - Val MCC: 0.79
Epoch 3/30
Batch 654/3274 - Loss: 0.3446 - Accuracy: 0.8125
Batch 1308/3274 - Loss: 0.3508 - Accuracy: 0.8438
Batch 1962/3274 - Loss: 0.3261 - Accuracy: 0.8438
Batch 2616/3274 - Loss: 0.3364 - Accuracy: 0.8125
Batch 3270/3274 - Loss: 0.3821 - Accuracy: 0.8438
Scalings = [(2.22, 2.43, 0.99, 1.12), (2.11, 1.63, 0.91, 0.66), (2.11, 1.74, 0.97, 0.64), (2.17, 1.68, 1.04, 0.63), (2.01, 1.57, 0.94, 0.58), (2.21, 1.56, 0.98, 0.46), (2.36, 1.77, 1.11, 0.46), (2.46, 2.02, 1.02, 0.57), (2.52, 2.61, 1.14, 0.74), (2.76, 2.27, 1.26, 0.76), (2.72, 2.44, 1.22, 0.75), (2.97, 2.45, 1.25, 0.72), (2.77, 2.79, 1.12, 0.97), (3.44, 2.83, 1.41, 1.0), (3.61, 3.04, 1.45, 0.95), (3.83, 3.52, 1.55, 1.29), (3.73, 3.36, 1.44, 1.14), (3.65, 3.77, 1.37, 1.38), (4.39, 4.04, 1.53, 1.4), (3.98, 3.28, 1.62, 1.23), (3.58, 4.25, 1.53, 1.37), (4.01, 3.93, 1.88, 1.46), (3.68, 4.1, 1.52, 1.56), (3.35, 3.56, 1.37, 1.28)]
Epoch 3 - Train Loss: 0.3268 - Train Acc: 0.8664 - Val Loss: 0.2746 - Val Acc: 0.8984 - Val MCC: 0.80
Epoch 4/30
Batch 654/3274 - Loss: 0.1725 - Accuracy: 0.9375
Batch 1308/3274 - Loss: 0.3858 - Accuracy: 0.8750
Batch 1962/3274 - Loss: 0.4284 - Accuracy: 0.8125
Batch 2616/3274 - Loss: 0.2902 - Accuracy: 0.8438
Batch 3270/3274 - Loss: 0.2373 - Accuracy: 0.9062
Scalings = [(2.74, 2.69, 1.01, 1.01), (2.54, 1.97, 0.86, 0.54), (2.57, 2.14, 0.93, 0.56), (2.44, 2.06, 0.91, 0.53), (2.64, 2.01, 0.97, 0.48), (2.68, 1.98, 0.97, 0.31), (2.83, 2.34, 1.06, 0.36), (2.86, 2.5, 0.94, 0.44), (3.04, 3.26, 1.12, 0.64), (3.22, 2.66, 1.2, 0.61), (3.2, 2.99, 1.19, 0.66), (3.58, 2.96, 1.29, 0.59), (3.21, 3.31, 1.04, 0.88), (3.88, 3.28, 1.39, 0.91), (4.18, 3.55, 1.48, 0.85), (4.36, 4.11, 1.56, 1.32), (4.22, 3.89, 1.45, 1.11), (4.31, 4.32, 1.4, 1.39), (5.01, 4.72, 1.61, 1.48), (4.55, 3.9, 1.64, 1.25), (4.23, 4.88, 1.56, 1.41), (4.66, 4.74, 2.01, 1.59), (3.96, 4.67, 1.45, 1.61), (3.56, 4.19, 1.27, 1.32)]
Epoch 4 - Train Loss: 0.3089 - Train Acc: 0.8763 - Val Loss: 0.2704 - Val Acc: 0.9001 - Val MCC: 0.81
Epoch 5/30
Batch 654/3274 - Loss: 0.2072 - Accuracy: 0.9375
Batch 1308/3274 - Loss: 0.4537 - Accuracy: 0.7812
Batch 1962/3274 - Loss: 0.1866 - Accuracy: 0.9688
Batch 2616/3274 - Loss: 0.3774 - Accuracy: 0.7188
Batch 3270/3274 - Loss: 0.2687 - Accuracy: 0.9062
Scalings = [(3.13, 3.08, 1.01, 1.0), (2.91, 2.08, 0.84, 0.39), (2.74, 2.43, 0.83, 0.46), (2.76, 2.21, 0.84, 0.39), (2.91, 2.41, 0.9, 0.42), (3.11, 2.32, 0.97, 0.2), (3.05, 2.69, 0.97, 0.23), (3.19, 3.02, 0.89, 0.38), (3.43, 3.84, 1.1, 0.64), (3.61, 3.08, 1.19, 0.54), (3.49, 3.37, 1.12, 0.57), (4.08, 3.4, 1.33, 0.5), (3.63, 3.62, 1.02, 0.79), (4.25, 3.6, 1.4, 0.81), (4.47, 4.0, 1.45, 0.8), (4.7, 4.45, 1.56, 1.26), (4.64, 4.29, 1.49, 1.09), (4.79, 4.79, 1.43, 1.41), (5.4, 5.04, 1.66, 1.5), (4.95, 4.25, 1.64, 1.23), (4.73, 5.22, 1.59, 1.41), (5.09, 5.07, 2.1, 1.62), (4.31, 5.04, 1.43, 1.63), (3.81, 4.76, 1.23, 1.36)]
Epoch 5 - Train Loss: 0.3007 - Train Acc: 0.8792 - Val Loss: 0.2606 - Val Acc: 0.9037 - Val MCC: 0.81
Epoch 6/30
Batch 654/3274 - Loss: 0.2119 - Accuracy: 0.8750
Batch 1308/3274 - Loss: 0.3567 - Accuracy: 0.8438
Batch 1962/3274 - Loss: 0.2690 - Accuracy: 0.9062
Batch 2616/3274 - Loss: 0.2364 - Accuracy: 0.8750
Batch 3270/3274 - Loss: 0.4986 - Accuracy: 0.8438
Scalings = [(3.33, 3.39, 0.92, 0.98), (3.17, 2.27, 0.78, 0.25), (2.99, 2.9, 0.77, 0.48), (2.8, 2.55, 0.66, 0.33), (3.09, 2.65, 0.81, 0.32), (3.42, 2.7, 0.92, 0.17), (3.39, 3.15, 0.96, 0.2), (3.56, 3.42, 0.88, 0.32), (3.7, 4.27, 1.02, 0.61), (3.8, 3.46, 1.09, 0.5), (3.82, 3.74, 1.13, 0.53), (4.4, 3.74, 1.35, 0.44), (3.95, 3.92, 0.98, 0.73), (4.62, 3.94, 1.43, 0.76), (4.72, 4.32, 1.42, 0.73), (5.0, 4.75, 1.57, 1.24), (4.99, 4.59, 1.54, 1.07), (5.04, 5.05, 1.41, 1.4), (5.78, 5.29, 1.72, 1.53), (5.29, 4.62, 1.67, 1.26), (4.94, 5.56, 1.54, 1.42), (5.23, 5.34, 2.06, 1.62), (4.68, 5.37, 1.43, 1.66), (4.03, 5.04, 1.17, 1.36)]
Epoch 6 - Train Loss: 0.2937 - Train Acc: 0.8826 - Val Loss: 0.2205 - Val Acc: 0.9187 - Val MCC: 0.84
Epoch 7/30
Batch 654/3274 - Loss: 0.1330 - Accuracy: 1.0000
Batch 1308/3274 - Loss: 0.3330 - Accuracy: 0.8438
Batch 1962/3274 - Loss: 0.2484 - Accuracy: 0.9375
Batch 2616/3274 - Loss: 0.1974 - Accuracy: 0.9375
Batch 3270/3274 - Loss: 0.3325 - Accuracy: 0.8125
Scalings = [(3.39, 3.51, 0.79, 0.89), (3.26, 2.68, 0.71, 0.3), (3.21, 3.08, 0.76, 0.42), (3.16, 2.86, 0.67, 0.28), (3.42, 2.96, 0.82, 0.29), (3.79, 3.1, 0.95, 0.14), (3.67, 3.59, 0.95, 0.18), (3.84, 3.77, 0.89, 0.31), (3.99, 4.63, 1.03, 0.62), (4.1, 3.74, 1.1, 0.46), (4.04, 4.0, 1.08, 0.47), (4.61, 4.12, 1.32, 0.42), (4.26, 4.22, 1.0, 0.7), (4.78, 4.26, 1.4, 0.76), (4.99, 4.7, 1.44, 0.75), (5.22, 5.0, 1.59, 1.23), (5.26, 4.85, 1.57, 1.04), (5.35, 5.29, 1.46, 1.39), (6.06, 5.51, 1.75, 1.57), (5.56, 4.98, 1.69, 1.31), (5.22, 5.75, 1.54, 1.42), (5.46, 5.56, 2.1, 1.64), (4.82, 5.57, 1.39, 1.67), (4.21, 5.21, 1.11, 1.34)]
Epoch 7 - Train Loss: 0.2880 - Train Acc: 0.8845 - Val Loss: 0.2774 - Val Acc: 0.9054 - Val MCC: 0.82
Epoch 8/30
Batch 654/3274 - Loss: 0.2511 - Accuracy: 0.9062
Batch 1308/3274 - Loss: 0.5159 - Accuracy: 0.7812
Batch 1962/3274 - Loss: 0.3474 - Accuracy: 0.8438
Batch 2616/3274 - Loss: 0.4337 - Accuracy: 0.8438
Batch 3270/3274 - Loss: 0.1833 - Accuracy: 0.9688
Scalings = [(3.63, 3.52, 0.82, 0.76), (3.32, 2.83, 0.64, 0.25), (3.43, 3.18, 0.73, 0.35), (3.37, 3.13, 0.65, 0.25), (3.65, 3.25, 0.81, 0.27), (3.95, 3.36, 0.93, 0.12), (3.86, 3.85, 0.94, 0.14), (3.99, 4.09, 0.85, 0.3), (4.17, 4.92, 1.01, 0.62), (4.3, 4.03, 1.1, 0.45), (4.19, 4.25, 1.04, 0.45), (4.86, 4.38, 1.35, 0.38), (4.52, 4.38, 1.01, 0.66), (4.95, 4.48, 1.38, 0.73), (5.13, 4.92, 1.4, 0.72), (5.46, 5.18, 1.63, 1.2), (5.46, 5.05, 1.59, 1.03), (5.53, 5.39, 1.47, 1.35), (6.23, 5.67, 1.76, 1.58), (5.63, 5.16, 1.64, 1.3), (5.44, 5.82, 1.54, 1.39), (5.62, 5.74, 2.09, 1.64), (4.88, 5.73, 1.32, 1.68), (4.28, 5.54, 1.04, 1.37)]
Epoch 8 - Train Loss: 0.2837 - Train Acc: 0.8875 - Val Loss: 0.2556 - Val Acc: 0.9118 - Val MCC: 0.83
Epoch 9/30
Batch 654/3274 - Loss: 0.3927 - Accuracy: 0.8438
Batch 1308/3274 - Loss: 0.3777 - Accuracy: 0.8438
Batch 1962/3274 - Loss: 0.4827 - Accuracy: 0.8125
Batch 2616/3274 - Loss: 0.2149 - Accuracy: 0.9062
Batch 3270/3274 - Loss: 0.1571 - Accuracy: 0.9688
Scalings = [(3.87, 3.68, 0.85, 0.73), (3.42, 3.01, 0.59, 0.19), (3.46, 3.48, 0.63, 0.36), (3.59, 3.3, 0.65, 0.21), (3.76, 3.64, 0.76, 0.28), (4.06, 3.73, 0.86, 0.14), (4.05, 4.17, 0.92, 0.15), (4.23, 4.32, 0.85, 0.27), (4.33, 5.14, 0.99, 0.6), (4.46, 4.34, 1.07, 0.46), (4.34, 4.5, 1.01, 0.43), (5.0, 4.65, 1.34, 0.38), (4.66, 4.59, 0.99, 0.66), (5.09, 4.62, 1.4, 0.69), (5.28, 5.14, 1.38, 0.7), (5.64, 5.33, 1.67, 1.18), (5.59, 5.22, 1.6, 1.03), (5.65, 5.63, 1.47, 1.38), (6.42, 5.78, 1.79, 1.57), (5.77, 5.41, 1.63, 1.33), (5.64, 5.9, 1.54, 1.38), (5.8, 5.9, 2.14, 1.65), (4.95, 5.85, 1.26, 1.67), (4.65, 5.79, 1.1, 1.38)]
Epoch 9 - Train Loss: 0.2801 - Train Acc: 0.8880 - Val Loss: 0.2405 - Val Acc: 0.9156 - Val MCC: 0.83
Epoch 10/30
Batch 654/3274 - Loss: 0.2239 - Accuracy: 0.9375
Batch 1308/3274 - Loss: 0.3447 - Accuracy: 0.8750
Batch 1962/3274 - Loss: 0.3068 - Accuracy: 0.9062
Batch 2616/3274 - Loss: 0.3009 - Accuracy: 0.9375
Batch 3270/3274 - Loss: 0.3508 - Accuracy: 0.8125
Scalings = [(3.97, 3.86, 0.82, 0.75), (3.55, 3.26, 0.58, 0.22), (3.54, 3.71, 0.62, 0.38), (3.93, 3.53, 0.74, 0.21), (3.81, 3.81, 0.7, 0.25), (4.29, 3.9, 0.89, 0.12), (4.08, 4.39, 0.86, 0.14), (4.46, 4.58, 0.87, 0.31), (4.48, 5.33, 0.98, 0.61), (4.58, 4.49, 1.06, 0.43), (4.49, 4.69, 1.0, 0.43), (5.13, 4.84, 1.35, 0.36), (4.78, 4.74, 0.97, 0.63), (5.19, 4.8, 1.38, 0.69), (5.4, 5.33, 1.38, 0.69), (5.7, 5.49, 1.63, 1.19), (5.74, 5.32, 1.61, 1.01), (5.81, 5.74, 1.49, 1.38), (6.57, 5.84, 1.83, 1.55), (5.99, 5.58, 1.69, 1.35), (5.78, 6.07, 1.54, 1.39), (5.85, 6.0, 2.11, 1.63), (5.08, 5.94, 1.26, 1.67), (4.65, 5.86, 1.04, 1.36)]
Epoch 10 - Train Loss: 0.2779 - Train Acc: 0.8900 - Val Loss: 0.2405 - Val Acc: 0.9145 - Val MCC: 0.83
Epoch 11/30
Batch 654/3274 - Loss: 0.3100 - Accuracy: 0.8750
Batch 1308/3274 - Loss: 0.3628 - Accuracy: 0.8750
Batch 1962/3274 - Loss: 0.1852 - Accuracy: 0.9375
Batch 2616/3274 - Loss: 0.2621 - Accuracy: 0.8438
Batch 3270/3274 - Loss: 0.1054 - Accuracy: 1.0000
Scalings = [(4.17, 3.93, 0.87, 0.71), (3.8, 3.31, 0.61, 0.16), (3.66, 3.85, 0.57, 0.36), (4.03, 3.58, 0.7, 0.16), (3.95, 3.95, 0.7, 0.21), (4.48, 4.09, 0.92, 0.1), (4.31, 4.57, 0.89, 0.11), (4.57, 4.74, 0.85, 0.29), (4.58, 5.42, 0.96, 0.57), (4.78, 4.69, 1.12, 0.44), (4.6, 4.91, 1.0, 0.43), (5.24, 5.06, 1.35, 0.37), (4.93, 4.86, 0.98, 0.62), (5.28, 4.93, 1.37, 0.68), (5.51, 5.46, 1.38, 0.68), (5.81, 5.51, 1.65, 1.14), (5.82, 5.43, 1.62, 0.99), (5.97, 5.84, 1.51, 1.38), (6.69, 5.88, 1.84, 1.53), (6.04, 5.64, 1.67, 1.33), (5.87, 6.08, 1.53, 1.36), (6.01, 6.1, 2.15, 1.64), (5.35, 6.08, 1.29, 1.68), (4.57, 5.97, 0.96, 1.36)]
Epoch 11 - Train Loss: 0.2745 - Train Acc: 0.8906 - Val Loss: 0.2596 - Val Acc: 0.9087 - Val MCC: 0.82
Epoch 12/30
Batch 654/3274 - Loss: 0.1658 - Accuracy: 0.9375
Batch 1308/3274 - Loss: 0.3392 - Accuracy: 0.8438
Batch 1962/3274 - Loss: 0.1667 - Accuracy: 0.9375
Batch 2616/3274 - Loss: 0.2747 - Accuracy: 0.9062
Batch 3270/3274 - Loss: 0.2816 - Accuracy: 0.9062
Scalings = [(4.29, 4.04, 0.89, 0.72), (3.88, 3.45, 0.6, 0.17), (3.81, 3.92, 0.58, 0.34), (4.11, 3.81, 0.69, 0.18), (4.0, 4.06, 0.67, 0.2), (4.54, 4.32, 0.88, 0.13), (4.42, 4.73, 0.89, 0.11), (4.64, 4.93, 0.83, 0.31), (4.64, 5.51, 0.93, 0.55), (4.86, 4.81, 1.1, 0.41), (4.63, 5.03, 0.95, 0.43), (5.32, 5.18, 1.34, 0.36), (5.07, 5.03, 1.01, 0.66), (5.37, 5.01, 1.38, 0.67), (5.59, 5.58, 1.38, 0.69), (5.91, 5.6, 1.68, 1.12), (5.93, 5.5, 1.67, 0.99), (6.01, 5.9, 1.51, 1.38), (6.76, 5.99, 1.84, 1.56), (6.14, 5.78, 1.69, 1.34), (5.98, 6.18, 1.52, 1.38), (6.06, 6.17, 2.13, 1.64), (5.32, 6.21, 1.24, 1.69), (4.59, 6.13, 0.93, 1.37)]
Epoch 12 - Train Loss: 0.2737 - Train Acc: 0.8913 - Val Loss: 0.2486 - Val Acc: 0.9125 - Val MCC: 0.83
Epoch 13/30
Batch 654/3274 - Loss: 0.0771 - Accuracy: 0.9688
Batch 1308/3274 - Loss: 0.2497 - Accuracy: 0.9062
Batch 1962/3274 - Loss: 0.1719 - Accuracy: 0.9688
Batch 2616/3274 - Loss: 0.1838 - Accuracy: 0.9375
Batch 3270/3274 - Loss: 0.3393 - Accuracy: 0.8750
Scalings = [(4.36, 4.24, 0.9, 0.76), (3.94, 3.56, 0.58, 0.17), (4.07, 4.02, 0.65, 0.35), (4.18, 3.9, 0.69, 0.16), (4.06, 4.28, 0.65, 0.24), (4.74, 4.54, 0.94, 0.15), (4.53, 4.92, 0.89, 0.12), (4.76, 5.04, 0.86, 0.31), (4.79, 5.63, 0.98, 0.59), (4.87, 4.89, 1.04, 0.41), (4.72, 5.13, 0.97, 0.42), (5.38, 5.32, 1.34, 0.37), (5.15, 5.11, 1.0, 0.64), (5.46, 5.1, 1.38, 0.67), (5.66, 5.77, 1.38, 0.72), (5.96, 5.69, 1.65, 1.12), (6.01, 5.6, 1.68, 1.0), (6.04, 5.93, 1.5, 1.34), (6.89, 6.03, 1.9, 1.55), (6.24, 5.85, 1.71, 1.35), (6.1, 6.28, 1.53, 1.4), (6.18, 6.2, 2.16, 1.63), (5.38, 6.32, 1.23, 1.7), (4.61, 6.18, 0.9, 1.36)]
Epoch 13 - Train Loss: 0.2707 - Train Acc: 0.8927 - Val Loss: 0.2395 - Val Acc: 0.9123 - Val MCC: 0.83
Epoch 14/30
Batch 654/3274 - Loss: 0.2875 - Accuracy: 0.8438
Batch 1308/3274 - Loss: 0.1968 - Accuracy: 0.9375
Batch 1962/3274 - Loss: 0.1432 - Accuracy: 0.9375
Batch 2616/3274 - Loss: 0.3166 - Accuracy: 0.8750
Batch 3270/3274 - Loss: 0.1978 - Accuracy: 0.8750
Scalings = [(4.38, 4.32, 0.86, 0.77), (4.04, 3.54, 0.59, 0.13), (4.2, 4.13, 0.65, 0.36), (4.34, 3.99, 0.73, 0.16), (4.21, 4.27, 0.68, 0.19), (4.79, 4.55, 0.94, 0.12), (4.64, 4.97, 0.91, 0.1), (4.84, 5.12, 0.85, 0.29), (4.88, 5.73, 0.99, 0.6), (4.92, 5.02, 1.04, 0.44), (4.76, 5.24, 0.95, 0.43), (5.46, 5.42, 1.37, 0.38), (5.24, 5.18, 1.02, 0.63), (5.46, 5.17, 1.37, 0.67), (5.74, 5.87, 1.4, 0.73), (6.01, 5.75, 1.66, 1.13), (6.1, 5.68, 1.72, 1.01), (6.15, 6.0, 1.55, 1.35), (6.98, 6.08, 1.93, 1.55), (6.32, 5.96, 1.72, 1.36), (6.23, 6.35, 1.57, 1.41), (6.23, 6.26, 2.17, 1.64), (5.43, 6.38, 1.23, 1.71), (4.7, 6.35, 0.89, 1.38)]
Epoch 14 - Train Loss: 0.2679 - Train Acc: 0.8938 - Val Loss: 0.2479 - Val Acc: 0.9143 - Val MCC: 0.83
Epoch 15/30
Batch 654/3274 - Loss: 0.2586 - Accuracy: 0.9375
Batch 1308/3274 - Loss: 0.2418 - Accuracy: 0.8750
Batch 1962/3274 - Loss: 0.2104 - Accuracy: 0.9375
Batch 2616/3274 - Loss: 0.2416 - Accuracy: 0.9375
Batch 3270/3274 - Loss: 0.3125 - Accuracy: 0.8750
Scalings = [(4.49, 4.39, 0.9, 0.78), (4.1, 3.56, 0.56, 0.12), (4.2, 4.21, 0.62, 0.37), (4.41, 4.12, 0.71, 0.17), (4.32, 4.38, 0.69, 0.2), (4.87, 4.74, 0.94, 0.15), (4.58, 5.17, 0.85, 0.13), (4.85, 5.2, 0.81, 0.29), (4.95, 5.77, 1.0, 0.59), (4.94, 5.14, 1.02, 0.45), (4.8, 5.31, 0.94, 0.43), (5.48, 5.48, 1.35, 0.38), (5.31, 5.21, 1.03, 0.61), (5.51, 5.22, 1.38, 0.66), (5.8, 5.93, 1.4, 0.71), (6.0, 5.78, 1.65, 1.12), (6.11, 5.74, 1.71, 1.02), (6.18, 6.07, 1.55, 1.37), (6.97, 6.09, 1.91, 1.54), (6.35, 5.97, 1.73, 1.36), (6.25, 6.36, 1.56, 1.4), (6.25, 6.25, 2.17, 1.62), (5.5, 6.43, 1.23, 1.72), (4.64, 6.34, 0.83, 1.37)]
Epoch 15 - Train Loss: 0.2681 - Train Acc: 0.8936 - Val Loss: 0.2345 - Val Acc: 0.9154 - Val MCC: 0.83
Epoch 16/30
Batch 654/3274 - Loss: 0.3619 - Accuracy: 0.8750
Batch 1308/3274 - Loss: 0.3797 - Accuracy: 0.8750
Batch 1962/3274 - Loss: 0.2435 - Accuracy: 0.9062
Batch 2616/3274 - Loss: 0.2201 - Accuracy: 0.9062
Batch 3270/3274 - Loss: 0.4072 - Accuracy: 0.7812
Scalings = [(4.51, 4.44, 0.88, 0.77), (4.03, 3.72, 0.51, 0.16), (4.23, 4.19, 0.61, 0.33), (4.5, 4.19, 0.72, 0.17), (4.4, 4.53, 0.71, 0.22), (4.97, 4.81, 0.94, 0.16), (4.61, 5.27, 0.82, 0.12), (4.85, 5.36, 0.8, 0.32), (4.96, 5.87, 0.99, 0.62), (5.0, 5.21, 1.03, 0.46), (4.85, 5.35, 0.95, 0.43), (5.53, 5.58, 1.36, 0.41), (5.38, 5.28, 1.04, 0.63), (5.55, 5.28, 1.39, 0.69), (5.79, 6.02, 1.37, 0.74), (6.03, 5.82, 1.64, 1.12), (6.15, 5.79, 1.73, 1.03), (6.25, 6.04, 1.58, 1.35), (6.99, 6.16, 1.92, 1.57), (6.35, 6.04, 1.71, 1.38), (6.27, 6.41, 1.55, 1.42), (6.22, 6.35, 2.14, 1.65), (5.41, 6.52, 1.18, 1.73), (4.64, 6.43, 0.81, 1.38)]
Epoch 16 - Train Loss: 0.2657 - Train Acc: 0.8939 - Val Loss: 0.2413 - Val Acc: 0.9158 - Val MCC: 0.83
Epoch 17/30
Batch 654/3274 - Loss: 0.2927 - Accuracy: 0.9062
Batch 1308/3274 - Loss: 0.0925 - Accuracy: 1.0000
Batch 1962/3274 - Loss: 0.3583 - Accuracy: 0.7812
Batch 2616/3274 - Loss: 0.1106 - Accuracy: 0.9688
Batch 3270/3274 - Loss: 0.2469 - Accuracy: 0.8438
Scalings = [(4.5, 4.41, 0.86, 0.74), (4.05, 3.86, 0.51, 0.19), (4.23, 4.18, 0.59, 0.31), (4.52, 4.2, 0.72, 0.14), (4.49, 4.55, 0.74, 0.2), (4.94, 4.88, 0.91, 0.16), (4.64, 5.35, 0.81, 0.11), (4.83, 5.43, 0.77, 0.33), (5.0, 5.91, 1.0, 0.63), (5.03, 5.27, 1.04, 0.47), (4.9, 5.39, 0.98, 0.43), (5.55, 5.64, 1.37, 0.42), (5.39, 5.33, 1.03, 0.65), (5.57, 5.33, 1.39, 0.7), (5.81, 6.08, 1.37, 0.77), (6.08, 5.83, 1.68, 1.12), (6.2, 5.79, 1.76, 1.02), (6.3, 6.06, 1.6, 1.35), (7.03, 6.16, 1.94, 1.56), (6.35, 6.05, 1.7, 1.37), (6.34, 6.49, 1.55, 1.43), (6.22, 6.32, 2.12, 1.63), (5.54, 6.55, 1.21, 1.73), (4.68, 6.47, 0.82, 1.39)]
Epoch 17 - Train Loss: 0.2647 - Train Acc: 0.8955 - Val Loss: 0.2340 - Val Acc: 0.9171 - Val MCC: 0.84
Epoch 18/30
Batch 654/3274 - Loss: 0.2169 - Accuracy: 0.9375
Batch 1308/3274 - Loss: 0.2220 - Accuracy: 0.9375
Batch 1962/3274 - Loss: 0.2327 - Accuracy: 0.8750
Batch 2616/3274 - Loss: 0.0794 - Accuracy: 0.9688
Batch 3270/3274 - Loss: 0.2324 - Accuracy: 0.9375
Scalings = [(4.61, 4.39, 0.93, 0.71), (4.15, 3.93, 0.53, 0.19), (4.2, 4.26, 0.56, 0.33), (4.53, 4.22, 0.71, 0.13), (4.55, 4.59, 0.76, 0.21), (5.0, 4.98, 0.92, 0.18), (4.7, 5.46, 0.83, 0.13), (4.95, 5.48, 0.83, 0.35), (5.02, 5.96, 1.0, 0.65), (5.06, 5.31, 1.04, 0.48), (4.92, 5.41, 0.98, 0.42), (5.58, 5.65, 1.37, 0.41), (5.43, 5.35, 1.04, 0.67), (5.55, 5.35, 1.38, 0.7), (5.81, 6.11, 1.37, 0.79), (6.09, 5.87, 1.69, 1.13), (6.22, 5.8, 1.77, 1.04), (6.29, 6.07, 1.59, 1.36), (7.07, 6.19, 1.97, 1.57), (6.37, 6.1, 1.7, 1.38), (6.39, 6.5, 1.57, 1.43), (6.22, 6.34, 2.11, 1.63), (5.47, 6.54, 1.17, 1.72), (4.75, 6.59, 0.82, 1.41)]
Epoch 18 - Train Loss: 0.2626 - Train Acc: 0.8958 - Val Loss: 0.2384 - Val Acc: 0.9152 - Val MCC: 0.83
Epoch 19/30
Batch 654/3274 - Loss: 0.4069 - Accuracy: 0.8438
Batch 1308/3274 - Loss: 0.3364 - Accuracy: 0.8750
Batch 1962/3274 - Loss: 0.2127 - Accuracy: 0.9688
Batch 2616/3274 - Loss: 0.3745 - Accuracy: 0.8125
Batch 3270/3274 - Loss: 0.1528 - Accuracy: 0.9688
Scalings = [(4.65, 4.35, 0.94, 0.67), (4.13, 3.92, 0.51, 0.17), (4.24, 4.23, 0.57, 0.31), (4.51, 4.22, 0.69, 0.11), (4.51, 4.62, 0.72, 0.2), (4.99, 4.99, 0.92, 0.16), (4.69, 5.53, 0.82, 0.14), (4.92, 5.5, 0.8, 0.35), (5.06, 5.97, 1.03, 0.65), (5.06, 5.33, 1.05, 0.48), (4.92, 5.45, 0.97, 0.43), (5.61, 5.67, 1.39, 0.42), (5.47, 5.38, 1.07, 0.67), (5.55, 5.35, 1.37, 0.69), (5.83, 6.1, 1.39, 0.78), (6.09, 5.89, 1.68, 1.14), (6.22, 5.84, 1.79, 1.07), (6.3, 6.07, 1.6, 1.36), (7.1, 6.23, 1.98, 1.6), (6.4, 6.16, 1.72, 1.41), (6.4, 6.5, 1.56, 1.43), (6.24, 6.35, 2.11, 1.63), (5.58, 6.6, 1.19, 1.73), (4.84, 6.59, 0.84, 1.41)]
Epoch 19 - Train Loss: 0.2613 - Train Acc: 0.8967 - Val Loss: 0.2490 - Val Acc: 0.9123 - Val MCC: 0.83
Epoch 20/30
Batch 654/3274 - Loss: 0.1305 - Accuracy: 0.9688
Batch 1308/3274 - Loss: 0.4206 - Accuracy: 0.8125
Batch 1962/3274 - Loss: 0.2599 - Accuracy: 0.8750
Batch 2616/3274 - Loss: 0.3072 - Accuracy: 0.8125
Batch 3270/3274 - Loss: 0.1173 - Accuracy: 0.9688
Scalings = [(4.66, 4.45, 0.95, 0.72), (4.12, 3.88, 0.49, 0.16), (4.32, 4.23, 0.6, 0.3), (4.41, 4.3, 0.65, 0.13), (4.43, 4.63, 0.68, 0.19), (4.95, 5.04, 0.9, 0.17), (4.75, 5.59, 0.84, 0.15), (4.94, 5.53, 0.81, 0.36), (5.06, 5.96, 1.03, 0.64), (5.07, 5.38, 1.06, 0.5), (4.92, 5.49, 0.97, 0.46), (5.6, 5.69, 1.39, 0.44), (5.49, 5.4, 1.08, 0.68), (5.56, 5.36, 1.38, 0.7), (5.85, 6.12, 1.4, 0.8), (6.13, 5.89, 1.7, 1.14), (6.22, 5.84, 1.8, 1.08), (6.29, 6.1, 1.6, 1.37), (7.16, 6.21, 2.02, 1.59), (6.42, 6.16, 1.73, 1.41), (6.37, 6.53, 1.54, 1.44), (6.22, 6.38, 2.09, 1.65), (5.53, 6.63, 1.16, 1.74), (4.79, 6.68, 0.8, 1.43)]
Epoch 20 - Train Loss: 0.2609 - Train Acc: 0.8972 - Val Loss: 0.2594 - Val Acc: 0.9109 - Val MCC: 0.83
Epoch 21/30
Batch 654/3274 - Loss: 0.1689 - Accuracy: 0.9375
Batch 1308/3274 - Loss: 0.2485 - Accuracy: 0.9375
Batch 1962/3274 - Loss: 0.0927 - Accuracy: 0.9688
Batch 2616/3274 - Loss: 0.0967 - Accuracy: 0.9688
Batch 3270/3274 - Loss: 0.2728 - Accuracy: 0.8750
Scalings = [(4.7, 4.48, 0.98, 0.73), (4.17, 3.92, 0.5, 0.16), (4.33, 4.28, 0.58, 0.32), (4.49, 4.36, 0.69, 0.14), (4.43, 4.67, 0.67, 0.21), (4.99, 5.04, 0.92, 0.15), (4.77, 5.59, 0.86, 0.13), (4.94, 5.55, 0.81, 0.37), (5.11, 5.96, 1.05, 0.66), (5.06, 5.41, 1.04, 0.52), (4.94, 5.49, 0.98, 0.46), (5.6, 5.69, 1.39, 0.45), (5.48, 5.4, 1.09, 0.67), (5.56, 5.38, 1.39, 0.74), (5.86, 6.12, 1.41, 0.82), (6.13, 5.88, 1.71, 1.14), (6.2, 5.83, 1.79, 1.07), (6.29, 6.07, 1.6, 1.36), (7.13, 6.24, 2.01, 1.6), (6.43, 6.15, 1.74, 1.41), (6.38, 6.56, 1.53, 1.46), (6.19, 6.39, 2.08, 1.66), (5.55, 6.67, 1.17, 1.75), (4.77, 6.69, 0.78, 1.43)]
Epoch 21 - Train Loss: 0.2576 - Train Acc: 0.8976 - Val Loss: 0.2561 - Val Acc: 0.9143 - Val MCC: 0.83
Epoch 22/30
Batch 654/3274 - Loss: 0.3532 - Accuracy: 0.8438
Batch 1308/3274 - Loss: 0.2237 - Accuracy: 0.9375
Batch 1962/3274 - Loss: 0.1181 - Accuracy: 0.9688
Batch 2616/3274 - Loss: 0.4123 - Accuracy: 0.8125
Batch 3270/3274 - Loss: 0.3188 - Accuracy: 0.9375
Scalings = [(4.69, 4.49, 0.97, 0.74), (4.19, 3.97, 0.51, 0.18), (4.36, 4.26, 0.58, 0.3), (4.47, 4.39, 0.67, 0.14), (4.41, 4.7, 0.64, 0.21), (4.98, 5.08, 0.92, 0.16), (4.8, 5.63, 0.87, 0.13), (4.92, 5.57, 0.79, 0.39), (5.14, 5.96, 1.08, 0.68), (5.07, 5.41, 1.06, 0.51), (4.93, 5.52, 0.98, 0.47), (5.61, 5.7, 1.41, 0.46), (5.5, 5.4, 1.11, 0.68), (5.58, 5.4, 1.41, 0.75), (5.88, 6.11, 1.42, 0.82), (6.12, 5.87, 1.71, 1.14), (6.19, 5.85, 1.8, 1.1), (6.29, 6.08, 1.6, 1.38), (7.14, 6.24, 2.03, 1.61), (6.43, 6.17, 1.74, 1.42), (6.37, 6.59, 1.53, 1.46), (6.16, 6.38, 2.05, 1.65), (5.54, 6.63, 1.15, 1.74), (4.78, 6.68, 0.78, 1.42)]
Epoch 22 - Train Loss: 0.2580 - Train Acc: 0.8976 - Val Loss: 0.2319 - Val Acc: 0.9207 - Val MCC: 0.84
Epoch 23/30
Batch 654/3274 - Loss: 0.1742 - Accuracy: 0.9688
Batch 1308/3274 - Loss: 0.2200 - Accuracy: 0.9062
Batch 1962/3274 - Loss: 0.2747 - Accuracy: 0.8750
Batch 2616/3274 - Loss: 0.1858 - Accuracy: 0.9688
Batch 3270/3274 - Loss: 0.2407 - Accuracy: 0.9375
Scalings = [(4.66, 4.53, 0.96, 0.77), (4.23, 3.97, 0.53, 0.18), (4.37, 4.29, 0.58, 0.31), (4.44, 4.36, 0.66, 0.13), (4.38, 4.67, 0.64, 0.19), (4.99, 5.04, 0.92, 0.15), (4.79, 5.67, 0.87, 0.15), (4.92, 5.58, 0.79, 0.39), (5.14, 5.95, 1.11, 0.68), (5.06, 5.43, 1.06, 0.53), (4.92, 5.53, 0.98, 0.48), (5.6, 5.7, 1.41, 0.47), (5.52, 5.42, 1.13, 0.7), (5.56, 5.38, 1.4, 0.75), (5.88, 6.14, 1.44, 0.84), (6.11, 5.88, 1.72, 1.15), (6.21, 5.84, 1.83, 1.11), (6.27, 6.1, 1.61, 1.39), (7.14, 6.22, 2.04, 1.62), (6.41, 6.18, 1.75, 1.43), (6.36, 6.6, 1.53, 1.48), (6.17, 6.4, 2.07, 1.66), (5.54, 6.68, 1.14, 1.76), (4.81, 6.69, 0.78, 1.42)]
Epoch 23 - Train Loss: 0.2551 - Train Acc: 0.8989 - Val Loss: 0.2212 - Val Acc: 0.9211 - Val MCC: 0.84
Epoch 24/30
Batch 654/3274 - Loss: 0.1255 - Accuracy: 0.9688
Batch 1308/3274 - Loss: 0.2409 - Accuracy: 0.9062
Batch 1962/3274 - Loss: 0.2496 - Accuracy: 0.8438
Batch 2616/3274 - Loss: 0.1347 - Accuracy: 0.9688
Batch 3270/3274 - Loss: 0.2049 - Accuracy: 0.9375
Scalings = [(4.68, 4.52, 0.98, 0.77), (4.25, 3.96, 0.54, 0.17), (4.34, 4.26, 0.56, 0.3), (4.42, 4.41, 0.65, 0.14), (4.4, 4.69, 0.65, 0.2), (5.01, 5.07, 0.93, 0.16), (4.83, 5.7, 0.9, 0.16), (4.92, 5.59, 0.79, 0.41), (5.13, 5.94, 1.1, 0.69), (5.05, 5.42, 1.05, 0.54), (4.92, 5.53, 0.99, 0.49), (5.58, 5.7, 1.41, 0.48), (5.52, 5.4, 1.14, 0.71), (5.55, 5.37, 1.41, 0.76), (5.9, 6.13, 1.45, 0.85), (6.11, 5.87, 1.74, 1.16), (6.19, 5.85, 1.83, 1.13), (6.28, 6.1, 1.62, 1.41), (7.15, 6.23, 2.07, 1.63), (6.44, 6.2, 1.77, 1.44), (6.37, 6.58, 1.53, 1.48), (6.17, 6.39, 2.08, 1.66), (5.52, 6.68, 1.13, 1.76), (4.79, 6.72, 0.78, 1.43)]
Epoch 24 - Train Loss: 0.2548 - Train Acc: 0.8991 - Val Loss: 0.2485 - Val Acc: 0.9156 - Val MCC: 0.83
Epoch 25/30
Batch 654/3274 - Loss: 0.2360 - Accuracy: 0.9062
Batch 1308/3274 - Loss: 0.5136 - Accuracy: 0.8125
Batch 1962/3274 - Loss: 0.1003 - Accuracy: 0.9375
Batch 2616/3274 - Loss: 0.2508 - Accuracy: 0.8750
Batch 3270/3274 - Loss: 0.3474 - Accuracy: 0.8438
Scalings = [(4.68, 4.51, 0.99, 0.76), (4.23, 3.98, 0.53, 0.18), (4.39, 4.3, 0.59, 0.32), (4.46, 4.4, 0.67, 0.14), (4.44, 4.7, 0.67, 0.21), (5.01, 5.09, 0.93, 0.16), (4.85, 5.72, 0.93, 0.15), (4.93, 5.6, 0.8, 0.42), (5.11, 5.94, 1.09, 0.7), (5.04, 5.43, 1.05, 0.56), (4.92, 5.53, 1.0, 0.5), (5.59, 5.69, 1.43, 0.49), (5.53, 5.41, 1.16, 0.73), (5.53, 5.36, 1.4, 0.76), (5.89, 6.14, 1.46, 0.87), (6.1, 5.87, 1.74, 1.16), (6.19, 5.83, 1.84, 1.13), (6.28, 6.09, 1.63, 1.41), (7.13, 6.2, 2.06, 1.62), (6.44, 6.2, 1.77, 1.44), (6.38, 6.56, 1.54, 1.47), (6.17, 6.4, 2.07, 1.66), (5.47, 6.7, 1.11, 1.77), (4.79, 6.74, 0.77, 1.43)]
Epoch 25 - Train Loss: 0.2535 - Train Acc: 0.9005 - Val Loss: 0.2374 - Val Acc: 0.9185 - Val MCC: 0.84
Epoch 26/30
Batch 654/3274 - Loss: 0.4088 - Accuracy: 0.8750
Batch 1308/3274 - Loss: 0.1513 - Accuracy: 0.9375
Batch 1962/3274 - Loss: 0.1594 - Accuracy: 0.9688
Batch 2616/3274 - Loss: 0.2140 - Accuracy: 0.9062
Batch 3270/3274 - Loss: 0.1462 - Accuracy: 0.9688
Scalings = [(4.68, 4.52, 1.0, 0.77), (4.25, 3.97, 0.55, 0.18), (4.4, 4.3, 0.59, 0.33), (4.44, 4.43, 0.66, 0.16), (4.44, 4.73, 0.67, 0.22), (5.01, 5.08, 0.94, 0.16), (4.86, 5.74, 0.93, 0.17), (4.93, 5.58, 0.81, 0.42), (5.09, 5.94, 1.08, 0.71), (5.03, 5.42, 1.05, 0.57), (4.92, 5.51, 1.0, 0.5), (5.59, 5.67, 1.45, 0.49), (5.51, 5.4, 1.15, 0.73), (5.51, 5.35, 1.39, 0.77), (5.89, 6.11, 1.46, 0.87), (6.09, 5.87, 1.74, 1.17), (6.18, 5.84, 1.84, 1.15), (6.26, 6.08, 1.63, 1.41), (7.13, 6.19, 2.07, 1.63), (6.44, 6.22, 1.78, 1.45), (6.37, 6.54, 1.54, 1.47), (6.15, 6.39, 2.07, 1.66), (5.45, 6.7, 1.1, 1.77), (4.78, 6.76, 0.77, 1.44)]
Epoch 26 - Train Loss: 0.2524 - Train Acc: 0.9009 - Val Loss: 0.2327 - Val Acc: 0.9196 - Val MCC: 0.84
Epoch 27/30
Batch 654/3274 - Loss: 0.3655 - Accuracy: 0.8438
Batch 1308/3274 - Loss: 0.1716 - Accuracy: 0.9062
Batch 1962/3274 - Loss: 0.2035 - Accuracy: 0.9375
Batch 2616/3274 - Loss: 0.3786 - Accuracy: 0.9062
Batch 3270/3274 - Loss: 0.2369 - Accuracy: 0.9375
Scalings = [(4.68, 4.51, 1.0, 0.77), (4.25, 3.94, 0.55, 0.17), (4.4, 4.3, 0.6, 0.33), (4.42, 4.42, 0.65, 0.15), (4.43, 4.71, 0.67, 0.22), (5.01, 5.09, 0.94, 0.17), (4.86, 5.75, 0.94, 0.17), (4.93, 5.58, 0.81, 0.44), (5.09, 5.94, 1.09, 0.72), (5.02, 5.4, 1.05, 0.56), (4.92, 5.5, 1.01, 0.51), (5.58, 5.67, 1.45, 0.5), (5.5, 5.39, 1.16, 0.74), (5.51, 5.35, 1.4, 0.79), (5.89, 6.11, 1.47, 0.88), (6.07, 5.87, 1.74, 1.18), (6.18, 5.83, 1.86, 1.15), (6.27, 6.09, 1.64, 1.42), (7.11, 6.2, 2.07, 1.64), (6.42, 6.22, 1.77, 1.46), (6.36, 6.53, 1.54, 1.47), (6.16, 6.38, 2.08, 1.65), (5.45, 6.68, 1.1, 1.76), (4.77, 6.75, 0.77, 1.44)]
Epoch 27 - Train Loss: 0.2532 - Train Acc: 0.8999 - Val Loss: 0.2293 - Val Acc: 0.9189 - Val MCC: 0.84
Epoch 28/30
Batch 654/3274 - Loss: 0.3848 - Accuracy: 0.8750
Batch 1308/3274 - Loss: 0.2582 - Accuracy: 0.9062
Batch 1962/3274 - Loss: 0.2061 - Accuracy: 0.8438
Batch 2616/3274 - Loss: 0.2751 - Accuracy: 0.8750
Batch 3270/3274 - Loss: 0.2839 - Accuracy: 0.9062
Scalings = [(4.68, 4.51, 1.0, 0.77), (4.25, 3.93, 0.55, 0.17), (4.39, 4.31, 0.59, 0.34), (4.42, 4.42, 0.66, 0.15), (4.44, 4.72, 0.67, 0.23), (5.02, 5.1, 0.95, 0.17), (4.85, 5.75, 0.94, 0.17), (4.92, 5.59, 0.81, 0.45), (5.09, 5.93, 1.09, 0.73), (5.02, 5.39, 1.06, 0.56), (4.92, 5.5, 1.02, 0.51), (5.58, 5.67, 1.45, 0.51), (5.51, 5.39, 1.17, 0.75), (5.5, 5.35, 1.4, 0.79), (5.88, 6.1, 1.47, 0.89), (6.06, 5.87, 1.74, 1.18), (6.18, 5.82, 1.87, 1.16), (6.27, 6.08, 1.64, 1.43), (7.11, 6.18, 2.07, 1.64), (6.43, 6.22, 1.78, 1.46), (6.36, 6.53, 1.54, 1.48), (6.15, 6.38, 2.08, 1.65), (5.44, 6.7, 1.1, 1.77), (4.77, 6.78, 0.77, 1.45)]
Epoch 28 - Train Loss: 0.2501 - Train Acc: 0.9017 - Val Loss: 0.2296 - Val Acc: 0.9209 - Val MCC: 0.84
Epoch 29/30
Batch 654/3274 - Loss: 0.2325 - Accuracy: 0.9062
Batch 1308/3274 - Loss: 0.1561 - Accuracy: 0.9688
Batch 1962/3274 - Loss: 0.3956 - Accuracy: 0.9062
Batch 2616/3274 - Loss: 0.1970 - Accuracy: 0.9375
Batch 3270/3274 - Loss: 0.2840 - Accuracy: 0.8438
Scalings = [(4.67, 4.51, 1.01, 0.78), (4.24, 3.92, 0.55, 0.16), (4.39, 4.31, 0.6, 0.34), (4.41, 4.42, 0.65, 0.15), (4.44, 4.72, 0.68, 0.23), (5.02, 5.1, 0.95, 0.17), (4.85, 5.74, 0.94, 0.16), (4.92, 5.59, 0.82, 0.46), (5.09, 5.93, 1.1, 0.74), (5.01, 5.39, 1.06, 0.57), (4.92, 5.5, 1.02, 0.51), (5.57, 5.67, 1.45, 0.51), (5.5, 5.38, 1.17, 0.75), (5.49, 5.35, 1.4, 0.8), (5.88, 6.11, 1.48, 0.9), (6.06, 5.87, 1.74, 1.19), (6.17, 5.82, 1.87, 1.17), (6.26, 6.09, 1.65, 1.43), (7.1, 6.18, 2.07, 1.64), (6.43, 6.22, 1.78, 1.47), (6.36, 6.53, 1.54, 1.48), (6.15, 6.38, 2.08, 1.66), (5.44, 6.7, 1.11, 1.77), (4.77, 6.8, 0.77, 1.45)]
Epoch 29 - Train Loss: 0.2497 - Train Acc: 0.9012 - Val Loss: 0.2388 - Val Acc: 0.9184 - Val MCC: 0.84
Epoch 30/30
Batch 654/3274 - Loss: 0.2927 - Accuracy: 0.8438
Batch 1308/3274 - Loss: 0.3704 - Accuracy: 0.8125
Batch 1962/3274 - Loss: 0.2365 - Accuracy: 0.9062
Batch 2616/3274 - Loss: 0.1734 - Accuracy: 0.9062
Batch 3270/3274 - Loss: 0.2487 - Accuracy: 0.8750
Scalings = [(4.67, 4.51, 1.0, 0.78), (4.24, 3.91, 0.55, 0.16), (4.39, 4.3, 0.59, 0.34), (4.41, 4.42, 0.65, 0.15), (4.44, 4.72, 0.68, 0.23), (5.03, 5.1, 0.95, 0.17), (4.85, 5.74, 0.94, 0.17), (4.93, 5.59, 0.82, 0.45), (5.09, 5.93, 1.1, 0.74), (5.01, 5.39, 1.06, 0.57), (4.92, 5.49, 1.02, 0.51), (5.57, 5.66, 1.45, 0.51), (5.5, 5.38, 1.17, 0.75), (5.49, 5.35, 1.4, 0.8), (5.87, 6.1, 1.48, 0.9), (6.05, 5.87, 1.74, 1.19), (6.17, 5.82, 1.87, 1.17), (6.26, 6.08, 1.65, 1.43), (7.1, 6.18, 2.07, 1.64), (6.43, 6.22, 1.78, 1.47), (6.36, 6.53, 1.54, 1.48), (6.15, 6.38, 2.08, 1.66), (5.44, 6.7, 1.1, 1.77), (4.77, 6.79, 0.77, 1.45)]
Epoch 30 - Train Loss: 0.2492 - Train Acc: 0.9011 - Val Loss: 0.2338 - Val Acc: 0.9193 - Val MCC: 0.84
Average Time per Epoch: 3609.20s
