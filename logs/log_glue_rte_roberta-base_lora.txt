Running experiment with data: glue_rte, model: FacebookAI/roberta-base, LoRA type: lora
RobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-11): 12 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=768, out_features=2, bias=True)
  )
)
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        38,603,520
│    │    └─Embedding: 3-2                                        394,752
│    │    └─Embedding: 3-3                                        768
│    │    └─LayerNorm: 3-4                                        1,536
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,054,464
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                590,592
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                1,538
==========================================================================================
Total params: 124,647,170
Trainable params: 124,647,170
Non-trainable params: 0
==========================================================================================
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        38,603,520
│    │    └─Embedding: 3-2                                        394,752
│    │    └─Embedding: 3-3                                        768
│    │    └─LayerNorm: 3-4                                        1,536
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,054,464
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                590,592
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                1,538
==========================================================================================
Total params: 124,647,170
Trainable params: 124,647,170
Non-trainable params: 0
==========================================================================================
RobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-11): 12 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSdpaSelfAttention(
              (query): LoRALayer(
                (linear): Linear(in_features=768, out_features=768, bias=True)
              )
              (key): LoRALayer(
                (linear): Linear(in_features=768, out_features=768, bias=True)
              )
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=768, out_features=2, bias=True)
  )
)
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        (38,603,520)
│    │    └─Embedding: 3-2                                        (394,752)
│    │    └─Embedding: 3-3                                        (768)
│    │    └─LayerNorm: 3-4                                        (1,536)
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,349,376
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                (590,592)
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                (1,538)
==========================================================================================
Total params: 124,942,082
Trainable params: 294,912
Non-trainable params: 124,647,170
==========================================================================================
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        (38,603,520)
│    │    └─Embedding: 3-2                                        (394,752)
│    │    └─Embedding: 3-3                                        (768)
│    │    └─LayerNorm: 3-4                                        (1,536)
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,349,376
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                (590,592)
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                (1,538)
==========================================================================================
Total params: 124,942,082
Trainable params: 294,912
Non-trainable params: 124,647,170
==========================================================================================
Epoch 1/20
Batch 15/78 - Loss: 0.7477 - Accuracy: 0.4375
Batch 30/78 - Loss: 0.6651 - Accuracy: 0.5938
Batch 45/78 - Loss: 0.7071 - Accuracy: 0.4688
Batch 60/78 - Loss: 0.6720 - Accuracy: 0.5938
Batch 75/78 - Loss: 0.7153 - Accuracy: 0.4688
Epoch 1 - Train Loss: 0.7072 - Train Acc: 0.4990 - Val Loss: 0.7090 - Val Acc: 0.4712 - Time: 44.11s
Epoch 2/20
Batch 15/78 - Loss: 0.6877 - Accuracy: 0.5938
Batch 30/78 - Loss: 0.7329 - Accuracy: 0.4062
Batch 45/78 - Loss: 0.6953 - Accuracy: 0.5312
Batch 60/78 - Loss: 0.7736 - Accuracy: 0.2812
Batch 75/78 - Loss: 0.7031 - Accuracy: 0.5000
Epoch 2 - Train Loss: 0.7047 - Train Acc: 0.4945 - Val Loss: 0.7087 - Val Acc: 0.4712 - Time: 43.13s
Epoch 3/20
Batch 15/78 - Loss: 0.6700 - Accuracy: 0.5625
Batch 30/78 - Loss: 0.6887 - Accuracy: 0.5312
Batch 45/78 - Loss: 0.7275 - Accuracy: 0.4688
Batch 60/78 - Loss: 0.7003 - Accuracy: 0.5000
Batch 75/78 - Loss: 0.7006 - Accuracy: 0.5312
Epoch 3 - Train Loss: 0.7059 - Train Acc: 0.4936 - Val Loss: 0.7083 - Val Acc: 0.4712 - Time: 46.82s
Epoch 4/20
Batch 15/78 - Loss: 0.7309 - Accuracy: 0.5312
Batch 30/78 - Loss: 0.7919 - Accuracy: 0.2812
Batch 45/78 - Loss: 0.6986 - Accuracy: 0.5000
Batch 60/78 - Loss: 0.7095 - Accuracy: 0.4688
Batch 75/78 - Loss: 0.7253 - Accuracy: 0.4375
Epoch 4 - Train Loss: 0.7041 - Train Acc: 0.4965 - Val Loss: 0.7079 - Val Acc: 0.4712 - Time: 45.60s
Epoch 5/20
Batch 15/78 - Loss: 0.7061 - Accuracy: 0.4688
Batch 30/78 - Loss: 0.6923 - Accuracy: 0.5312
Batch 45/78 - Loss: 0.6671 - Accuracy: 0.6875
Batch 60/78 - Loss: 0.6671 - Accuracy: 0.6250
Batch 75/78 - Loss: 0.6646 - Accuracy: 0.5938
Epoch 5 - Train Loss: 0.7015 - Train Acc: 0.4968 - Val Loss: 0.7076 - Val Acc: 0.4712 - Time: 46.71s
Epoch 6/20
Batch 15/78 - Loss: 0.7219 - Accuracy: 0.5000
Batch 30/78 - Loss: 0.6635 - Accuracy: 0.6250
Batch 45/78 - Loss: 0.7158 - Accuracy: 0.4688
Batch 60/78 - Loss: 0.7372 - Accuracy: 0.4375
Batch 75/78 - Loss: 0.7354 - Accuracy: 0.4062
Epoch 6 - Train Loss: 0.7047 - Train Acc: 0.4925 - Val Loss: 0.7072 - Val Acc: 0.4712 - Time: 44.31s
Epoch 7/20
Batch 15/78 - Loss: 0.6709 - Accuracy: 0.6250
Batch 30/78 - Loss: 0.7459 - Accuracy: 0.4062
Batch 45/78 - Loss: 0.6965 - Accuracy: 0.5000
Batch 60/78 - Loss: 0.6939 - Accuracy: 0.6250
Batch 75/78 - Loss: 0.7169 - Accuracy: 0.4688
Epoch 7 - Train Loss: 0.7028 - Train Acc: 0.4963 - Val Loss: 0.7069 - Val Acc: 0.4712 - Time: 45.50s
Epoch 8/20
Batch 15/78 - Loss: 0.7467 - Accuracy: 0.3438
Batch 30/78 - Loss: 0.7132 - Accuracy: 0.4375
Batch 45/78 - Loss: 0.7190 - Accuracy: 0.4062
Batch 60/78 - Loss: 0.6918 - Accuracy: 0.5312
Batch 75/78 - Loss: 0.6974 - Accuracy: 0.5000
Epoch 8 - Train Loss: 0.7046 - Train Acc: 0.4959 - Val Loss: 0.7066 - Val Acc: 0.4712 - Time: 44.51s
Epoch 9/20
Batch 15/78 - Loss: 0.7018 - Accuracy: 0.4688
Batch 30/78 - Loss: 0.7134 - Accuracy: 0.4062
Batch 45/78 - Loss: 0.6980 - Accuracy: 0.5000
Batch 60/78 - Loss: 0.6975 - Accuracy: 0.5000
Batch 75/78 - Loss: 0.6943 - Accuracy: 0.5312
Epoch 9 - Train Loss: 0.7009 - Train Acc: 0.5001 - Val Loss: 0.7063 - Val Acc: 0.4712 - Time: 43.89s
Epoch 10/20
Batch 15/78 - Loss: 0.6569 - Accuracy: 0.6250
Batch 30/78 - Loss: 0.7024 - Accuracy: 0.4062
Batch 45/78 - Loss: 0.7114 - Accuracy: 0.4062
Batch 60/78 - Loss: 0.6910 - Accuracy: 0.5625
Batch 75/78 - Loss: 0.7104 - Accuracy: 0.5000
Epoch 10 - Train Loss: 0.7018 - Train Acc: 0.4911 - Val Loss: 0.7060 - Val Acc: 0.4712 - Time: 47.04s
Epoch 11/20
Batch 15/78 - Loss: 0.6632 - Accuracy: 0.6875
Batch 30/78 - Loss: 0.7036 - Accuracy: 0.4375
Batch 45/78 - Loss: 0.7127 - Accuracy: 0.4688
Batch 60/78 - Loss: 0.6645 - Accuracy: 0.6562
Batch 75/78 - Loss: 0.7554 - Accuracy: 0.3438
Epoch 11 - Train Loss: 0.7032 - Train Acc: 0.4924 - Val Loss: 0.7058 - Val Acc: 0.4712 - Time: 44.42s
Epoch 12/20
Batch 15/78 - Loss: 0.7175 - Accuracy: 0.4688
Batch 30/78 - Loss: 0.7193 - Accuracy: 0.4062
Batch 45/78 - Loss: 0.6723 - Accuracy: 0.5312
Batch 60/78 - Loss: 0.7070 - Accuracy: 0.5000
Batch 75/78 - Loss: 0.7090 - Accuracy: 0.5000
Epoch 12 - Train Loss: 0.7013 - Train Acc: 0.5024 - Val Loss: 0.7055 - Val Acc: 0.4712 - Time: 47.41s
Epoch 13/20
Batch 15/78 - Loss: 0.7420 - Accuracy: 0.4375
Batch 30/78 - Loss: 0.7109 - Accuracy: 0.5312
Batch 45/78 - Loss: 0.7272 - Accuracy: 0.4062
Batch 60/78 - Loss: 0.6759 - Accuracy: 0.6250
Batch 75/78 - Loss: 0.7096 - Accuracy: 0.4375
Epoch 13 - Train Loss: 0.7015 - Train Acc: 0.5035 - Val Loss: 0.7053 - Val Acc: 0.4712 - Time: 47.44s
Epoch 14/20
Batch 15/78 - Loss: 0.7042 - Accuracy: 0.4688
Batch 30/78 - Loss: 0.7247 - Accuracy: 0.4375
Batch 45/78 - Loss: 0.7250 - Accuracy: 0.4062
Batch 60/78 - Loss: 0.7032 - Accuracy: 0.4688
Batch 75/78 - Loss: 0.7344 - Accuracy: 0.3438
Epoch 14 - Train Loss: 0.7014 - Train Acc: 0.4916 - Val Loss: 0.7050 - Val Acc: 0.4712 - Time: 48.23s
Epoch 15/20
Batch 15/78 - Loss: 0.7144 - Accuracy: 0.4375
Batch 30/78 - Loss: 0.6978 - Accuracy: 0.4375
Batch 45/78 - Loss: 0.7145 - Accuracy: 0.4375
Batch 60/78 - Loss: 0.6829 - Accuracy: 0.5312
Batch 75/78 - Loss: 0.6312 - Accuracy: 0.7500
Epoch 15 - Train Loss: 0.6999 - Train Acc: 0.5002 - Val Loss: 0.7047 - Val Acc: 0.4712 - Time: 47.43s
Epoch 16/20
Batch 15/78 - Loss: 0.7102 - Accuracy: 0.3750
Batch 30/78 - Loss: 0.7058 - Accuracy: 0.4375
Batch 45/78 - Loss: 0.7049 - Accuracy: 0.5312
Batch 60/78 - Loss: 0.6874 - Accuracy: 0.4688
Batch 75/78 - Loss: 0.7335 - Accuracy: 0.3750
Epoch 16 - Train Loss: 0.7034 - Train Acc: 0.4971 - Val Loss: 0.7044 - Val Acc: 0.4712 - Time: 44.27s
Epoch 17/20
Batch 15/78 - Loss: 0.7506 - Accuracy: 0.3750
Batch 30/78 - Loss: 0.7001 - Accuracy: 0.5000
Batch 45/78 - Loss: 0.6966 - Accuracy: 0.4688
Batch 60/78 - Loss: 0.6701 - Accuracy: 0.6562
Batch 75/78 - Loss: 0.6768 - Accuracy: 0.5938
Epoch 17 - Train Loss: 0.7012 - Train Acc: 0.5065 - Val Loss: 0.7040 - Val Acc: 0.4712 - Time: 45.59s
Epoch 18/20
Batch 15/78 - Loss: 0.6849 - Accuracy: 0.5625
Batch 30/78 - Loss: 0.7169 - Accuracy: 0.3750
Batch 45/78 - Loss: 0.6820 - Accuracy: 0.6250
Batch 60/78 - Loss: 0.6984 - Accuracy: 0.5625
Batch 75/78 - Loss: 0.7189 - Accuracy: 0.4375
Epoch 18 - Train Loss: 0.7022 - Train Acc: 0.4919 - Val Loss: 0.7037 - Val Acc: 0.4712 - Time: 48.39s
Epoch 19/20
Batch 15/78 - Loss: 0.6965 - Accuracy: 0.5000
Batch 30/78 - Loss: 0.6869 - Accuracy: 0.5312
Batch 45/78 - Loss: 0.6934 - Accuracy: 0.5625
Batch 60/78 - Loss: 0.7080 - Accuracy: 0.5000
Batch 75/78 - Loss: 0.7032 - Accuracy: 0.4375
Epoch 19 - Train Loss: 0.7010 - Train Acc: 0.4968 - Val Loss: 0.7033 - Val Acc: 0.4712 - Time: 44.66s
Epoch 20/20
Batch 15/78 - Loss: 0.7327 - Accuracy: 0.3750
Batch 30/78 - Loss: 0.7261 - Accuracy: 0.3125
Batch 45/78 - Loss: 0.6959 - Accuracy: 0.5312
Batch 60/78 - Loss: 0.7000 - Accuracy: 0.5312
Batch 75/78 - Loss: 0.6736 - Accuracy: 0.5625
Epoch 20 - Train Loss: 0.6989 - Train Acc: 0.5046 - Val Loss: 0.7029 - Val Acc: 0.4712 - Time: 45.56s
Average Time per Epoch: 45.75s
Validation Loss: 0.7029, Validation Accuracy: 0.4712
