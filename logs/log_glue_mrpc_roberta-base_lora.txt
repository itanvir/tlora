Running experiment with data: glue_mrpc, model: FacebookAI/roberta-base, LoRA type: lora
RobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-11): 12 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=768, out_features=2, bias=True)
  )
)
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        38,603,520
│    │    └─Embedding: 3-2                                        394,752
│    │    └─Embedding: 3-3                                        768
│    │    └─LayerNorm: 3-4                                        1,536
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,054,464
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                590,592
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                1,538
==========================================================================================
Total params: 124,647,170
Trainable params: 124,647,170
Non-trainable params: 0
==========================================================================================
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        38,603,520
│    │    └─Embedding: 3-2                                        394,752
│    │    └─Embedding: 3-3                                        768
│    │    └─LayerNorm: 3-4                                        1,536
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,054,464
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                590,592
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                1,538
==========================================================================================
Total params: 124,647,170
Trainable params: 124,647,170
Non-trainable params: 0
==========================================================================================
RobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-11): 12 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSdpaSelfAttention(
              (query): LoRALayer(
                (linear): Linear(in_features=768, out_features=768, bias=True)
              )
              (key): LoRALayer(
                (linear): Linear(in_features=768, out_features=768, bias=True)
              )
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=768, out_features=2, bias=True)
  )
)
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        (38,603,520)
│    │    └─Embedding: 3-2                                        (394,752)
│    │    └─Embedding: 3-3                                        (768)
│    │    └─LayerNorm: 3-4                                        (1,536)
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,349,376
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                (590,592)
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                (1,538)
==========================================================================================
Total params: 124,942,082
Trainable params: 294,912
Non-trainable params: 124,647,170
==========================================================================================
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        (38,603,520)
│    │    └─Embedding: 3-2                                        (394,752)
│    │    └─Embedding: 3-3                                        (768)
│    │    └─LayerNorm: 3-4                                        (1,536)
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,349,376
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                (590,592)
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                (1,538)
==========================================================================================
Total params: 124,942,082
Trainable params: 294,912
Non-trainable params: 124,647,170
==========================================================================================
Epoch 1/20
Batch 23/115 - Loss: 0.7374 - Accuracy: 0.4375
Batch 46/115 - Loss: 0.8044 - Accuracy: 0.2188
Batch 69/115 - Loss: 0.7222 - Accuracy: 0.4375
Batch 92/115 - Loss: 0.8198 - Accuracy: 0.2500
Batch 115/115 - Loss: 0.7010 - Accuracy: 0.6000
Epoch 1 - Train Loss: 0.7621 - Train Acc: 0.3560 - Val Loss: 0.7588 - Val Acc: 0.3141 - Time: 25.23s
Epoch 2/20
Batch 23/115 - Loss: 0.7353 - Accuracy: 0.4688
Batch 46/115 - Loss: 0.8229 - Accuracy: 0.1562
Batch 69/115 - Loss: 0.7106 - Accuracy: 0.4688
Batch 92/115 - Loss: 0.7799 - Accuracy: 0.3125
Batch 115/115 - Loss: 0.7394 - Accuracy: 0.4500
Epoch 2 - Train Loss: 0.7588 - Train Acc: 0.3523 - Val Loss: 0.7562 - Val Acc: 0.3141 - Time: 23.60s
Epoch 3/20
Batch 23/115 - Loss: 0.7240 - Accuracy: 0.4375
Batch 46/115 - Loss: 0.7378 - Accuracy: 0.4062
Batch 69/115 - Loss: 0.7661 - Accuracy: 0.2812
Batch 92/115 - Loss: 0.7439 - Accuracy: 0.4062
Batch 115/115 - Loss: 0.7403 - Accuracy: 0.5000
Epoch 3 - Train Loss: 0.7551 - Train Acc: 0.3682 - Val Loss: 0.7530 - Val Acc: 0.3141 - Time: 23.74s
Epoch 4/20
Batch 23/115 - Loss: 0.7445 - Accuracy: 0.3750
Batch 46/115 - Loss: 0.7346 - Accuracy: 0.3438
Batch 69/115 - Loss: 0.7480 - Accuracy: 0.3750
Batch 92/115 - Loss: 0.7379 - Accuracy: 0.3750
Batch 115/115 - Loss: 0.7398 - Accuracy: 0.3000
Epoch 4 - Train Loss: 0.7498 - Train Acc: 0.3659 - Val Loss: 0.7484 - Val Acc: 0.3141 - Time: 23.96s
Epoch 5/20
Batch 23/115 - Loss: 0.7460 - Accuracy: 0.3750
Batch 46/115 - Loss: 0.7510 - Accuracy: 0.3750
Batch 69/115 - Loss: 0.7217 - Accuracy: 0.4375
Batch 92/115 - Loss: 0.7504 - Accuracy: 0.3750
Batch 115/115 - Loss: 0.7252 - Accuracy: 0.3500
Epoch 5 - Train Loss: 0.7436 - Train Acc: 0.3585 - Val Loss: 0.7414 - Val Acc: 0.3141 - Time: 24.11s
Epoch 6/20
Batch 23/115 - Loss: 0.7477 - Accuracy: 0.3125
Batch 46/115 - Loss: 0.7292 - Accuracy: 0.3750
Batch 69/115 - Loss: 0.7066 - Accuracy: 0.4062
Batch 92/115 - Loss: 0.7282 - Accuracy: 0.3125
Batch 115/115 - Loss: 0.7391 - Accuracy: 0.3000
Epoch 6 - Train Loss: 0.7352 - Train Acc: 0.3599 - Val Loss: 0.7255 - Val Acc: 0.3141 - Time: 24.29s
Epoch 7/20
Batch 23/115 - Loss: 0.7144 - Accuracy: 0.4688
Batch 46/115 - Loss: 0.7017 - Accuracy: 0.4062
Batch 69/115 - Loss: 0.7365 - Accuracy: 0.2500
Batch 92/115 - Loss: 0.6955 - Accuracy: 0.4375
Batch 115/115 - Loss: 0.6884 - Accuracy: 0.5500
Epoch 7 - Train Loss: 0.7154 - Train Acc: 0.3814 - Val Loss: 0.6996 - Val Acc: 0.3550 - Time: 24.65s
Epoch 8/20
Batch 23/115 - Loss: 0.6850 - Accuracy: 0.5000
Batch 46/115 - Loss: 0.7010 - Accuracy: 0.3438
Batch 69/115 - Loss: 0.6854 - Accuracy: 0.5312
Batch 92/115 - Loss: 0.6901 - Accuracy: 0.5938
Batch 115/115 - Loss: 0.6971 - Accuracy: 0.4500
Epoch 8 - Train Loss: 0.6927 - Train Acc: 0.4901 - Val Loss: 0.6690 - Val Acc: 0.7011 - Time: 25.20s
Epoch 9/20
Batch 23/115 - Loss: 0.6683 - Accuracy: 0.5938
Batch 46/115 - Loss: 0.7147 - Accuracy: 0.4688
Batch 69/115 - Loss: 0.6858 - Accuracy: 0.5312
Batch 92/115 - Loss: 0.6374 - Accuracy: 0.6875
Batch 115/115 - Loss: 0.6156 - Accuracy: 0.7500
Epoch 9 - Train Loss: 0.6668 - Train Acc: 0.6334 - Val Loss: 0.6406 - Val Acc: 0.7043 - Time: 24.66s
Epoch 10/20
Batch 23/115 - Loss: 0.6520 - Accuracy: 0.6562
Batch 46/115 - Loss: 0.7126 - Accuracy: 0.5312
Batch 69/115 - Loss: 0.6264 - Accuracy: 0.6562
Batch 92/115 - Loss: 0.6498 - Accuracy: 0.6875
Batch 115/115 - Loss: 0.6702 - Accuracy: 0.7500
Epoch 10 - Train Loss: 0.6483 - Train Acc: 0.6853 - Val Loss: 0.6224 - Val Acc: 0.6939 - Time: 24.84s
Epoch 11/20
Batch 23/115 - Loss: 0.6331 - Accuracy: 0.7188
Batch 46/115 - Loss: 0.7097 - Accuracy: 0.5625
Batch 69/115 - Loss: 0.6484 - Accuracy: 0.6250
Batch 92/115 - Loss: 0.6622 - Accuracy: 0.6562
Batch 115/115 - Loss: 0.6802 - Accuracy: 0.6500
Epoch 11 - Train Loss: 0.6363 - Train Acc: 0.6899 - Val Loss: 0.6119 - Val Acc: 0.6963 - Time: 24.26s
Epoch 12/20
Batch 23/115 - Loss: 0.5944 - Accuracy: 0.7188
Batch 46/115 - Loss: 0.5833 - Accuracy: 0.8125
Batch 69/115 - Loss: 0.5937 - Accuracy: 0.7812
Batch 92/115 - Loss: 0.6540 - Accuracy: 0.6562
Batch 115/115 - Loss: 0.6885 - Accuracy: 0.5500
Epoch 12 - Train Loss: 0.6263 - Train Acc: 0.6928 - Val Loss: 0.6056 - Val Acc: 0.6939 - Time: 24.65s
Epoch 13/20
Batch 23/115 - Loss: 0.6919 - Accuracy: 0.5625
Batch 46/115 - Loss: 0.5669 - Accuracy: 0.7812
Batch 69/115 - Loss: 0.6222 - Accuracy: 0.7188
Batch 92/115 - Loss: 0.6250 - Accuracy: 0.6875
Batch 115/115 - Loss: 0.5571 - Accuracy: 0.8500
Epoch 13 - Train Loss: 0.6221 - Train Acc: 0.6914 - Val Loss: 0.6015 - Val Acc: 0.6995 - Time: 24.66s
Epoch 14/20
Batch 23/115 - Loss: 0.5786 - Accuracy: 0.7500
Batch 46/115 - Loss: 0.6858 - Accuracy: 0.5625
Batch 69/115 - Loss: 0.6618 - Accuracy: 0.5938
Batch 92/115 - Loss: 0.6736 - Accuracy: 0.6250
Batch 115/115 - Loss: 0.6035 - Accuracy: 0.7000
Epoch 14 - Train Loss: 0.6193 - Train Acc: 0.6960 - Val Loss: 0.5977 - Val Acc: 0.6995 - Time: 24.60s
Epoch 15/20
Batch 23/115 - Loss: 0.6023 - Accuracy: 0.7188
Batch 46/115 - Loss: 0.6399 - Accuracy: 0.6562
Batch 69/115 - Loss: 0.6480 - Accuracy: 0.7188
Batch 92/115 - Loss: 0.5459 - Accuracy: 0.8125
Batch 115/115 - Loss: 0.5306 - Accuracy: 0.8500
Epoch 15 - Train Loss: 0.6128 - Train Acc: 0.6984 - Val Loss: 0.5944 - Val Acc: 0.6995 - Time: 24.38s
Epoch 16/20
Batch 23/115 - Loss: 0.5655 - Accuracy: 0.7188
Batch 46/115 - Loss: 0.6176 - Accuracy: 0.7188
Batch 69/115 - Loss: 0.5933 - Accuracy: 0.7500
Batch 92/115 - Loss: 0.6038 - Accuracy: 0.7500
Batch 115/115 - Loss: 0.5546 - Accuracy: 0.8000
Epoch 16 - Train Loss: 0.6118 - Train Acc: 0.6972 - Val Loss: 0.5925 - Val Acc: 0.6995 - Time: 24.90s
Epoch 17/20
Batch 23/115 - Loss: 0.5712 - Accuracy: 0.7188
Batch 46/115 - Loss: 0.5585 - Accuracy: 0.7500
Batch 69/115 - Loss: 0.6364 - Accuracy: 0.6875
Batch 92/115 - Loss: 0.5401 - Accuracy: 0.7500
Batch 115/115 - Loss: 0.5958 - Accuracy: 0.7000
Epoch 17 - Train Loss: 0.6057 - Train Acc: 0.6998 - Val Loss: 0.5899 - Val Acc: 0.7019 - Time: 24.43s
Epoch 18/20
Batch 23/115 - Loss: 0.6892 - Accuracy: 0.5625
Batch 46/115 - Loss: 0.6928 - Accuracy: 0.5625
Batch 69/115 - Loss: 0.5561 - Accuracy: 0.8438
Batch 92/115 - Loss: 0.5976 - Accuracy: 0.6875
Batch 115/115 - Loss: 0.4925 - Accuracy: 0.8500
Epoch 18 - Train Loss: 0.6044 - Train Acc: 0.6990 - Val Loss: 0.5874 - Val Acc: 0.7067 - Time: 24.89s
Epoch 19/20
Batch 23/115 - Loss: 0.6259 - Accuracy: 0.6562
Batch 46/115 - Loss: 0.5896 - Accuracy: 0.7188
Batch 69/115 - Loss: 0.5056 - Accuracy: 0.8125
Batch 92/115 - Loss: 0.6232 - Accuracy: 0.6875
Batch 115/115 - Loss: 0.5067 - Accuracy: 0.7500
Epoch 19 - Train Loss: 0.6024 - Train Acc: 0.7022 - Val Loss: 0.5849 - Val Acc: 0.7035 - Time: 25.21s
Epoch 20/20
Batch 23/115 - Loss: 0.5493 - Accuracy: 0.8125
Batch 46/115 - Loss: 0.5130 - Accuracy: 0.8438
Batch 69/115 - Loss: 0.6350 - Accuracy: 0.7188
Batch 92/115 - Loss: 0.5421 - Accuracy: 0.8125
Batch 115/115 - Loss: 0.5535 - Accuracy: 0.7500
Epoch 20 - Train Loss: 0.6002 - Train Acc: 0.7043 - Val Loss: 0.5817 - Val Acc: 0.7059 - Time: 24.95s
Average Time per Epoch: 24.56s
Validation Loss: 0.5817, Validation Accuracy: 0.7059
