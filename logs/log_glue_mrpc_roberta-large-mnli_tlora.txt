Running experiment with data: glue_mrpc, model: FacebookAI/roberta-large-mnli, LoRA type: tlora
RobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 1024, padding_idx=1)
      (position_embeddings): Embedding(514, 1024, padding_idx=1)
      (token_type_embeddings): Embedding(1, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-23): 24 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSdpaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHead(
    (dense): Linear(in_features=1024, out_features=1024, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=1024, out_features=3, bias=True)
  )
)
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        51,471,360
│    │    └─Embedding: 3-2                                        526,336
│    │    └─Embedding: 3-3                                        1,024
│    │    └─LayerNorm: 3-4                                        2,048
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       302,309,376
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                1,049,600
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                3,075
==========================================================================================
Total params: 355,362,819
Trainable params: 355,362,819
Non-trainable params: 0
==========================================================================================
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        51,471,360
│    │    └─Embedding: 3-2                                        526,336
│    │    └─Embedding: 3-3                                        1,024
│    │    └─LayerNorm: 3-4                                        2,048
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       302,309,376
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                1,049,600
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                3,075
==========================================================================================
Total params: 355,362,819
Trainable params: 355,362,819
Non-trainable params: 0
==========================================================================================
RobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 1024, padding_idx=1)
      (position_embeddings): Embedding(514, 1024, padding_idx=1)
      (token_type_embeddings): Embedding(1, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-23): 24 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSdpaSelfAttention(
              (query): TLoRALayer(
                (linear): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.5, inplace=False)
              )
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): TLoRALayer(
                (linear): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.5, inplace=False)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHead(
    (dense): Linear(in_features=1024, out_features=1024, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=1024, out_features=3, bias=True)
  )
)
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        (51,471,360)
│    │    └─Embedding: 3-2                                        (526,336)
│    │    └─Embedding: 3-3                                        (1,024)
│    │    └─LayerNorm: 3-4                                        (2,048)
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       305,504,304
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                (1,049,600)
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                (3,075)
==========================================================================================
Total params: 358,557,747
Trainable params: 49,200
Non-trainable params: 358,508,547
==========================================================================================
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        (51,471,360)
│    │    └─Embedding: 3-2                                        (526,336)
│    │    └─Embedding: 3-3                                        (1,024)
│    │    └─LayerNorm: 3-4                                        (2,048)
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       305,504,304
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                (1,049,600)
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                (3,075)
==========================================================================================
Total params: 358,557,747
Trainable params: 49,200
Non-trainable params: 358,508,547
==========================================================================================
Epoch 1/30
Batch 23/115 - Loss: 2.2834 - Accuracy: 0.4062
Batch 46/115 - Loss: 2.2164 - Accuracy: 0.3438
Batch 69/115 - Loss: 2.0492 - Accuracy: 0.4062
Batch 92/115 - Loss: 0.9418 - Accuracy: 0.5625
Batch 115/115 - Loss: 0.6654 - Accuracy: 0.7500
Scalings = [(0.09, 0.11, 1.02, 1.02), (0.09, 0.11, 1.01, 1.03), (0.07, 0.15, 1.01, 1.03), (0.12, 0.14, 1.02, 1.02), (0.13, 0.21, 1.02, 1.04), (0.16, 0.22, 1.03, 1.04), (0.15, 0.25, 1.03, 1.03), (0.21, 0.23, 1.03, 1.03), (0.17, 0.33, 1.03, 1.04), (0.19, 0.34, 1.03, 1.04), (0.16, 0.44, 1.02, 1.04), (0.24, 0.44, 1.04, 1.04), (0.24, 0.4, 1.04, 1.04), (0.25, 0.35, 1.03, 1.04), (0.22, 0.44, 1.04, 1.04), (0.27, 0.51, 1.04, 1.04), (0.27, 0.51, 1.04, 1.04), (0.38, 0.53, 1.04, 1.04), (0.38, 0.61, 1.04, 1.04), (0.4, 0.6, 1.04, 1.04), (0.42, 0.59, 1.04, 1.04), (0.34, 0.62, 1.04, 1.04), (0.33, 0.61, 1.04, 1.04), (0.26, 0.51, 1.04, 1.04)]
Epoch 1 - Train Loss: 1.6233 - Train Acc: 0.4671 - Val Loss: 0.7372 - Val Acc: 0.6803 - Time: 135.39s
Epoch 2/30
Batch 23/115 - Loss: 0.6028 - Accuracy: 0.7188
Batch 46/115 - Loss: 0.6526 - Accuracy: 0.6562
Batch 69/115 - Loss: 0.6599 - Accuracy: 0.6875
Batch 92/115 - Loss: 0.5744 - Accuracy: 0.7188
Batch 115/115 - Loss: 0.7337 - Accuracy: 0.5500
Scalings = [(0.13, 0.14, 1.02, 1.03), (0.13, 0.14, 1.0, 1.03), (0.1, 0.17, 1.01, 1.02), (0.17, 0.19, 1.03, 1.03), (0.19, 0.26, 1.03, 1.04), (0.19, 0.26, 1.03, 1.04), (0.21, 0.3, 1.04, 1.04), (0.26, 0.29, 1.04, 1.04), (0.24, 0.39, 1.05, 1.05), (0.24, 0.39, 1.04, 1.04), (0.21, 0.52, 1.03, 1.05), (0.3, 0.51, 1.05, 1.05), (0.31, 0.45, 1.05, 1.04), (0.3, 0.42, 1.04, 1.04), (0.27, 0.55, 1.04, 1.05), (0.33, 0.69, 1.05, 1.07), (0.4, 0.63, 1.07, 1.05), (0.57, 0.68, 1.07, 1.05), (0.5, 0.91, 1.05, 1.08), (0.64, 0.87, 1.08, 1.07), (0.54, 0.98, 1.05, 1.08), (0.5, 1.01, 1.06, 1.08), (0.47, 1.03, 1.06, 1.09), (0.37, 0.96, 1.06, 1.09)]
Epoch 2 - Train Loss: 0.6217 - Train Acc: 0.7056 - Val Loss: 0.5726 - Val Acc: 0.7292 - Time: 123.00s
Epoch 3/30
Batch 23/115 - Loss: 0.6200 - Accuracy: 0.6250
Batch 46/115 - Loss: 0.7466 - Accuracy: 0.6250
Batch 69/115 - Loss: 0.4699 - Accuracy: 0.7812
Batch 92/115 - Loss: 0.5475 - Accuracy: 0.7500
Batch 115/115 - Loss: 0.3515 - Accuracy: 0.9000
Scalings = [(0.2, 0.21, 1.01, 1.03), (0.21, 0.22, 0.99, 1.05), (0.18, 0.26, 1.05, 1.03), (0.27, 0.26, 1.05, 1.03), (0.28, 0.34, 1.04, 1.04), (0.25, 0.27, 1.03, 1.03), (0.29, 0.35, 1.06, 1.04), (0.28, 0.4, 1.03, 1.07), (0.38, 0.48, 1.09, 1.06), (0.3, 0.44, 1.05, 1.04), (0.29, 0.62, 1.03, 1.06), (0.39, 0.6, 1.07, 1.06), (0.36, 0.52, 1.06, 1.04), (0.37, 0.56, 1.04, 1.05), (0.33, 0.62, 1.05, 1.06), (0.43, 0.87, 1.06, 1.09), (0.56, 0.77, 1.11, 1.06), (0.78, 0.89, 1.11, 1.07), (0.65, 1.05, 1.07, 1.09), (0.87, 0.98, 1.12, 1.08), (0.63, 1.25, 1.06, 1.1), (0.62, 1.13, 1.08, 1.08), (0.58, 1.19, 1.08, 1.1), (0.45, 1.27, 1.07, 1.15)]
Epoch 3 - Train Loss: 0.5379 - Train Acc: 0.7456 - Val Loss: 0.5564 - Val Acc: 0.7476 - Time: 127.47s
Epoch 4/30
Batch 23/115 - Loss: 0.4182 - Accuracy: 0.8438
Batch 46/115 - Loss: 0.5268 - Accuracy: 0.7188
Batch 69/115 - Loss: 0.5362 - Accuracy: 0.7188
Batch 92/115 - Loss: 0.6003 - Accuracy: 0.8125
Batch 115/115 - Loss: 0.5296 - Accuracy: 0.7500
Scalings = [(0.29, 0.29, 1.02, 1.04), (0.29, 0.26, 1.0, 1.04), (0.23, 0.34, 1.05, 1.04), (0.32, 0.32, 1.04, 1.03), (0.36, 0.43, 1.04, 1.05), (0.3, 0.33, 1.02, 1.03), (0.37, 0.39, 1.08, 1.04), (0.35, 0.5, 1.03, 1.08), (0.46, 0.52, 1.1, 1.06), (0.42, 0.48, 1.08, 1.04), (0.41, 0.7, 1.06, 1.07), (0.48, 0.68, 1.08, 1.06), (0.45, 0.63, 1.07, 1.06), (0.5, 0.69, 1.07, 1.06), (0.43, 0.66, 1.07, 1.05), (0.55, 0.99, 1.08, 1.1), (0.75, 1.0, 1.16, 1.08), (0.97, 1.08, 1.15, 1.08), (0.82, 1.2, 1.09, 1.11), (1.04, 1.15, 1.15, 1.09), (0.73, 1.47, 1.07, 1.12), (0.81, 1.22, 1.12, 1.08), (0.79, 1.33, 1.13, 1.11), (0.58, 1.51, 1.1, 1.19)]
Epoch 4 - Train Loss: 0.4875 - Train Acc: 0.7785 - Val Loss: 0.5365 - Val Acc: 0.7917 - Time: 125.31s
Epoch 5/30
Batch 23/115 - Loss: 0.3723 - Accuracy: 0.8438
Batch 46/115 - Loss: 0.4470 - Accuracy: 0.7500
Batch 69/115 - Loss: 0.2574 - Accuracy: 0.9062
Batch 92/115 - Loss: 0.3863 - Accuracy: 0.7500
Batch 115/115 - Loss: 0.2451 - Accuracy: 1.0000
Scalings = [(0.36, 0.34, 1.03, 1.03), (0.3, 0.29, 0.97, 1.03), (0.27, 0.38, 1.04, 1.04), (0.38, 0.37, 1.04, 1.02), (0.41, 0.44, 1.03, 1.03), (0.36, 0.37, 1.02, 1.02), (0.47, 0.45, 1.1, 1.03), (0.44, 0.52, 1.03, 1.06), (0.52, 0.57, 1.09, 1.05), (0.5, 0.52, 1.09, 1.03), (0.55, 0.71, 1.09, 1.06), (0.56, 0.71, 1.09, 1.05), (0.59, 0.79, 1.1, 1.07), (0.66, 0.84, 1.1, 1.07), (0.64, 0.77, 1.15, 1.06), (0.71, 1.14, 1.11, 1.11), (1.02, 1.2, 1.23, 1.1), (1.29, 1.26, 1.23, 1.09), (1.1, 1.38, 1.16, 1.13), (1.24, 1.33, 1.18, 1.11), (0.97, 1.71, 1.11, 1.15), (1.04, 1.42, 1.18, 1.1), (1.14, 1.49, 1.21, 1.13), (0.93, 1.71, 1.19, 1.21)]
Epoch 5 - Train Loss: 0.4313 - Train Acc: 0.8016 - Val Loss: 0.4439 - Val Acc: 0.8253 - Time: 128.16s
Epoch 6/30
Batch 23/115 - Loss: 0.7276 - Accuracy: 0.6875
Batch 46/115 - Loss: 0.4023 - Accuracy: 0.8125
Batch 69/115 - Loss: 0.5498 - Accuracy: 0.6562
Batch 92/115 - Loss: 0.2650 - Accuracy: 0.8750
Batch 115/115 - Loss: 0.2768 - Accuracy: 0.9000
Scalings = [(0.42, 0.41, 1.05, 1.06), (0.35, 0.34, 0.97, 1.03), (0.31, 0.44, 1.03, 1.04), (0.46, 0.45, 1.06, 1.03), (0.46, 0.55, 1.03, 1.05), (0.42, 0.47, 1.01, 1.03), (0.55, 0.5, 1.11, 1.03), (0.54, 0.57, 1.05, 1.06), (0.64, 0.58, 1.11, 1.03), (0.58, 0.6, 1.09, 1.03), (0.68, 0.72, 1.11, 1.05), (0.68, 0.74, 1.1, 1.05), (0.68, 0.81, 1.11, 1.06), (0.78, 1.01, 1.12, 1.1), (0.88, 0.85, 1.23, 1.05), (0.88, 1.27, 1.15, 1.12), (1.19, 1.34, 1.26, 1.11), (1.49, 1.38, 1.27, 1.1), (1.36, 1.48, 1.23, 1.14), (1.36, 1.48, 1.2, 1.12), (1.19, 1.91, 1.16, 1.17), (1.17, 1.54, 1.2, 1.11), (1.44, 1.58, 1.28, 1.15), (1.22, 1.84, 1.24, 1.22)]
Epoch 6 - Train Loss: 0.4055 - Train Acc: 0.8160 - Val Loss: 0.3794 - Val Acc: 0.8446 - Time: 118.25s
Epoch 7/30
Batch 23/115 - Loss: 0.2996 - Accuracy: 0.9062
Batch 46/115 - Loss: 0.4147 - Accuracy: 0.7812
Batch 69/115 - Loss: 0.2361 - Accuracy: 0.9688
Batch 92/115 - Loss: 0.5435 - Accuracy: 0.7500
Batch 115/115 - Loss: 0.3398 - Accuracy: 0.9000
Scalings = [(0.48, 0.46, 1.05, 1.05), (0.4, 0.37, 0.97, 1.02), (0.33, 0.52, 1.02, 1.05), (0.49, 0.52, 1.06, 1.03), (0.49, 0.56, 1.01, 1.03), (0.49, 0.5, 1.02, 1.02), (0.63, 0.49, 1.12, 1.01), (0.6, 0.58, 1.05, 1.04), (0.71, 0.6, 1.1, 1.02), (0.67, 0.63, 1.1, 1.02), (0.82, 0.76, 1.14, 1.04), (0.81, 0.81, 1.12, 1.05), (0.79, 0.84, 1.13, 1.06), (0.87, 1.08, 1.13, 1.1), (1.0, 0.95, 1.26, 1.06), (1.03, 1.32, 1.18, 1.12), (1.34, 1.41, 1.28, 1.1), (1.69, 1.44, 1.32, 1.09), (1.54, 1.6, 1.27, 1.15), (1.45, 1.65, 1.22, 1.13), (1.32, 2.08, 1.18, 1.19), (1.39, 1.66, 1.25, 1.12), (1.64, 1.71, 1.31, 1.16), (1.61, 2.02, 1.31, 1.24)]
Epoch 7 - Train Loss: 0.3798 - Train Acc: 0.8328 - Val Loss: 0.3980 - Val Acc: 0.8494 - Time: 127.59s
Epoch 8/30
Batch 23/115 - Loss: 0.4190 - Accuracy: 0.8438
Batch 46/115 - Loss: 0.2112 - Accuracy: 0.9375
Batch 69/115 - Loss: 0.2850 - Accuracy: 0.8750
Batch 92/115 - Loss: 0.4752 - Accuracy: 0.7812
Batch 115/115 - Loss: 0.2057 - Accuracy: 0.9500
Scalings = [(0.56, 0.52, 1.09, 1.07), (0.45, 0.41, 0.97, 1.01), (0.37, 0.58, 1.01, 1.06), (0.59, 0.59, 1.08, 1.04), (0.57, 0.64, 1.03, 1.04), (0.54, 0.57, 1.02, 1.02), (0.72, 0.55, 1.13, 1.01), (0.67, 0.62, 1.05, 1.04), (0.81, 0.7, 1.11, 1.03), (0.75, 0.7, 1.11, 1.01), (0.94, 0.77, 1.17, 1.03), (0.9, 0.83, 1.13, 1.04), (0.93, 0.9, 1.17, 1.05), (0.96, 1.15, 1.14, 1.1), (1.16, 1.01, 1.29, 1.06), (1.16, 1.4, 1.2, 1.12), (1.5, 1.51, 1.31, 1.11), (1.8, 1.53, 1.34, 1.09), (1.67, 1.7, 1.31, 1.17), (1.55, 1.78, 1.23, 1.14), (1.44, 2.19, 1.2, 1.2), (1.51, 1.72, 1.27, 1.13), (1.81, 1.81, 1.35, 1.18), (1.77, 2.16, 1.33, 1.25)]
Epoch 8 - Train Loss: 0.3521 - Train Acc: 0.8504 - Val Loss: 0.4296 - Val Acc: 0.8518 - Time: 126.90s
Epoch 9/30
Batch 23/115 - Loss: 0.1745 - Accuracy: 0.9062
Batch 46/115 - Loss: 0.4213 - Accuracy: 0.7812
Batch 69/115 - Loss: 0.3490 - Accuracy: 0.8125
Batch 92/115 - Loss: 0.4150 - Accuracy: 0.8438
Batch 115/115 - Loss: 0.3679 - Accuracy: 0.8500
Scalings = [(0.63, 0.55, 1.11, 1.06), (0.5, 0.44, 0.97, 1.0), (0.4, 0.64, 1.0, 1.06), (0.62, 0.63, 1.07, 1.04), (0.64, 0.68, 1.05, 1.04), (0.58, 0.61, 1.02, 1.01), (0.82, 0.63, 1.16, 1.01), (0.73, 0.66, 1.05, 1.03), (0.88, 0.83, 1.12, 1.05), (0.84, 0.75, 1.12, 1.01), (1.03, 0.82, 1.18, 1.03), (1.0, 0.87, 1.15, 1.03), (1.03, 0.9, 1.18, 1.04), (1.07, 1.21, 1.16, 1.1), (1.25, 1.03, 1.3, 1.04), (1.28, 1.48, 1.23, 1.13), (1.58, 1.57, 1.32, 1.11), (1.95, 1.58, 1.37, 1.09), (1.8, 1.78, 1.33, 1.18), (1.6, 1.89, 1.24, 1.15), (1.54, 2.31, 1.21, 1.22), (1.61, 1.81, 1.29, 1.14), (1.93, 1.89, 1.37, 1.19), (2.01, 2.29, 1.36, 1.26)]
Epoch 9 - Train Loss: 0.3399 - Train Acc: 0.8471 - Val Loss: 0.3737 - Val Acc: 0.8734 - Time: 127.59s
Epoch 10/30
Batch 23/115 - Loss: 0.3879 - Accuracy: 0.8125
Batch 46/115 - Loss: 0.4084 - Accuracy: 0.8125
Batch 69/115 - Loss: 0.3869 - Accuracy: 0.7812
Batch 92/115 - Loss: 0.3834 - Accuracy: 0.8438
Batch 115/115 - Loss: 0.2399 - Accuracy: 0.9000
Scalings = [(0.69, 0.6, 1.13, 1.07), (0.52, 0.45, 0.96, 0.99), (0.43, 0.69, 1.0, 1.07), (0.67, 0.69, 1.08, 1.04), (0.68, 0.71, 1.05, 1.04), (0.6, 0.67, 1.01, 1.01), (0.9, 0.66, 1.18, 1.0), (0.81, 0.71, 1.06, 1.03), (0.93, 0.88, 1.12, 1.04), (0.92, 0.81, 1.14, 1.01), (1.11, 0.84, 1.19, 1.02), (1.07, 0.9, 1.16, 1.03), (1.03, 0.93, 1.17, 1.04), (1.15, 1.27, 1.18, 1.1), (1.32, 1.08, 1.31, 1.04), (1.41, 1.51, 1.26, 1.13), (1.67, 1.61, 1.34, 1.11), (2.0, 1.65, 1.38, 1.09), (1.88, 1.85, 1.35, 1.19), (1.7, 1.95, 1.25, 1.16), (1.62, 2.39, 1.23, 1.23), (1.67, 1.87, 1.3, 1.14), (2.01, 1.97, 1.38, 1.2), (2.1, 2.39, 1.38, 1.27)]
Epoch 10 - Train Loss: 0.3220 - Train Acc: 0.8562 - Val Loss: 0.3673 - Val Acc: 0.8710 - Time: 128.96s
Epoch 11/30
Batch 23/115 - Loss: 0.1992 - Accuracy: 0.9688
Batch 46/115 - Loss: 0.3931 - Accuracy: 0.8125
Batch 69/115 - Loss: 0.6819 - Accuracy: 0.7188
Batch 92/115 - Loss: 0.2520 - Accuracy: 0.9375
Batch 115/115 - Loss: 0.2964 - Accuracy: 0.8500
Scalings = [(0.75, 0.65, 1.17, 1.07), (0.56, 0.53, 0.96, 1.0), (0.48, 0.71, 1.01, 1.06), (0.71, 0.75, 1.08, 1.05), (0.75, 0.69, 1.06, 1.01), (0.66, 0.67, 1.02, 1.0), (0.98, 0.66, 1.2, 0.99), (0.82, 0.73, 1.05, 1.02), (1.01, 0.92, 1.13, 1.04), (0.97, 0.86, 1.14, 1.0), (1.17, 0.85, 1.19, 1.02), (1.22, 0.96, 1.2, 1.03), (1.11, 0.96, 1.18, 1.03), (1.21, 1.3, 1.19, 1.1), (1.41, 1.1, 1.32, 1.04), (1.5, 1.52, 1.28, 1.12), (1.76, 1.62, 1.35, 1.11), (2.08, 1.68, 1.4, 1.09), (2.0, 1.91, 1.37, 1.2), (1.78, 2.03, 1.26, 1.17), (1.69, 2.51, 1.24, 1.25), (1.78, 1.92, 1.33, 1.14), (2.1, 2.04, 1.4, 1.21), (2.25, 2.45, 1.4, 1.27)]
Epoch 11 - Train Loss: 0.3292 - Train Acc: 0.8593 - Val Loss: 0.3561 - Val Acc: 0.8662 - Time: 128.58s
Epoch 12/30
Batch 23/115 - Loss: 0.3781 - Accuracy: 0.8438
Batch 46/115 - Loss: 0.4241 - Accuracy: 0.7188
Batch 69/115 - Loss: 0.1968 - Accuracy: 0.8750
Batch 92/115 - Loss: 0.2270 - Accuracy: 0.9062
Batch 115/115 - Loss: 0.2756 - Accuracy: 0.9000
Scalings = [(0.8, 0.69, 1.18, 1.07), (0.6, 0.58, 0.96, 1.0), (0.52, 0.75, 1.01, 1.06), (0.73, 0.78, 1.07, 1.05), (0.82, 0.73, 1.08, 1.01), (0.72, 0.72, 1.02, 1.0), (1.02, 0.7, 1.21, 0.99), (0.88, 0.78, 1.06, 1.02), (1.08, 0.97, 1.14, 1.04), (1.05, 0.9, 1.15, 1.0), (1.25, 0.89, 1.21, 1.01), (1.28, 1.02, 1.2, 1.03), (1.2, 1.02, 1.2, 1.03), (1.27, 1.33, 1.2, 1.09), (1.45, 1.15, 1.32, 1.04), (1.6, 1.55, 1.31, 1.12), (1.81, 1.65, 1.35, 1.1), (2.17, 1.74, 1.42, 1.1), (2.07, 1.99, 1.39, 1.22), (1.85, 2.08, 1.27, 1.17), (1.73, 2.56, 1.24, 1.25), (1.83, 2.0, 1.33, 1.16), (2.14, 2.1, 1.4, 1.22), (2.34, 2.49, 1.41, 1.27)]
Epoch 12 - Train Loss: 0.3145 - Train Acc: 0.8649 - Val Loss: 0.3092 - Val Acc: 0.8782 - Time: 127.96s
Epoch 13/30
Batch 23/115 - Loss: 0.2870 - Accuracy: 0.9062
Batch 46/115 - Loss: 0.2063 - Accuracy: 0.9062
Batch 69/115 - Loss: 0.2553 - Accuracy: 0.9062
Batch 92/115 - Loss: 0.2877 - Accuracy: 0.8438
Batch 115/115 - Loss: 0.1484 - Accuracy: 1.0000
Scalings = [(0.85, 0.73, 1.2, 1.09), (0.61, 0.61, 0.95, 1.01), (0.56, 0.76, 1.02, 1.05), (0.77, 0.8, 1.07, 1.05), (0.86, 0.73, 1.09, 1.01), (0.75, 0.72, 1.02, 0.99), (1.05, 0.71, 1.21, 0.98), (0.89, 0.8, 1.05, 1.01), (1.1, 0.96, 1.14, 1.03), (1.08, 0.95, 1.15, 1.0), (1.31, 0.92, 1.22, 1.01), (1.31, 1.04, 1.2, 1.03), (1.24, 1.03, 1.21, 1.03), (1.31, 1.34, 1.2, 1.08), (1.52, 1.17, 1.33, 1.04), (1.71, 1.55, 1.34, 1.12), (1.85, 1.67, 1.36, 1.1), (2.21, 1.76, 1.43, 1.09), (2.08, 2.03, 1.39, 1.22), (1.88, 2.11, 1.27, 1.17), (1.81, 2.59, 1.26, 1.26), (1.88, 2.02, 1.34, 1.16), (2.16, 2.11, 1.4, 1.22), (2.36, 2.52, 1.41, 1.27)]
Epoch 13 - Train Loss: 0.3134 - Train Acc: 0.8652 - Val Loss: 0.3374 - Val Acc: 0.8710 - Time: 128.37s
Epoch 14/30
Batch 23/115 - Loss: 0.3033 - Accuracy: 0.9062
Batch 46/115 - Loss: 0.3294 - Accuracy: 0.7812
Batch 69/115 - Loss: 0.2710 - Accuracy: 0.9375
Batch 92/115 - Loss: 0.3193 - Accuracy: 0.8750
Batch 115/115 - Loss: 0.2511 - Accuracy: 0.8500
Scalings = [(0.92, 0.81, 1.22, 1.11), (0.64, 0.68, 0.93, 1.0), (0.67, 0.81, 1.01, 1.05), (0.83, 0.81, 1.08, 1.04), (0.97, 0.77, 1.12, 1.0), (0.8, 0.72, 1.03, 0.98), (1.1, 0.71, 1.21, 0.97), (0.95, 0.83, 1.06, 1.01), (1.13, 0.97, 1.14, 1.03), (1.16, 0.98, 1.17, 0.99), (1.38, 0.95, 1.24, 1.01), (1.35, 1.08, 1.21, 1.03), (1.31, 1.05, 1.21, 1.03), (1.38, 1.38, 1.22, 1.09), (1.57, 1.2, 1.34, 1.04), (1.78, 1.57, 1.35, 1.12), (1.88, 1.71, 1.36, 1.11), (2.22, 1.81, 1.42, 1.1), (2.11, 2.07, 1.39, 1.23), (1.94, 2.15, 1.28, 1.17), (1.83, 2.63, 1.26, 1.26), (1.95, 2.04, 1.35, 1.16), (2.22, 2.17, 1.41, 1.23), (2.39, 2.58, 1.42, 1.28)]
Epoch 14 - Train Loss: 0.3024 - Train Acc: 0.8745 - Val Loss: 0.3261 - Val Acc: 0.8814 - Time: 128.66s
Epoch 15/30
Batch 23/115 - Loss: 0.1669 - Accuracy: 0.9375
Batch 46/115 - Loss: 0.2989 - Accuracy: 0.8125
Batch 69/115 - Loss: 0.2036 - Accuracy: 0.9375
Batch 92/115 - Loss: 0.2954 - Accuracy: 0.8438
Batch 115/115 - Loss: 0.2123 - Accuracy: 0.8500
Scalings = [(0.95, 0.85, 1.23, 1.12), (0.67, 0.69, 0.93, 1.0), (0.69, 0.84, 1.02, 1.06), (0.84, 0.86, 1.08, 1.04), (1.0, 0.8, 1.12, 1.0), (0.82, 0.74, 1.03, 0.97), (1.11, 0.74, 1.21, 0.97), (0.99, 0.88, 1.07, 1.01), (1.17, 1.0, 1.14, 1.03), (1.17, 1.0, 1.16, 0.99), (1.41, 0.96, 1.24, 1.01), (1.4, 1.1, 1.22, 1.02), (1.37, 1.08, 1.23, 1.03), (1.44, 1.41, 1.24, 1.09), (1.62, 1.22, 1.35, 1.04), (1.83, 1.57, 1.36, 1.12), (1.95, 1.75, 1.37, 1.11), (2.27, 1.85, 1.44, 1.1), (2.14, 2.11, 1.39, 1.23), (1.98, 2.18, 1.29, 1.17), (1.85, 2.68, 1.26, 1.27), (1.94, 2.09, 1.35, 1.16), (2.23, 2.24, 1.41, 1.24), (2.41, 2.64, 1.42, 1.28)]
Epoch 15 - Train Loss: 0.3032 - Train Acc: 0.8751 - Val Loss: 0.3154 - Val Acc: 0.8806 - Time: 130.31s
Epoch 16/30
Batch 23/115 - Loss: 0.5894 - Accuracy: 0.7500
Batch 46/115 - Loss: 0.4056 - Accuracy: 0.8438
Batch 69/115 - Loss: 0.3643 - Accuracy: 0.8125
Batch 92/115 - Loss: 0.4176 - Accuracy: 0.7812
Batch 115/115 - Loss: 0.2294 - Accuracy: 0.9000
Scalings = [(0.97, 0.89, 1.24, 1.14), (0.69, 0.71, 0.93, 1.0), (0.71, 0.83, 1.02, 1.05), (0.87, 0.86, 1.08, 1.04), (1.03, 0.8, 1.12, 0.99), (0.85, 0.78, 1.03, 0.98), (1.15, 0.75, 1.21, 0.97), (1.01, 0.88, 1.07, 1.01), (1.2, 1.0, 1.15, 1.02), (1.19, 1.03, 1.16, 0.98), (1.44, 0.97, 1.24, 1.0), (1.44, 1.15, 1.23, 1.03), (1.42, 1.09, 1.24, 1.02), (1.47, 1.43, 1.24, 1.09), (1.68, 1.25, 1.36, 1.04), (1.89, 1.58, 1.38, 1.11), (1.98, 1.78, 1.38, 1.11), (2.28, 1.87, 1.44, 1.1), (2.17, 2.14, 1.4, 1.24), (2.0, 2.2, 1.29, 1.17), (1.9, 2.7, 1.27, 1.27), (1.98, 2.11, 1.35, 1.16), (2.28, 2.27, 1.42, 1.25), (2.44, 2.65, 1.42, 1.28)]
Epoch 16 - Train Loss: 0.2993 - Train Acc: 0.8774 - Val Loss: 0.3328 - Val Acc: 0.8718 - Time: 131.08s
Epoch 17/30
Batch 23/115 - Loss: 0.3000 - Accuracy: 0.8125
Batch 46/115 - Loss: 0.3071 - Accuracy: 0.8750
Batch 69/115 - Loss: 0.3583 - Accuracy: 0.9062
Batch 92/115 - Loss: 0.2185 - Accuracy: 0.9062
Batch 115/115 - Loss: 0.2951 - Accuracy: 0.9000
Scalings = [(1.0, 0.91, 1.25, 1.14), (0.71, 0.71, 0.93, 0.99), (0.72, 0.86, 1.01, 1.05), (0.89, 0.88, 1.08, 1.04), (1.04, 0.81, 1.12, 0.99), (0.86, 0.82, 1.03, 0.98), (1.19, 0.77, 1.22, 0.97), (1.02, 0.91, 1.07, 1.01), (1.26, 1.02, 1.16, 1.02), (1.25, 1.05, 1.17, 0.98), (1.49, 1.0, 1.25, 1.01), (1.49, 1.17, 1.24, 1.03), (1.45, 1.11, 1.25, 1.02), (1.52, 1.45, 1.25, 1.09), (1.71, 1.3, 1.36, 1.05), (1.95, 1.6, 1.4, 1.11), (1.99, 1.8, 1.38, 1.11), (2.29, 1.88, 1.44, 1.1), (2.2, 2.17, 1.4, 1.24), (2.02, 2.23, 1.29, 1.18), (1.94, 2.72, 1.28, 1.27), (2.01, 2.12, 1.36, 1.16), (2.29, 2.31, 1.42, 1.25), (2.44, 2.7, 1.42, 1.28)]
Epoch 17 - Train Loss: 0.2903 - Train Acc: 0.8798 - Val Loss: 0.3334 - Val Acc: 0.8710 - Time: 132.27s
Epoch 18/30
Batch 23/115 - Loss: 0.3783 - Accuracy: 0.8125
Batch 46/115 - Loss: 0.3433 - Accuracy: 0.8438
Batch 69/115 - Loss: 0.2727 - Accuracy: 0.9062
Batch 92/115 - Loss: 0.2227 - Accuracy: 0.9062
Batch 115/115 - Loss: 0.1179 - Accuracy: 1.0000
Scalings = [(1.02, 0.93, 1.26, 1.15), (0.72, 0.73, 0.93, 1.0), (0.72, 0.89, 1.01, 1.06), (0.91, 0.9, 1.09, 1.04), (1.07, 0.81, 1.12, 0.98), (0.89, 0.84, 1.03, 0.98), (1.19, 0.8, 1.22, 0.97), (1.06, 0.93, 1.07, 1.0), (1.29, 1.04, 1.17, 1.02), (1.26, 1.08, 1.17, 0.98), (1.53, 1.01, 1.25, 1.0), (1.56, 1.17, 1.25, 1.02), (1.5, 1.11, 1.25, 1.01), (1.55, 1.47, 1.26, 1.09), (1.75, 1.3, 1.36, 1.04), (2.01, 1.63, 1.42, 1.12), (2.02, 1.81, 1.38, 1.11), (2.33, 1.9, 1.45, 1.1), (2.22, 2.2, 1.41, 1.25), (2.06, 2.27, 1.3, 1.18), (2.0, 2.76, 1.29, 1.28), (2.05, 2.13, 1.37, 1.17), (2.33, 2.36, 1.43, 1.26), (2.47, 2.71, 1.42, 1.28)]
Epoch 18 - Train Loss: 0.2860 - Train Acc: 0.8788 - Val Loss: 0.3321 - Val Acc: 0.8790 - Time: 130.16s
Epoch 19/30
Batch 23/115 - Loss: 0.5562 - Accuracy: 0.7500
Batch 46/115 - Loss: 0.1817 - Accuracy: 0.8750
Batch 69/115 - Loss: 0.4674 - Accuracy: 0.8125
Batch 92/115 - Loss: 0.1361 - Accuracy: 0.9688
Batch 115/115 - Loss: 0.2778 - Accuracy: 0.8500
Scalings = [(1.05, 0.95, 1.27, 1.16), (0.74, 0.75, 0.94, 1.0), (0.73, 0.89, 1.01, 1.05), (0.94, 0.87, 1.09, 1.02), (1.08, 0.81, 1.12, 0.98), (0.9, 0.84, 1.03, 0.97), (1.19, 0.8, 1.21, 0.96), (1.07, 0.94, 1.07, 1.0), (1.32, 1.08, 1.17, 1.03), (1.3, 1.11, 1.18, 0.98), (1.56, 1.03, 1.26, 1.0), (1.58, 1.2, 1.26, 1.03), (1.54, 1.13, 1.27, 1.01), (1.59, 1.5, 1.27, 1.09), (1.77, 1.32, 1.36, 1.04), (2.05, 1.64, 1.43, 1.12), (2.04, 1.83, 1.39, 1.11), (2.35, 1.93, 1.45, 1.1), (2.23, 2.22, 1.41, 1.25), (2.08, 2.29, 1.3, 1.18), (2.02, 2.79, 1.3, 1.28), (2.08, 2.15, 1.37, 1.17), (2.38, 2.39, 1.44, 1.26), (2.52, 2.75, 1.43, 1.28)]
Epoch 19 - Train Loss: 0.2889 - Train Acc: 0.8751 - Val Loss: 0.3132 - Val Acc: 0.8830 - Time: 129.48s
Epoch 20/30
Batch 23/115 - Loss: 0.2175 - Accuracy: 0.9375
Batch 46/115 - Loss: 0.2122 - Accuracy: 0.8750
Batch 69/115 - Loss: 0.4119 - Accuracy: 0.8125
Batch 92/115 - Loss: 0.4552 - Accuracy: 0.7188
Batch 115/115 - Loss: 0.3088 - Accuracy: 0.9000
Scalings = [(1.07, 0.98, 1.29, 1.17), (0.74, 0.76, 0.93, 1.0), (0.74, 0.9, 1.01, 1.05), (0.95, 0.87, 1.09, 1.02), (1.09, 0.82, 1.12, 0.97), (0.92, 0.85, 1.04, 0.97), (1.2, 0.82, 1.21, 0.97), (1.09, 0.95, 1.08, 1.0), (1.35, 1.11, 1.18, 1.03), (1.32, 1.12, 1.19, 0.98), (1.59, 1.07, 1.26, 1.01), (1.59, 1.23, 1.25, 1.03), (1.57, 1.14, 1.27, 1.01), (1.6, 1.52, 1.27, 1.09), (1.79, 1.32, 1.37, 1.03), (2.07, 1.64, 1.43, 1.11), (2.07, 1.84, 1.39, 1.11), (2.36, 1.95, 1.45, 1.1), (2.26, 2.26, 1.41, 1.26), (2.11, 2.32, 1.3, 1.19), (2.05, 2.81, 1.3, 1.28), (2.1, 2.15, 1.38, 1.17), (2.39, 2.42, 1.44, 1.27), (2.54, 2.77, 1.43, 1.29)]
Epoch 20 - Train Loss: 0.2954 - Train Acc: 0.8779 - Val Loss: 0.3274 - Val Acc: 0.8766 - Time: 128.25s
Epoch 21/30
Batch 23/115 - Loss: 0.2252 - Accuracy: 0.9375
Batch 46/115 - Loss: 0.1919 - Accuracy: 0.9375
Batch 69/115 - Loss: 0.2057 - Accuracy: 0.9062
Batch 92/115 - Loss: 0.2104 - Accuracy: 0.9062
Batch 115/115 - Loss: 0.6581 - Accuracy: 0.7000
Scalings = [(1.09, 1.0, 1.3, 1.18), (0.75, 0.77, 0.93, 1.0), (0.74, 0.89, 1.01, 1.05), (0.96, 0.89, 1.1, 1.02), (1.11, 0.83, 1.12, 0.97), (0.93, 0.85, 1.04, 0.97), (1.2, 0.83, 1.21, 0.96), (1.12, 0.98, 1.08, 1.0), (1.36, 1.1, 1.18, 1.03), (1.34, 1.14, 1.19, 0.98), (1.61, 1.09, 1.27, 1.01), (1.63, 1.25, 1.26, 1.03), (1.59, 1.16, 1.28, 1.01), (1.62, 1.54, 1.27, 1.1), (1.81, 1.34, 1.37, 1.04), (2.1, 1.66, 1.44, 1.12), (2.11, 1.85, 1.4, 1.11), (2.36, 1.97, 1.45, 1.11), (2.27, 2.27, 1.41, 1.26), (2.14, 2.35, 1.31, 1.19), (2.08, 2.84, 1.31, 1.28), (2.12, 2.17, 1.38, 1.17), (2.4, 2.44, 1.44, 1.27), (2.54, 2.78, 1.43, 1.29)]
Epoch 21 - Train Loss: 0.2756 - Train Acc: 0.8795 - Val Loss: 0.3267 - Val Acc: 0.8886 - Time: 128.45s
Epoch 22/30
Batch 23/115 - Loss: 0.3500 - Accuracy: 0.8125
Batch 46/115 - Loss: 0.2752 - Accuracy: 0.8750
Batch 69/115 - Loss: 0.5682 - Accuracy: 0.7188
Batch 92/115 - Loss: 0.2847 - Accuracy: 0.8750
Batch 115/115 - Loss: 0.2029 - Accuracy: 0.8500
Scalings = [(1.11, 1.01, 1.31, 1.19), (0.75, 0.77, 0.93, 1.0), (0.74, 0.88, 1.01, 1.04), (0.97, 0.91, 1.1, 1.03), (1.12, 0.84, 1.12, 0.97), (0.94, 0.87, 1.04, 0.97), (1.21, 0.83, 1.21, 0.96), (1.11, 0.97, 1.08, 1.0), (1.37, 1.1, 1.18, 1.02), (1.35, 1.16, 1.19, 0.99), (1.61, 1.09, 1.26, 1.01), (1.64, 1.26, 1.26, 1.03), (1.61, 1.17, 1.28, 1.01), (1.63, 1.56, 1.28, 1.1), (1.82, 1.35, 1.37, 1.04), (2.12, 1.67, 1.44, 1.12), (2.12, 1.87, 1.4, 1.11), (2.39, 1.98, 1.46, 1.11), (2.27, 2.29, 1.41, 1.26), (2.17, 2.37, 1.31, 1.19), (2.11, 2.85, 1.32, 1.29), (2.13, 2.17, 1.38, 1.17), (2.41, 2.45, 1.44, 1.27), (2.56, 2.79, 1.43, 1.29)]
Epoch 22 - Train Loss: 0.2823 - Train Acc: 0.8813 - Val Loss: 0.3313 - Val Acc: 0.8814 - Time: 131.99s
Epoch 23/30
Batch 23/115 - Loss: 0.3039 - Accuracy: 0.8438
Batch 46/115 - Loss: 0.1808 - Accuracy: 0.9062
Batch 69/115 - Loss: 0.2534 - Accuracy: 0.8750
Batch 92/115 - Loss: 0.3531 - Accuracy: 0.8125
Batch 115/115 - Loss: 0.2936 - Accuracy: 0.8000
Scalings = [(1.12, 1.03, 1.31, 1.19), (0.77, 0.77, 0.93, 0.99), (0.75, 0.91, 1.01, 1.04), (1.01, 0.91, 1.11, 1.02), (1.12, 0.85, 1.12, 0.97), (0.95, 0.88, 1.04, 0.97), (1.24, 0.85, 1.22, 0.96), (1.12, 0.98, 1.08, 1.0), (1.39, 1.12, 1.19, 1.03), (1.37, 1.17, 1.2, 0.99), (1.63, 1.1, 1.27, 1.01), (1.64, 1.27, 1.26, 1.03), (1.63, 1.18, 1.28, 1.01), (1.65, 1.58, 1.28, 1.1), (1.83, 1.36, 1.37, 1.04), (2.15, 1.69, 1.45, 1.12), (2.13, 1.88, 1.4, 1.11), (2.4, 2.0, 1.46, 1.11), (2.29, 2.31, 1.41, 1.26), (2.17, 2.39, 1.31, 1.19), (2.11, 2.87, 1.31, 1.29), (2.15, 2.18, 1.38, 1.17), (2.42, 2.47, 1.45, 1.28), (2.56, 2.82, 1.43, 1.29)]
Epoch 23 - Train Loss: 0.2724 - Train Acc: 0.8907 - Val Loss: 0.3187 - Val Acc: 0.8862 - Time: 130.29s
Epoch 24/30
Batch 23/115 - Loss: 0.2142 - Accuracy: 0.9062
Batch 46/115 - Loss: 0.1448 - Accuracy: 1.0000
Batch 69/115 - Loss: 0.1309 - Accuracy: 1.0000
Batch 92/115 - Loss: 0.1989 - Accuracy: 0.8750
Batch 115/115 - Loss: 0.2261 - Accuracy: 0.9000
Scalings = [(1.13, 1.04, 1.31, 1.2), (0.77, 0.77, 0.93, 0.99), (0.75, 0.91, 1.01, 1.04), (1.02, 0.93, 1.11, 1.03), (1.13, 0.85, 1.12, 0.97), (0.95, 0.88, 1.04, 0.97), (1.25, 0.85, 1.22, 0.96), (1.13, 0.99, 1.08, 1.0), (1.41, 1.12, 1.19, 1.03), (1.37, 1.19, 1.19, 0.99), (1.65, 1.11, 1.27, 1.01), (1.65, 1.27, 1.26, 1.03), (1.64, 1.18, 1.29, 1.01), (1.66, 1.59, 1.28, 1.1), (1.85, 1.36, 1.37, 1.04), (2.17, 1.68, 1.46, 1.11), (2.15, 1.88, 1.4, 1.11), (2.41, 2.01, 1.46, 1.11), (2.3, 2.32, 1.41, 1.26), (2.16, 2.39, 1.31, 1.19), (2.12, 2.87, 1.31, 1.29), (2.17, 2.18, 1.39, 1.17), (2.43, 2.47, 1.45, 1.28), (2.55, 2.81, 1.43, 1.29)]
Epoch 24 - Train Loss: 0.2634 - Train Acc: 0.8888 - Val Loss: 0.3342 - Val Acc: 0.8838 - Time: 128.43s
Epoch 25/30
Batch 23/115 - Loss: 0.1205 - Accuracy: 0.9688
Batch 46/115 - Loss: 0.2449 - Accuracy: 0.9062
Batch 69/115 - Loss: 0.1132 - Accuracy: 0.9688
Batch 92/115 - Loss: 0.3517 - Accuracy: 0.8125
Batch 115/115 - Loss: 0.5296 - Accuracy: 0.9000
Scalings = [(1.14, 1.05, 1.32, 1.2), (0.79, 0.78, 0.93, 0.99), (0.76, 0.9, 1.01, 1.04), (1.02, 0.93, 1.11, 1.03), (1.13, 0.86, 1.12, 0.97), (0.97, 0.88, 1.04, 0.96), (1.26, 0.84, 1.22, 0.95), (1.13, 1.0, 1.08, 1.0), (1.41, 1.13, 1.19, 1.03), (1.39, 1.19, 1.2, 0.99), (1.66, 1.12, 1.27, 1.01), (1.66, 1.28, 1.26, 1.03), (1.65, 1.19, 1.29, 1.01), (1.67, 1.59, 1.28, 1.1), (1.86, 1.38, 1.38, 1.04), (2.19, 1.69, 1.47, 1.12), (2.16, 1.88, 1.41, 1.11), (2.41, 2.01, 1.46, 1.11), (2.31, 2.33, 1.42, 1.27), (2.17, 2.41, 1.31, 1.19), (2.13, 2.88, 1.32, 1.29), (2.17, 2.19, 1.39, 1.17), (2.43, 2.49, 1.45, 1.28), (2.57, 2.83, 1.43, 1.29)]
Epoch 25 - Train Loss: 0.2731 - Train Acc: 0.8869 - Val Loss: 0.3261 - Val Acc: 0.8886 - Time: 129.39s
Epoch 26/30
Batch 23/115 - Loss: 0.3751 - Accuracy: 0.8750
Batch 46/115 - Loss: 0.3527 - Accuracy: 0.7812
Batch 69/115 - Loss: 0.3436 - Accuracy: 0.8750
Batch 92/115 - Loss: 0.4118 - Accuracy: 0.8750
Batch 115/115 - Loss: 0.1077 - Accuracy: 1.0000
Scalings = [(1.15, 1.06, 1.33, 1.21), (0.79, 0.78, 0.93, 0.99), (0.76, 0.91, 1.01, 1.04), (1.03, 0.93, 1.11, 1.02), (1.14, 0.87, 1.12, 0.97), (0.97, 0.88, 1.05, 0.96), (1.26, 0.85, 1.22, 0.96), (1.13, 1.01, 1.08, 1.0), (1.43, 1.13, 1.2, 1.03), (1.39, 1.19, 1.2, 0.98), (1.66, 1.12, 1.27, 1.01), (1.66, 1.28, 1.26, 1.03), (1.66, 1.19, 1.29, 1.01), (1.69, 1.6, 1.29, 1.1), (1.87, 1.38, 1.38, 1.04), (2.2, 1.69, 1.47, 1.12), (2.17, 1.89, 1.41, 1.11), (2.43, 2.02, 1.47, 1.11), (2.32, 2.35, 1.42, 1.27), (2.19, 2.41, 1.31, 1.19), (2.14, 2.89, 1.32, 1.29), (2.17, 2.2, 1.39, 1.17), (2.43, 2.49, 1.44, 1.28), (2.58, 2.83, 1.43, 1.29)]
Epoch 26 - Train Loss: 0.2732 - Train Acc: 0.8832 - Val Loss: 0.3318 - Val Acc: 0.8742 - Time: 127.95s
Epoch 27/30
Batch 23/115 - Loss: 0.4682 - Accuracy: 0.8438
Batch 46/115 - Loss: 0.3033 - Accuracy: 0.8750
Batch 69/115 - Loss: 0.1474 - Accuracy: 0.9688
Batch 92/115 - Loss: 0.2253 - Accuracy: 0.9062
Batch 115/115 - Loss: 0.3084 - Accuracy: 0.8500
Scalings = [(1.15, 1.07, 1.33, 1.21), (0.8, 0.79, 0.94, 0.99), (0.76, 0.9, 1.01, 1.04), (1.03, 0.92, 1.11, 1.02), (1.13, 0.88, 1.12, 0.97), (0.98, 0.88, 1.05, 0.96), (1.27, 0.85, 1.22, 0.96), (1.14, 1.01, 1.08, 1.0), (1.43, 1.14, 1.2, 1.03), (1.4, 1.2, 1.2, 0.98), (1.66, 1.13, 1.27, 1.01), (1.67, 1.29, 1.26, 1.03), (1.67, 1.21, 1.29, 1.01), (1.69, 1.6, 1.29, 1.1), (1.87, 1.39, 1.38, 1.04), (2.21, 1.7, 1.47, 1.12), (2.17, 1.9, 1.41, 1.11), (2.44, 2.03, 1.47, 1.11), (2.32, 2.36, 1.42, 1.27), (2.2, 2.42, 1.31, 1.2), (2.15, 2.9, 1.32, 1.29), (2.18, 2.2, 1.39, 1.17), (2.43, 2.5, 1.45, 1.28), (2.58, 2.84, 1.43, 1.29)]
Epoch 27 - Train Loss: 0.2688 - Train Acc: 0.8895 - Val Loss: 0.3315 - Val Acc: 0.8718 - Time: 127.55s
Epoch 28/30
Batch 23/115 - Loss: 0.2160 - Accuracy: 0.8750
Batch 46/115 - Loss: 0.3499 - Accuracy: 0.8438
Batch 69/115 - Loss: 0.3617 - Accuracy: 0.8438
Batch 92/115 - Loss: 0.2332 - Accuracy: 0.9375
Batch 115/115 - Loss: 0.3162 - Accuracy: 0.9000
Scalings = [(1.15, 1.07, 1.33, 1.21), (0.8, 0.79, 0.93, 0.99), (0.76, 0.91, 1.01, 1.04), (1.03, 0.93, 1.11, 1.02), (1.13, 0.88, 1.12, 0.97), (0.98, 0.88, 1.05, 0.96), (1.27, 0.86, 1.22, 0.96), (1.14, 1.01, 1.08, 1.0), (1.44, 1.14, 1.2, 1.03), (1.4, 1.2, 1.2, 0.98), (1.67, 1.13, 1.27, 1.01), (1.67, 1.29, 1.26, 1.03), (1.67, 1.21, 1.29, 1.01), (1.7, 1.6, 1.29, 1.1), (1.88, 1.4, 1.38, 1.04), (2.22, 1.71, 1.47, 1.12), (2.18, 1.9, 1.41, 1.11), (2.44, 2.04, 1.47, 1.11), (2.33, 2.36, 1.42, 1.27), (2.2, 2.43, 1.31, 1.2), (2.16, 2.9, 1.32, 1.29), (2.18, 2.2, 1.39, 1.17), (2.43, 2.51, 1.45, 1.28), (2.59, 2.85, 1.44, 1.29)]
Epoch 28 - Train Loss: 0.2663 - Train Acc: 0.8883 - Val Loss: 0.3171 - Val Acc: 0.8838 - Time: 128.32s
Epoch 29/30
Batch 23/115 - Loss: 0.3188 - Accuracy: 0.8750
Batch 46/115 - Loss: 0.1912 - Accuracy: 0.9375
Batch 69/115 - Loss: 0.1856 - Accuracy: 0.8750
Batch 92/115 - Loss: 0.3665 - Accuracy: 0.7812
Batch 115/115 - Loss: 0.2676 - Accuracy: 0.8500
Scalings = [(1.16, 1.07, 1.33, 1.21), (0.8, 0.79, 0.94, 0.99), (0.76, 0.91, 1.01, 1.04), (1.04, 0.93, 1.11, 1.02), (1.14, 0.88, 1.12, 0.97), (0.99, 0.88, 1.05, 0.96), (1.27, 0.86, 1.22, 0.96), (1.14, 1.02, 1.08, 1.0), (1.44, 1.14, 1.2, 1.03), (1.4, 1.2, 1.2, 0.98), (1.67, 1.14, 1.27, 1.01), (1.67, 1.29, 1.27, 1.03), (1.68, 1.21, 1.29, 1.01), (1.71, 1.6, 1.29, 1.1), (1.88, 1.4, 1.38, 1.04), (2.22, 1.71, 1.47, 1.12), (2.18, 1.9, 1.41, 1.11), (2.44, 2.04, 1.47, 1.11), (2.33, 2.36, 1.42, 1.27), (2.21, 2.43, 1.32, 1.2), (2.16, 2.9, 1.32, 1.29), (2.19, 2.2, 1.39, 1.17), (2.43, 2.51, 1.45, 1.28), (2.59, 2.85, 1.44, 1.29)]
Epoch 29 - Train Loss: 0.2598 - Train Acc: 0.8952 - Val Loss: 0.3219 - Val Acc: 0.8838 - Time: 128.25s
Epoch 30/30
Batch 23/115 - Loss: 0.2327 - Accuracy: 0.8750
Batch 46/115 - Loss: 0.2611 - Accuracy: 0.8438
Batch 69/115 - Loss: 0.3198 - Accuracy: 0.8438
Batch 92/115 - Loss: 0.2563 - Accuracy: 0.8750
Batch 115/115 - Loss: 0.1146 - Accuracy: 0.9500
Scalings = [(1.16, 1.07, 1.33, 1.21), (0.8, 0.79, 0.94, 0.99), (0.76, 0.91, 1.01, 1.04), (1.04, 0.93, 1.11, 1.02), (1.14, 0.88, 1.12, 0.97), (0.99, 0.88, 1.05, 0.96), (1.27, 0.86, 1.22, 0.96), (1.14, 1.02, 1.08, 1.0), (1.44, 1.14, 1.2, 1.03), (1.41, 1.21, 1.2, 0.98), (1.67, 1.14, 1.27, 1.01), (1.67, 1.29, 1.27, 1.03), (1.68, 1.21, 1.3, 1.01), (1.71, 1.6, 1.29, 1.1), (1.88, 1.4, 1.38, 1.04), (2.22, 1.71, 1.47, 1.12), (2.18, 1.9, 1.41, 1.11), (2.44, 2.04, 1.47, 1.11), (2.33, 2.36, 1.42, 1.27), (2.21, 2.43, 1.32, 1.2), (2.16, 2.9, 1.32, 1.29), (2.18, 2.2, 1.39, 1.17), (2.43, 2.51, 1.45, 1.28), (2.59, 2.85, 1.44, 1.29)]
Epoch 30 - Train Loss: 0.2636 - Train Acc: 0.8868 - Val Loss: 0.3234 - Val Acc: 0.8838 - Time: 127.34s
Average Time per Epoch: 128.39s
