Running experiment with data: glue_sst2, model: FacebookAI/roberta-large-mnli, LoRA type: tlora
RobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 1024, padding_idx=1)
      (position_embeddings): Embedding(514, 1024, padding_idx=1)
      (token_type_embeddings): Embedding(1, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-23): 24 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSdpaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHead(
    (dense): Linear(in_features=1024, out_features=1024, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=1024, out_features=3, bias=True)
  )
)
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        51,471,360
│    │    └─Embedding: 3-2                                        526,336
│    │    └─Embedding: 3-3                                        1,024
│    │    └─LayerNorm: 3-4                                        2,048
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       302,309,376
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                1,049,600
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                3,075
==========================================================================================
Total params: 355,362,819
Trainable params: 355,362,819
Non-trainable params: 0
==========================================================================================
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        51,471,360
│    │    └─Embedding: 3-2                                        526,336
│    │    └─Embedding: 3-3                                        1,024
│    │    └─LayerNorm: 3-4                                        2,048
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       302,309,376
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                1,049,600
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                3,075
==========================================================================================
Total params: 355,362,819
Trainable params: 355,362,819
Non-trainable params: 0
==========================================================================================
RobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 1024, padding_idx=1)
      (position_embeddings): Embedding(514, 1024, padding_idx=1)
      (token_type_embeddings): Embedding(1, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-23): 24 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSdpaSelfAttention(
              (query): TLoRALayer(
                (linear): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.5, inplace=False)
              )
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): TLoRALayer(
                (linear): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.5, inplace=False)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHead(
    (dense): Linear(in_features=1024, out_features=1024, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=1024, out_features=3, bias=True)
  )
)
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        (51,471,360)
│    │    └─Embedding: 3-2                                        (526,336)
│    │    └─Embedding: 3-3                                        (1,024)
│    │    └─LayerNorm: 3-4                                        (2,048)
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       305,504,304
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                (1,049,600)
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                (3,075)
==========================================================================================
Total params: 358,557,747
Trainable params: 49,200
Non-trainable params: 358,508,547
==========================================================================================
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        (51,471,360)
│    │    └─Embedding: 3-2                                        (526,336)
│    │    └─Embedding: 3-3                                        (1,024)
│    │    └─LayerNorm: 3-4                                        (2,048)
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       305,504,304
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                (1,049,600)
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                (3,075)
==========================================================================================
Total params: 358,557,747
Trainable params: 49,200
Non-trainable params: 358,508,547
==========================================================================================
Epoch 1/30
Batch 421/2105 - Loss: 0.6049 - Accuracy: 0.7500
Batch 842/2105 - Loss: 0.3374 - Accuracy: 0.8750
Batch 1263/2105 - Loss: 0.1465 - Accuracy: 0.9375
Batch 1684/2105 - Loss: 0.2192 - Accuracy: 0.9688
Batch 2105/2105 - Loss: 0.1194 - Accuracy: 1.0000
Scalings = [(0.52, 0.64, 1.05, 1.18), (0.58, 0.52, 1.1, 1.06), (0.48, 0.52, 1.01, 1.05), (0.5, 0.48, 1.05, 1.03), (0.47, 0.46, 1.04, 1.02), (0.5, 0.43, 1.05, 1.01), (0.55, 0.52, 1.07, 1.05), (0.65, 0.61, 1.1, 1.04), (0.63, 0.63, 1.12, 1.06), (0.69, 0.6, 1.12, 1.06), (0.82, 0.73, 1.18, 1.07), (1.04, 0.59, 1.21, 1.04), (0.85, 0.69, 1.16, 1.09), (1.32, 0.96, 1.3, 1.1), (0.89, 0.83, 1.14, 1.07), (1.22, 0.77, 1.25, 1.07), (0.87, 0.8, 1.17, 1.06), (1.14, 0.99, 1.2, 1.11), (1.25, 0.98, 1.32, 1.13), (1.27, 1.32, 1.3, 1.18), (1.05, 0.98, 1.24, 1.14), (0.95, 1.34, 1.23, 1.19), (0.96, 1.06, 1.22, 1.16), (0.88, 1.18, 1.18, 1.14)]
Epoch 1 - Train Loss: 0.5029 - Train Acc: 0.7885 - Val Loss: 0.2266 - Val Acc: 0.9128 - Val MCC: 0.83
Epoch 2/30
Batch 421/2105 - Loss: 0.1570 - Accuracy: 0.9688
Batch 842/2105 - Loss: 0.1380 - Accuracy: 0.9688
Batch 1263/2105 - Loss: 0.2929 - Accuracy: 0.8750
Batch 1684/2105 - Loss: 0.1962 - Accuracy: 0.9062
Batch 2105/2105 - Loss: 0.4420 - Accuracy: 0.7619
Scalings = [(1.55, 1.46, 1.18, 1.18), (1.36, 1.06, 1.1, 0.89), (1.45, 0.97, 1.11, 0.86), (1.37, 0.88, 1.07, 0.78), (1.36, 0.85, 1.07, 0.82), (1.48, 0.82, 1.09, 0.79), (1.53, 1.04, 1.15, 0.86), (1.35, 1.07, 1.08, 0.83), (1.55, 1.03, 1.19, 0.91), (1.59, 1.0, 1.16, 0.89), (1.69, 1.35, 1.22, 0.97), (1.68, 1.04, 1.22, 0.87), (1.77, 1.15, 1.24, 0.96), (2.22, 1.59, 1.39, 1.02), (1.88, 1.29, 1.22, 0.96), (2.55, 1.3, 1.48, 0.98), (2.11, 1.46, 1.39, 0.96), (2.17, 1.92, 1.34, 1.16), (2.66, 1.89, 1.58, 1.22), (2.77, 2.55, 1.55, 1.36), (2.2, 2.08, 1.44, 1.32), (2.22, 2.54, 1.46, 1.34), (2.22, 2.35, 1.42, 1.48), (2.36, 2.34, 1.44, 1.23)]
Epoch 2 - Train Loss: 0.2668 - Train Acc: 0.8961 - Val Loss: 0.1842 - Val Acc: 0.9300 - Val MCC: 0.86
Epoch 3/30
Batch 421/2105 - Loss: 0.3300 - Accuracy: 0.8438
Batch 842/2105 - Loss: 0.1427 - Accuracy: 0.9688
Batch 1263/2105 - Loss: 0.3140 - Accuracy: 0.9062
Batch 1684/2105 - Loss: 0.2810 - Accuracy: 0.8750
Batch 2105/2105 - Loss: 0.3225 - Accuracy: 0.8571
Scalings = [(2.11, 1.92, 1.16, 1.08), (1.97, 1.37, 1.08, 0.68), (2.0, 1.22, 1.07, 0.62), (1.84, 1.26, 1.0, 0.56), (1.94, 1.21, 1.05, 0.61), (2.22, 1.27, 1.13, 0.61), (2.04, 1.58, 1.12, 0.74), (1.78, 1.53, 0.99, 0.64), (2.22, 1.45, 1.21, 0.75), (2.14, 1.4, 1.14, 0.71), (1.98, 1.79, 1.11, 0.85), (2.21, 1.37, 1.18, 0.66), (2.41, 1.57, 1.27, 0.8), (2.81, 2.07, 1.4, 0.93), (2.74, 1.78, 1.3, 0.82), (3.04, 1.75, 1.46, 0.86), (2.74, 1.98, 1.39, 0.84), (2.85, 2.62, 1.38, 1.21), (3.46, 2.65, 1.65, 1.28), (3.43, 3.38, 1.61, 1.48), (3.07, 2.85, 1.55, 1.42), (2.92, 3.29, 1.51, 1.38), (2.78, 3.28, 1.45, 1.62), (2.92, 2.92, 1.45, 1.23)]
Epoch 3 - Train Loss: 0.2471 - Train Acc: 0.9041 - Val Loss: 0.1885 - Val Acc: 0.9266 - Val MCC: 0.86
Epoch 4/30
Batch 421/2105 - Loss: 0.1352 - Accuracy: 0.9375
Batch 842/2105 - Loss: 0.2953 - Accuracy: 0.8750
Batch 1263/2105 - Loss: 0.2879 - Accuracy: 0.8438
Batch 1684/2105 - Loss: 0.1103 - Accuracy: 0.9688
Batch 2105/2105 - Loss: 0.0571 - Accuracy: 1.0000
Scalings = [(2.49, 2.35, 1.16, 1.08), (2.47, 1.63, 1.12, 0.55), (2.46, 1.53, 1.08, 0.5), (2.3, 1.49, 1.0, 0.43), (2.28, 1.48, 1.01, 0.51), (2.5, 1.52, 1.09, 0.46), (2.33, 1.85, 1.07, 0.57), (2.26, 1.86, 0.98, 0.49), (2.64, 1.74, 1.2, 0.62), (2.48, 1.71, 1.09, 0.57), (2.55, 2.15, 1.14, 0.72), (2.54, 1.64, 1.15, 0.49), (2.68, 1.9, 1.19, 0.66), (3.31, 2.47, 1.43, 0.83), (3.27, 2.16, 1.34, 0.72), (3.4, 2.1, 1.45, 0.77), (3.26, 2.42, 1.44, 0.78), (3.26, 2.99, 1.35, 1.18), (3.71, 3.18, 1.58, 1.31), (3.89, 3.71, 1.64, 1.47), (3.69, 3.45, 1.64, 1.52), (3.35, 3.88, 1.53, 1.4), (3.18, 3.77, 1.46, 1.67), (3.16, 3.25, 1.42, 1.21)]
Epoch 4 - Train Loss: 0.2331 - Train Acc: 0.9087 - Val Loss: 0.1755 - Val Acc: 0.9427 - Val MCC: 0.89
Epoch 5/30
Batch 421/2105 - Loss: 0.0741 - Accuracy: 0.9688
Batch 842/2105 - Loss: 0.2781 - Accuracy: 0.8750
Batch 1263/2105 - Loss: 0.2134 - Accuracy: 0.9375
Batch 1684/2105 - Loss: 0.1149 - Accuracy: 0.9688
Batch 2105/2105 - Loss: 0.4163 - Accuracy: 0.7619
Scalings = [(2.82, 2.62, 1.15, 1.03), (2.71, 1.78, 1.07, 0.43), (2.8, 1.72, 1.07, 0.4), (2.5, 1.73, 0.93, 0.32), (2.49, 1.67, 0.95, 0.41), (2.81, 1.7, 1.07, 0.34), (2.71, 2.12, 1.08, 0.48), (2.51, 2.17, 0.93, 0.42), (2.91, 2.02, 1.17, 0.54), (2.92, 1.95, 1.12, 0.48), (2.91, 2.55, 1.13, 0.67), (2.96, 1.93, 1.18, 0.42), (3.09, 2.17, 1.22, 0.53), (3.73, 2.86, 1.48, 0.78), (3.54, 2.6, 1.32, 0.66), (3.64, 2.44, 1.43, 0.71), (3.65, 2.77, 1.45, 0.71), (3.65, 3.4, 1.36, 1.2), (4.2, 3.69, 1.62, 1.36), (4.24, 4.14, 1.66, 1.54), (3.92, 3.87, 1.61, 1.61), (3.72, 4.15, 1.55, 1.4), (3.19, 4.22, 1.38, 1.71), (3.53, 3.46, 1.46, 1.18)]
Epoch 5 - Train Loss: 0.2230 - Train Acc: 0.9139 - Val Loss: 0.1804 - Val Acc: 0.9415 - Val MCC: 0.88
Epoch 6/30
Batch 421/2105 - Loss: 0.0924 - Accuracy: 0.9688
Batch 842/2105 - Loss: 0.1284 - Accuracy: 0.9375
Batch 1263/2105 - Loss: 0.1378 - Accuracy: 0.9375
Batch 1684/2105 - Loss: 0.1042 - Accuracy: 0.9688
Batch 2105/2105 - Loss: 0.1033 - Accuracy: 0.9524
Scalings = [(3.09, 2.81, 1.17, 0.98), (2.94, 1.99, 1.07, 0.38), (3.0, 2.02, 1.05, 0.37), (2.82, 1.93, 0.97, 0.25), (2.71, 1.85, 0.91, 0.35), (3.07, 1.93, 1.08, 0.28), (2.95, 2.39, 1.05, 0.44), (2.72, 2.46, 0.87, 0.35), (3.07, 2.22, 1.13, 0.46), (3.12, 2.19, 1.1, 0.41), (3.07, 2.91, 1.06, 0.63), (3.21, 2.19, 1.17, 0.34), (3.32, 2.41, 1.21, 0.47), (4.03, 3.16, 1.49, 0.72), (3.84, 2.84, 1.33, 0.57), (3.84, 2.68, 1.41, 0.65), (3.99, 3.03, 1.5, 0.65), (4.04, 3.55, 1.4, 1.15), (4.6, 3.97, 1.65, 1.39), (4.47, 4.34, 1.66, 1.52), (4.29, 4.23, 1.67, 1.65), (3.89, 4.42, 1.53, 1.4), (3.2, 4.44, 1.3, 1.71), (3.52, 3.62, 1.36, 1.15)]
Epoch 6 - Train Loss: 0.2195 - Train Acc: 0.9161 - Val Loss: 0.1918 - Val Acc: 0.9358 - Val MCC: 0.87
Epoch 7/30
Batch 421/2105 - Loss: 0.2318 - Accuracy: 0.9375
Batch 842/2105 - Loss: 0.3850 - Accuracy: 0.8125
Batch 1263/2105 - Loss: 0.0829 - Accuracy: 1.0000
Batch 1684/2105 - Loss: 0.1151 - Accuracy: 0.9688
Batch 2105/2105 - Loss: 0.1705 - Accuracy: 0.9524
Scalings = [(3.19, 3.03, 1.15, 0.95), (3.1, 2.27, 1.01, 0.34), (3.09, 2.26, 1.01, 0.32), (3.02, 2.17, 0.94, 0.22), (2.91, 2.14, 0.88, 0.32), (3.16, 2.15, 1.02, 0.22), (3.19, 2.62, 1.05, 0.38), (2.9, 2.71, 0.84, 0.3), (3.33, 2.46, 1.15, 0.43), (3.4, 2.5, 1.12, 0.38), (3.34, 3.13, 1.06, 0.57), (3.37, 2.42, 1.12, 0.28), (3.69, 2.65, 1.27, 0.39), (4.18, 3.4, 1.46, 0.67), (4.09, 3.11, 1.35, 0.53), (4.01, 3.01, 1.39, 0.64), (4.22, 3.33, 1.5, 0.62), (4.28, 3.76, 1.39, 1.15), (4.78, 4.21, 1.62, 1.4), (4.6, 4.51, 1.63, 1.52), (4.63, 4.47, 1.71, 1.68), (4.19, 4.64, 1.57, 1.4), (3.48, 4.63, 1.31, 1.71), (3.46, 3.75, 1.26, 1.12)]
Epoch 7 - Train Loss: 0.2128 - Train Acc: 0.9174 - Val Loss: 0.1663 - Val Acc: 0.9427 - Val MCC: 0.89
Epoch 8/30
Batch 421/2105 - Loss: 0.1314 - Accuracy: 0.9688
Batch 842/2105 - Loss: 0.1298 - Accuracy: 0.9688
Batch 1263/2105 - Loss: 0.1664 - Accuracy: 0.9688
Batch 1684/2105 - Loss: 0.0817 - Accuracy: 0.9688
Batch 2105/2105 - Loss: 0.2097 - Accuracy: 0.9048
Scalings = [(3.39, 3.22, 1.16, 0.94), (3.23, 2.44, 0.98, 0.29), (3.28, 2.48, 1.0, 0.28), (3.22, 2.48, 0.93, 0.21), (3.12, 2.32, 0.87, 0.27), (3.29, 2.37, 0.97, 0.19), (3.36, 2.7, 1.04, 0.3), (3.08, 3.01, 0.82, 0.29), (3.44, 2.69, 1.08, 0.39), (3.7, 2.68, 1.16, 0.33), (3.62, 3.41, 1.08, 0.56), (3.56, 2.65, 1.13, 0.24), (3.77, 2.86, 1.23, 0.34), (4.43, 3.66, 1.51, 0.65), (4.24, 3.35, 1.33, 0.48), (4.18, 3.17, 1.38, 0.58), (4.44, 3.61, 1.53, 0.61), (4.6, 3.94, 1.45, 1.14), (4.95, 4.37, 1.62, 1.4), (4.82, 4.69, 1.65, 1.53), (4.75, 4.62, 1.68, 1.68), (4.3, 4.76, 1.56, 1.38), (3.5, 4.78, 1.25, 1.69), (3.66, 3.96, 1.25, 1.11)]
Epoch 8 - Train Loss: 0.2090 - Train Acc: 0.9197 - Val Loss: 0.1839 - Val Acc: 0.9381 - Val MCC: 0.88
Epoch 9/30
Batch 421/2105 - Loss: 0.4151 - Accuracy: 0.8750
Batch 842/2105 - Loss: 0.1733 - Accuracy: 0.9688
Batch 1263/2105 - Loss: 0.2415 - Accuracy: 0.8750
Batch 1684/2105 - Loss: 0.1518 - Accuracy: 0.9688
Batch 2105/2105 - Loss: 0.0891 - Accuracy: 1.0000
Scalings = [(3.38, 3.27, 1.09, 0.87), (3.35, 2.56, 0.97, 0.27), (3.37, 2.59, 0.98, 0.25), (3.42, 2.76, 0.95, 0.24), (3.31, 2.37, 0.89, 0.22), (3.45, 2.66, 0.98, 0.21), (3.5, 2.94, 1.02, 0.32), (3.17, 3.16, 0.77, 0.27), (3.63, 2.91, 1.09, 0.37), (3.95, 2.88, 1.19, 0.31), (3.89, 3.62, 1.12, 0.53), (3.73, 2.87, 1.14, 0.21), (3.91, 3.05, 1.2, 0.33), (4.53, 3.89, 1.49, 0.62), (4.5, 3.57, 1.4, 0.45), (4.39, 3.38, 1.4, 0.56), (4.64, 3.84, 1.56, 0.58), (4.77, 4.1, 1.46, 1.14), (5.15, 4.56, 1.63, 1.42), (5.06, 4.84, 1.68, 1.54), (4.86, 4.69, 1.67, 1.65), (4.51, 4.86, 1.59, 1.36), (3.6, 4.91, 1.24, 1.69), (3.7, 4.18, 1.21, 1.1)]
Epoch 9 - Train Loss: 0.2076 - Train Acc: 0.9216 - Val Loss: 0.1741 - Val Acc: 0.9415 - Val MCC: 0.88
Epoch 10/30
Batch 421/2105 - Loss: 0.1382 - Accuracy: 0.9375
Batch 842/2105 - Loss: 0.1608 - Accuracy: 0.9375
Batch 1263/2105 - Loss: 0.2494 - Accuracy: 0.9062
Batch 1684/2105 - Loss: 0.1668 - Accuracy: 0.9688
Batch 2105/2105 - Loss: 0.1340 - Accuracy: 0.9048
Scalings = [(3.4, 3.41, 1.05, 0.88), (3.48, 2.62, 0.96, 0.2), (3.41, 2.72, 0.94, 0.23), (3.47, 2.75, 0.9, 0.17), (3.41, 2.56, 0.86, 0.23), (3.59, 2.72, 0.98, 0.16), (3.65, 3.06, 1.01, 0.28), (3.3, 3.35, 0.76, 0.27), (3.78, 3.07, 1.09, 0.35), (4.05, 3.09, 1.18, 0.31), (4.01, 3.84, 1.11, 0.53), (3.88, 3.13, 1.14, 0.22), (4.07, 3.25, 1.21, 0.32), (4.62, 4.11, 1.48, 0.62), (4.61, 3.82, 1.39, 0.43), (4.52, 3.58, 1.41, 0.58), (4.76, 4.06, 1.56, 0.58), (4.97, 4.25, 1.48, 1.14), (5.32, 4.65, 1.64, 1.41), (5.18, 4.88, 1.67, 1.5), (5.01, 4.76, 1.67, 1.63), (4.47, 5.02, 1.52, 1.35), (3.68, 5.07, 1.21, 1.71), (3.94, 4.34, 1.23, 1.1)]
Epoch 10 - Train Loss: 0.2042 - Train Acc: 0.9226 - Val Loss: 0.1734 - Val Acc: 0.9392 - Val MCC: 0.88
Epoch 11/30
Batch 421/2105 - Loss: 0.1877 - Accuracy: 0.9062
Batch 842/2105 - Loss: 0.3012 - Accuracy: 0.9062
Batch 1263/2105 - Loss: 0.1280 - Accuracy: 0.9375
Batch 1684/2105 - Loss: 0.1953 - Accuracy: 0.9375
Batch 2105/2105 - Loss: 0.0383 - Accuracy: 1.0000
Scalings = [(3.47, 3.52, 1.05, 0.86), (3.63, 2.81, 0.99, 0.21), (3.45, 2.78, 0.91, 0.19), (3.53, 2.87, 0.87, 0.16), (3.56, 2.68, 0.87, 0.21), (3.79, 2.9, 1.02, 0.17), (3.81, 3.21, 1.02, 0.27), (3.56, 3.52, 0.81, 0.25), (3.98, 3.21, 1.12, 0.32), (4.27, 3.23, 1.22, 0.28), (4.18, 3.98, 1.12, 0.51), (4.07, 3.28, 1.19, 0.2), (4.2, 3.43, 1.22, 0.29), (4.74, 4.25, 1.48, 0.6), (4.78, 4.0, 1.41, 0.41), (4.71, 3.72, 1.44, 0.55), (4.94, 4.18, 1.61, 0.57), (5.1, 4.32, 1.49, 1.11), (5.51, 4.76, 1.66, 1.4), (5.34, 4.99, 1.7, 1.51), (5.16, 4.86, 1.68, 1.62), (4.55, 5.17, 1.5, 1.36), (3.81, 5.19, 1.21, 1.72), (4.11, 4.34, 1.24, 1.08)]
Epoch 11 - Train Loss: 0.2057 - Train Acc: 0.9214 - Val Loss: 0.1648 - Val Acc: 0.9461 - Val MCC: 0.89
Epoch 12/30
Batch 421/2105 - Loss: 0.1124 - Accuracy: 0.9375
Batch 842/2105 - Loss: 0.1547 - Accuracy: 0.9375
Batch 1263/2105 - Loss: 0.0924 - Accuracy: 1.0000
Batch 1684/2105 - Loss: 0.1528 - Accuracy: 0.9688
Batch 2105/2105 - Loss: 0.3074 - Accuracy: 0.8571
Scalings = [(3.55, 3.65, 1.05, 0.89), (3.63, 2.94, 0.92, 0.21), (3.6, 2.97, 0.92, 0.21), (3.55, 3.09, 0.83, 0.19), (3.65, 2.85, 0.86, 0.23), (3.9, 3.04, 1.03, 0.16), (3.95, 3.39, 1.02, 0.26), (3.7, 3.74, 0.83, 0.26), (4.11, 3.36, 1.13, 0.31), (4.42, 3.36, 1.24, 0.27), (4.32, 4.15, 1.12, 0.51), (4.2, 3.4, 1.21, 0.19), (4.34, 3.62, 1.24, 0.31), (4.92, 4.45, 1.55, 0.61), (4.9, 4.16, 1.43, 0.41), (4.8, 3.9, 1.42, 0.54), (5.03, 4.32, 1.62, 0.57), (5.26, 4.41, 1.54, 1.12), (5.66, 4.85, 1.68, 1.42), (5.48, 5.08, 1.72, 1.51), (5.28, 4.98, 1.69, 1.64), (4.6, 5.29, 1.48, 1.35), (3.82, 5.3, 1.18, 1.73), (4.12, 4.41, 1.21, 1.07)]
Epoch 12 - Train Loss: 0.1998 - Train Acc: 0.9243 - Val Loss: 0.1739 - Val Acc: 0.9415 - Val MCC: 0.88
Epoch 13/30
Batch 421/2105 - Loss: 0.2118 - Accuracy: 0.9375
Batch 842/2105 - Loss: 0.4294 - Accuracy: 0.7812
Batch 1263/2105 - Loss: 0.1750 - Accuracy: 0.9375
Batch 1684/2105 - Loss: 0.2449 - Accuracy: 0.8438
Batch 2105/2105 - Loss: 0.2631 - Accuracy: 0.9524
Scalings = [(3.67, 3.77, 1.05, 0.91), (3.8, 3.06, 0.97, 0.2), (3.63, 3.14, 0.89, 0.22), (3.67, 3.16, 0.83, 0.16), (3.84, 2.98, 0.91, 0.21), (3.92, 3.14, 1.0, 0.15), (4.02, 3.5, 1.02, 0.26), (3.74, 3.88, 0.8, 0.27), (4.23, 3.5, 1.14, 0.31), (4.44, 3.51, 1.21, 0.25), (4.51, 4.3, 1.18, 0.49), (4.41, 3.55, 1.28, 0.2), (4.39, 3.81, 1.22, 0.3), (5.03, 4.56, 1.56, 0.61), (5.0, 4.28, 1.45, 0.39), (4.9, 4.08, 1.44, 0.57), (5.19, 4.41, 1.65, 0.56), (5.38, 4.49, 1.52, 1.13), (5.78, 4.88, 1.7, 1.4), (5.54, 5.17, 1.7, 1.51), (5.36, 5.04, 1.67, 1.64), (4.67, 5.35, 1.47, 1.35), (3.93, 5.4, 1.17, 1.74), (4.22, 4.54, 1.21, 1.06)]
Epoch 13 - Train Loss: 0.1986 - Train Acc: 0.9244 - Val Loss: 0.1700 - Val Acc: 0.9392 - Val MCC: 0.88
Epoch 14/30
Batch 421/2105 - Loss: 0.1631 - Accuracy: 0.9375
Batch 842/2105 - Loss: 0.1227 - Accuracy: 0.9375
Batch 1263/2105 - Loss: 0.2801 - Accuracy: 0.9375
Batch 1684/2105 - Loss: 0.1723 - Accuracy: 0.9688
Batch 2105/2105 - Loss: 0.2795 - Accuracy: 0.8571
Scalings = [(3.72, 3.84, 1.06, 0.92), (3.89, 3.25, 0.97, 0.23), (3.64, 3.28, 0.86, 0.22), (3.78, 3.25, 0.85, 0.14), (3.91, 3.06, 0.91, 0.2), (4.06, 3.2, 1.02, 0.13), (4.09, 3.62, 1.0, 0.24), (3.77, 4.0, 0.77, 0.25), (4.39, 3.56, 1.18, 0.28), (4.56, 3.62, 1.22, 0.24), (4.6, 4.44, 1.18, 0.5), (4.49, 3.64, 1.28, 0.18), (4.52, 3.91, 1.26, 0.28), (5.1, 4.72, 1.56, 0.62), (5.13, 4.41, 1.48, 0.39), (4.94, 4.14, 1.41, 0.54), (5.31, 4.53, 1.69, 0.55), (5.49, 4.58, 1.56, 1.14), (5.9, 4.95, 1.72, 1.4), (5.65, 5.24, 1.72, 1.52), (5.42, 5.08, 1.66, 1.63), (4.78, 5.37, 1.49, 1.33), (3.99, 5.43, 1.16, 1.73), (4.1, 4.61, 1.12, 1.06)]
Epoch 14 - Train Loss: 0.1983 - Train Acc: 0.9245 - Val Loss: 0.1595 - Val Acc: 0.9438 - Val MCC: 0.89
Epoch 15/30
Batch 421/2105 - Loss: 0.3642 - Accuracy: 0.8750
Batch 842/2105 - Loss: 0.0979 - Accuracy: 0.9688
Batch 1263/2105 - Loss: 0.1523 - Accuracy: 0.9375
Batch 1684/2105 - Loss: 0.1996 - Accuracy: 0.9375
Batch 2105/2105 - Loss: 0.1560 - Accuracy: 0.9048
Scalings = [(3.74, 3.88, 1.04, 0.9), (3.92, 3.35, 0.96, 0.23), (3.75, 3.28, 0.88, 0.18), (3.87, 3.37, 0.85, 0.16), (3.96, 3.12, 0.9, 0.2), (4.12, 3.43, 1.02, 0.16), (4.23, 3.75, 1.04, 0.25), (3.94, 4.17, 0.82, 0.29), (4.46, 3.73, 1.18, 0.29), (4.61, 3.75, 1.21, 0.25), (4.63, 4.52, 1.17, 0.5), (4.54, 3.73, 1.29, 0.17), (4.59, 4.02, 1.27, 0.28), (5.11, 4.78, 1.54, 0.61), (5.2, 4.51, 1.5, 0.39), (4.99, 4.23, 1.42, 0.54), (5.38, 4.65, 1.7, 0.56), (5.59, 4.64, 1.59, 1.15), (5.97, 5.0, 1.73, 1.41), (5.64, 5.27, 1.7, 1.5), (5.54, 5.15, 1.68, 1.65), (4.93, 5.47, 1.53, 1.34), (4.06, 5.54, 1.16, 1.75), (4.16, 4.76, 1.12, 1.07)]
Epoch 15 - Train Loss: 0.1967 - Train Acc: 0.9252 - Val Loss: 0.1536 - Val Acc: 0.9495 - Val MCC: 0.90
Epoch 16/30
Batch 421/2105 - Loss: 0.2743 - Accuracy: 0.8750
Batch 842/2105 - Loss: 0.1020 - Accuracy: 0.9688
Batch 1263/2105 - Loss: 0.2339 - Accuracy: 0.9062
Batch 1684/2105 - Loss: 0.2025 - Accuracy: 0.8750
Batch 2105/2105 - Loss: 0.1156 - Accuracy: 0.9524
Scalings = [(3.78, 3.91, 1.03, 0.89), (3.94, 3.55, 0.94, 0.29), (3.89, 3.38, 0.91, 0.19), (3.95, 3.44, 0.87, 0.15), (3.97, 3.19, 0.88, 0.2), (4.26, 3.43, 1.08, 0.14), (4.39, 3.86, 1.09, 0.25), (4.02, 4.18, 0.82, 0.26), (4.53, 3.81, 1.19, 0.29), (4.66, 3.88, 1.21, 0.26), (4.66, 4.6, 1.16, 0.5), (4.61, 3.85, 1.31, 0.18), (4.66, 4.13, 1.28, 0.3), (5.26, 4.86, 1.61, 0.61), (5.31, 4.6, 1.53, 0.39), (5.09, 4.35, 1.45, 0.55), (5.42, 4.7, 1.71, 0.56), (5.69, 4.66, 1.61, 1.13), (6.04, 5.02, 1.73, 1.39), (5.73, 5.34, 1.72, 1.52), (5.62, 5.18, 1.69, 1.64), (5.05, 5.49, 1.56, 1.34), (4.16, 5.53, 1.17, 1.73), (4.31, 4.84, 1.14, 1.07)]
Epoch 16 - Train Loss: 0.1949 - Train Acc: 0.9262 - Val Loss: 0.1611 - Val Acc: 0.9450 - Val MCC: 0.89
Epoch 17/30
Batch 421/2105 - Loss: 0.0854 - Accuracy: 1.0000
Batch 842/2105 - Loss: 0.1580 - Accuracy: 0.9062
Batch 1263/2105 - Loss: 0.2470 - Accuracy: 0.9375
Batch 1684/2105 - Loss: 0.1125 - Accuracy: 0.9688
Batch 2105/2105 - Loss: 0.1257 - Accuracy: 0.9524
Scalings = [(3.86, 3.93, 1.06, 0.87), (3.99, 3.52, 0.95, 0.25), (3.97, 3.41, 0.92, 0.18), (3.99, 3.44, 0.87, 0.12), (4.05, 3.26, 0.9, 0.2), (4.28, 3.52, 1.07, 0.14), (4.42, 3.91, 1.08, 0.24), (4.07, 4.21, 0.83, 0.24), (4.58, 3.88, 1.2, 0.29), (4.7, 3.96, 1.21, 0.25), (4.69, 4.68, 1.15, 0.5), (4.68, 3.97, 1.33, 0.2), (4.73, 4.19, 1.3, 0.3), (5.28, 4.91, 1.6, 0.61), (5.35, 4.66, 1.53, 0.38), (5.13, 4.43, 1.44, 0.56), (5.45, 4.75, 1.71, 0.54), (5.75, 4.72, 1.63, 1.14), (6.15, 5.01, 1.76, 1.37), (5.77, 5.41, 1.73, 1.53), (5.65, 5.26, 1.69, 1.67), (5.05, 5.54, 1.55, 1.33), (4.1, 5.6, 1.13, 1.74), (4.37, 4.86, 1.15, 1.06)]
Epoch 17 - Train Loss: 0.1922 - Train Acc: 0.9261 - Val Loss: 0.1706 - Val Acc: 0.9484 - Val MCC: 0.90
Epoch 18/30
Batch 421/2105 - Loss: 0.3055 - Accuracy: 0.8125
Batch 842/2105 - Loss: 0.1748 - Accuracy: 0.9375
Batch 1263/2105 - Loss: 0.4708 - Accuracy: 0.7500
Batch 1684/2105 - Loss: 0.1061 - Accuracy: 0.9688
Batch 2105/2105 - Loss: 0.7818 - Accuracy: 0.6667
Scalings = [(3.92, 3.97, 1.08, 0.87), (4.05, 3.58, 0.97, 0.25), (4.0, 3.55, 0.92, 0.22), (4.06, 3.53, 0.9, 0.14), (4.07, 3.35, 0.9, 0.21), (4.29, 3.6, 1.06, 0.14), (4.43, 4.01, 1.07, 0.25), (4.12, 4.31, 0.83, 0.25), (4.66, 3.98, 1.22, 0.3), (4.84, 4.09, 1.27, 0.26), (4.8, 4.75, 1.2, 0.5), (4.72, 4.03, 1.34, 0.19), (4.81, 4.3, 1.33, 0.31), (5.33, 4.96, 1.62, 0.61), (5.41, 4.73, 1.55, 0.38), (5.19, 4.48, 1.46, 0.56), (5.49, 4.8, 1.72, 0.54), (5.82, 4.74, 1.65, 1.13), (6.19, 5.07, 1.76, 1.38), (5.84, 5.45, 1.74, 1.53), (5.7, 5.29, 1.7, 1.67), (5.05, 5.59, 1.53, 1.33), (4.09, 5.72, 1.11, 1.78), (4.42, 4.94, 1.15, 1.06)]
Epoch 18 - Train Loss: 0.1886 - Train Acc: 0.9277 - Val Loss: 0.1693 - Val Acc: 0.9461 - Val MCC: 0.89
Epoch 19/30
Batch 421/2105 - Loss: 0.1123 - Accuracy: 0.9688
Batch 842/2105 - Loss: 0.2434 - Accuracy: 0.9375
Batch 1263/2105 - Loss: 0.0934 - Accuracy: 0.9688
Batch 1684/2105 - Loss: 0.0388 - Accuracy: 1.0000
Batch 2105/2105 - Loss: 0.0494 - Accuracy: 1.0000
Scalings = [(3.97, 3.98, 1.1, 0.86), (4.08, 3.65, 0.96, 0.26), (4.0, 3.54, 0.9, 0.2), (4.07, 3.62, 0.89, 0.16), (4.1, 3.35, 0.89, 0.19), (4.34, 3.59, 1.09, 0.12), (4.46, 4.04, 1.07, 0.24), (4.16, 4.38, 0.83, 0.26), (4.68, 3.99, 1.22, 0.28), (4.85, 4.09, 1.26, 0.24), (4.84, 4.81, 1.2, 0.51), (4.71, 4.09, 1.32, 0.2), (4.83, 4.37, 1.33, 0.3), (5.35, 5.02, 1.62, 0.62), (5.47, 4.82, 1.57, 0.4), (5.23, 4.54, 1.47, 0.56), (5.49, 4.85, 1.71, 0.54), (5.86, 4.76, 1.67, 1.14), (6.19, 5.06, 1.75, 1.37), (5.83, 5.46, 1.72, 1.53), (5.71, 5.33, 1.7, 1.68), (5.14, 5.62, 1.56, 1.33), (4.13, 5.74, 1.11, 1.78), (4.43, 4.95, 1.14, 1.05)]
Epoch 19 - Train Loss: 0.1898 - Train Acc: 0.9279 - Val Loss: 0.1643 - Val Acc: 0.9495 - Val MCC: 0.90
Epoch 20/30
Batch 421/2105 - Loss: 0.2587 - Accuracy: 0.8750
Batch 842/2105 - Loss: 0.3207 - Accuracy: 0.8750
Batch 1263/2105 - Loss: 0.0841 - Accuracy: 0.9688
Batch 1684/2105 - Loss: 0.1831 - Accuracy: 0.9062
Batch 2105/2105 - Loss: 0.0499 - Accuracy: 1.0000
Scalings = [(3.99, 4.01, 1.11, 0.88), (4.06, 3.67, 0.95, 0.26), (3.98, 3.54, 0.89, 0.19), (4.14, 3.63, 0.93, 0.15), (4.07, 3.37, 0.87, 0.18), (4.33, 3.64, 1.07, 0.12), (4.45, 4.09, 1.05, 0.24), (4.24, 4.43, 0.87, 0.27), (4.74, 4.05, 1.24, 0.28), (4.88, 4.16, 1.26, 0.25), (4.87, 4.78, 1.2, 0.49), (4.75, 4.13, 1.34, 0.2), (4.85, 4.44, 1.33, 0.33), (5.43, 5.09, 1.67, 0.65), (5.52, 4.82, 1.6, 0.38), (5.22, 4.57, 1.46, 0.56), (5.53, 4.9, 1.73, 0.55), (5.93, 4.8, 1.71, 1.15), (6.25, 5.06, 1.78, 1.35), (5.86, 5.48, 1.73, 1.53), (5.69, 5.37, 1.68, 1.68), (5.21, 5.66, 1.58, 1.33), (4.12, 5.8, 1.1, 1.79), (4.4, 5.04, 1.12, 1.06)]
Epoch 20 - Train Loss: 0.1877 - Train Acc: 0.9291 - Val Loss: 0.1652 - Val Acc: 0.9472 - Val MCC: 0.89
Epoch 21/30
Batch 421/2105 - Loss: 0.1705 - Accuracy: 0.9062
Batch 842/2105 - Loss: 0.0891 - Accuracy: 0.9688
Batch 1263/2105 - Loss: 0.0460 - Accuracy: 1.0000
Batch 1684/2105 - Loss: 0.2818 - Accuracy: 0.9062
Batch 2105/2105 - Loss: 0.2296 - Accuracy: 0.9048
Scalings = [(4.0, 4.01, 1.11, 0.87), (4.11, 3.68, 0.97, 0.25), (4.01, 3.54, 0.89, 0.18), (4.15, 3.64, 0.93, 0.15), (4.07, 3.4, 0.86, 0.19), (4.35, 3.7, 1.07, 0.13), (4.51, 4.13, 1.07, 0.23), (4.25, 4.45, 0.86, 0.27), (4.79, 4.1, 1.27, 0.28), (4.92, 4.21, 1.28, 0.26), (4.88, 4.83, 1.21, 0.5), (4.77, 4.16, 1.35, 0.19), (4.86, 4.51, 1.34, 0.34), (5.42, 5.11, 1.66, 0.65), (5.53, 4.85, 1.6, 0.38), (5.25, 4.6, 1.47, 0.56), (5.54, 4.93, 1.73, 0.56), (5.93, 4.83, 1.7, 1.17), (6.27, 5.07, 1.78, 1.35), (5.88, 5.49, 1.75, 1.53), (5.73, 5.36, 1.69, 1.67), (5.23, 5.65, 1.58, 1.32), (4.16, 5.8, 1.11, 1.79), (4.38, 5.08, 1.1, 1.06)]
Epoch 21 - Train Loss: 0.1860 - Train Acc: 0.9289 - Val Loss: 0.1732 - Val Acc: 0.9472 - Val MCC: 0.89
Epoch 22/30
Batch 421/2105 - Loss: 0.1722 - Accuracy: 0.9375
Batch 842/2105 - Loss: 0.0716 - Accuracy: 0.9688
Batch 1263/2105 - Loss: 0.2052 - Accuracy: 0.9375
Batch 1684/2105 - Loss: 0.1836 - Accuracy: 0.9062
Batch 2105/2105 - Loss: 0.1885 - Accuracy: 0.9524
Scalings = [(3.99, 4.04, 1.1, 0.89), (4.13, 3.67, 0.98, 0.24), (4.05, 3.59, 0.91, 0.2), (4.15, 3.65, 0.91, 0.14), (4.06, 3.43, 0.86, 0.19), (4.38, 3.73, 1.09, 0.14), (4.52, 4.19, 1.07, 0.25), (4.26, 4.5, 0.86, 0.27), (4.81, 4.13, 1.27, 0.29), (4.94, 4.27, 1.29, 0.27), (4.93, 4.85, 1.24, 0.5), (4.79, 4.18, 1.37, 0.19), (4.9, 4.55, 1.36, 0.35), (5.42, 5.16, 1.65, 0.67), (5.58, 4.91, 1.63, 0.4), (5.3, 4.64, 1.5, 0.57), (5.54, 4.95, 1.73, 0.56), (5.96, 4.83, 1.73, 1.17), (6.32, 5.07, 1.8, 1.35), (5.91, 5.5, 1.76, 1.53), (5.76, 5.38, 1.7, 1.69), (5.26, 5.69, 1.59, 1.32), (4.1, 5.84, 1.08, 1.8), (4.39, 5.09, 1.1, 1.06)]
Epoch 22 - Train Loss: 0.1852 - Train Acc: 0.9291 - Val Loss: 0.1659 - Val Acc: 0.9472 - Val MCC: 0.89
Epoch 23/30
Batch 421/2105 - Loss: 0.2518 - Accuracy: 0.8750
Batch 842/2105 - Loss: 0.3834 - Accuracy: 0.8438
Batch 1263/2105 - Loss: 0.0721 - Accuracy: 0.9688
Batch 1684/2105 - Loss: 0.4415 - Accuracy: 0.8750
Batch 2105/2105 - Loss: 0.2198 - Accuracy: 0.9048
Scalings = [(3.99, 4.05, 1.1, 0.9), (4.15, 3.63, 0.99, 0.21), (4.1, 3.56, 0.93, 0.18), (4.19, 3.66, 0.93, 0.14), (4.07, 3.46, 0.86, 0.2), (4.4, 3.77, 1.1, 0.14), (4.52, 4.23, 1.06, 0.25), (4.28, 4.53, 0.87, 0.27), (4.8, 4.17, 1.27, 0.29), (4.93, 4.3, 1.28, 0.27), (4.96, 4.89, 1.26, 0.52), (4.81, 4.22, 1.38, 0.2), (4.91, 4.59, 1.36, 0.36), (5.41, 5.19, 1.65, 0.68), (5.6, 4.95, 1.65, 0.41), (5.3, 4.69, 1.5, 0.58), (5.55, 4.96, 1.74, 0.56), (6.0, 4.83, 1.75, 1.17), (6.34, 5.11, 1.8, 1.36), (5.91, 5.52, 1.75, 1.54), (5.8, 5.39, 1.71, 1.69), (5.3, 5.69, 1.6, 1.32), (4.05, 5.85, 1.06, 1.81), (4.4, 5.1, 1.09, 1.06)]
Epoch 23 - Train Loss: 0.1850 - Train Acc: 0.9300 - Val Loss: 0.1668 - Val Acc: 0.9507 - Val MCC: 0.90
Epoch 24/30
Batch 421/2105 - Loss: 0.1243 - Accuracy: 0.9688
Batch 842/2105 - Loss: 0.3215 - Accuracy: 0.9062
Batch 1263/2105 - Loss: 0.1539 - Accuracy: 0.9375
Batch 1684/2105 - Loss: 0.0637 - Accuracy: 1.0000
Batch 2105/2105 - Loss: 0.1266 - Accuracy: 0.9524
Scalings = [(3.98, 4.05, 1.1, 0.9), (4.15, 3.66, 0.98, 0.22), (4.12, 3.56, 0.93, 0.17), (4.16, 3.68, 0.91, 0.14), (4.07, 3.47, 0.86, 0.2), (4.4, 3.81, 1.1, 0.15), (4.54, 4.27, 1.07, 0.26), (4.29, 4.55, 0.88, 0.28), (4.84, 4.19, 1.28, 0.29), (4.98, 4.32, 1.31, 0.27), (4.99, 4.92, 1.27, 0.53), (4.82, 4.27, 1.39, 0.22), (4.93, 4.62, 1.37, 0.38), (5.4, 5.21, 1.64, 0.69), (5.63, 4.98, 1.67, 0.42), (5.32, 4.71, 1.51, 0.59), (5.59, 4.99, 1.76, 0.57), (6.02, 4.84, 1.76, 1.18), (6.35, 5.14, 1.81, 1.37), (5.91, 5.54, 1.76, 1.54), (5.79, 5.39, 1.71, 1.69), (5.29, 5.72, 1.6, 1.32), (4.07, 5.84, 1.06, 1.8), (4.4, 5.12, 1.09, 1.06)]
Epoch 24 - Train Loss: 0.1828 - Train Acc: 0.9306 - Val Loss: 0.1711 - Val Acc: 0.9507 - Val MCC: 0.90
Epoch 25/30
Batch 421/2105 - Loss: 0.1742 - Accuracy: 0.9375
Batch 842/2105 - Loss: 0.2463 - Accuracy: 0.9375
Batch 1263/2105 - Loss: 0.1773 - Accuracy: 0.9688
Batch 1684/2105 - Loss: 0.1283 - Accuracy: 0.9062
Batch 2105/2105 - Loss: 0.1738 - Accuracy: 0.9524
Scalings = [(3.96, 4.06, 1.08, 0.91), (4.16, 3.67, 0.99, 0.23), (4.1, 3.56, 0.93, 0.17), (4.18, 3.72, 0.92, 0.16), (4.08, 3.46, 0.87, 0.19), (4.4, 3.83, 1.1, 0.15), (4.55, 4.29, 1.07, 0.26), (4.3, 4.57, 0.88, 0.28), (4.84, 4.21, 1.28, 0.3), (5.0, 4.33, 1.32, 0.26), (4.99, 4.92, 1.27, 0.53), (4.83, 4.29, 1.4, 0.22), (4.92, 4.62, 1.36, 0.38), (5.42, 5.22, 1.66, 0.7), (5.63, 5.01, 1.68, 0.43), (5.34, 4.73, 1.52, 0.59), (5.59, 4.99, 1.77, 0.57), (6.03, 4.84, 1.77, 1.17), (6.35, 5.14, 1.81, 1.37), (5.91, 5.52, 1.75, 1.54), (5.81, 5.4, 1.71, 1.7), (5.29, 5.72, 1.6, 1.32), (4.08, 5.85, 1.06, 1.81), (4.37, 5.11, 1.08, 1.06)]
Epoch 25 - Train Loss: 0.1830 - Train Acc: 0.9299 - Val Loss: 0.1810 - Val Acc: 0.9495 - Val MCC: 0.90
Epoch 26/30
Batch 421/2105 - Loss: 0.2494 - Accuracy: 0.9062
Batch 842/2105 - Loss: 0.0823 - Accuracy: 1.0000
Batch 1263/2105 - Loss: 0.3884 - Accuracy: 0.8750
Batch 1684/2105 - Loss: 0.1878 - Accuracy: 0.9062
Batch 2105/2105 - Loss: 0.3239 - Accuracy: 0.9048
Scalings = [(3.95, 4.06, 1.08, 0.9), (4.17, 3.69, 1.0, 0.23), (4.1, 3.58, 0.93, 0.18), (4.2, 3.72, 0.93, 0.15), (4.11, 3.46, 0.89, 0.19), (4.43, 3.86, 1.13, 0.16), (4.56, 4.3, 1.08, 0.25), (4.31, 4.6, 0.88, 0.3), (4.85, 4.23, 1.29, 0.3), (5.01, 4.35, 1.32, 0.27), (5.0, 4.94, 1.28, 0.53), (4.85, 4.32, 1.41, 0.23), (4.93, 4.64, 1.37, 0.39), (5.44, 5.22, 1.68, 0.7), (5.63, 5.03, 1.68, 0.44), (5.37, 4.73, 1.54, 0.59), (5.59, 5.01, 1.77, 0.58), (6.03, 4.84, 1.77, 1.18), (6.35, 5.14, 1.81, 1.37), (5.92, 5.51, 1.77, 1.53), (5.84, 5.41, 1.73, 1.7), (5.29, 5.75, 1.6, 1.33), (4.08, 5.87, 1.06, 1.82), (4.38, 5.12, 1.08, 1.06)]
Epoch 26 - Train Loss: 0.1819 - Train Acc: 0.9302 - Val Loss: 0.1699 - Val Acc: 0.9472 - Val MCC: 0.89
Epoch 27/30
Batch 421/2105 - Loss: 0.2216 - Accuracy: 0.9375
Batch 842/2105 - Loss: 0.1278 - Accuracy: 0.9062
Batch 1263/2105 - Loss: 0.2492 - Accuracy: 0.9062
Batch 1684/2105 - Loss: 0.1266 - Accuracy: 0.9688
Batch 2105/2105 - Loss: 0.1182 - Accuracy: 0.9524
Scalings = [(3.95, 4.05, 1.07, 0.9), (4.18, 3.7, 1.01, 0.24), (4.1, 3.57, 0.92, 0.17), (4.2, 3.73, 0.93, 0.15), (4.11, 3.48, 0.89, 0.2), (4.44, 3.84, 1.13, 0.15), (4.56, 4.31, 1.08, 0.26), (4.31, 4.61, 0.89, 0.3), (4.86, 4.25, 1.3, 0.31), (5.01, 4.36, 1.32, 0.27), (5.01, 4.95, 1.28, 0.54), (4.86, 4.32, 1.42, 0.23), (4.93, 4.65, 1.38, 0.39), (5.44, 5.22, 1.69, 0.71), (5.64, 5.04, 1.69, 0.44), (5.37, 4.74, 1.54, 0.6), (5.59, 5.0, 1.78, 0.58), (6.04, 4.85, 1.78, 1.19), (6.35, 5.14, 1.81, 1.37), (5.92, 5.51, 1.77, 1.53), (5.85, 5.39, 1.73, 1.7), (5.27, 5.73, 1.59, 1.32), (4.1, 5.88, 1.06, 1.82), (4.38, 5.13, 1.08, 1.06)]
Epoch 27 - Train Loss: 0.1804 - Train Acc: 0.9322 - Val Loss: 0.1706 - Val Acc: 0.9518 - Val MCC: 0.90
Epoch 28/30
Batch 421/2105 - Loss: 0.2979 - Accuracy: 0.8750
Batch 842/2105 - Loss: 0.1298 - Accuracy: 0.9375
Batch 1263/2105 - Loss: 0.1747 - Accuracy: 0.9062
Batch 1684/2105 - Loss: 0.2945 - Accuracy: 0.9375
Batch 2105/2105 - Loss: 0.0721 - Accuracy: 1.0000
Scalings = [(3.95, 4.06, 1.07, 0.91), (4.18, 3.7, 1.01, 0.24), (4.09, 3.57, 0.92, 0.17), (4.21, 3.73, 0.94, 0.15), (4.12, 3.49, 0.9, 0.2), (4.45, 3.84, 1.14, 0.15), (4.57, 4.32, 1.08, 0.26), (4.33, 4.61, 0.89, 0.3), (4.86, 4.27, 1.3, 0.32), (5.01, 4.37, 1.33, 0.28), (5.02, 4.96, 1.29, 0.55), (4.86, 4.33, 1.42, 0.23), (4.94, 4.66, 1.39, 0.4), (5.45, 5.22, 1.69, 0.71), (5.65, 5.03, 1.7, 0.44), (5.38, 4.75, 1.55, 0.6), (5.59, 5.01, 1.78, 0.58), (6.05, 4.85, 1.79, 1.19), (6.35, 5.13, 1.81, 1.37), (5.92, 5.52, 1.77, 1.54), (5.86, 5.39, 1.74, 1.7), (5.27, 5.74, 1.59, 1.32), (4.11, 5.89, 1.06, 1.82), (4.38, 5.14, 1.08, 1.06)]
Epoch 28 - Train Loss: 0.1792 - Train Acc: 0.9329 - Val Loss: 0.1667 - Val Acc: 0.9518 - Val MCC: 0.90
Epoch 29/30
Batch 421/2105 - Loss: 0.0901 - Accuracy: 0.9688
Batch 842/2105 - Loss: 0.0374 - Accuracy: 1.0000
Batch 1263/2105 - Loss: 0.1473 - Accuracy: 0.9688
Batch 1684/2105 - Loss: 0.1536 - Accuracy: 0.9375
Batch 2105/2105 - Loss: 0.0599 - Accuracy: 1.0000
Scalings = [(3.95, 4.05, 1.07, 0.91), (4.18, 3.7, 1.01, 0.24), (4.1, 3.57, 0.92, 0.17), (4.21, 3.73, 0.94, 0.15), (4.12, 3.49, 0.91, 0.2), (4.45, 3.84, 1.15, 0.15), (4.57, 4.33, 1.09, 0.26), (4.33, 4.62, 0.9, 0.31), (4.86, 4.27, 1.3, 0.32), (5.02, 4.37, 1.33, 0.28), (5.02, 4.96, 1.29, 0.55), (4.86, 4.33, 1.42, 0.23), (4.93, 4.66, 1.38, 0.4), (5.44, 5.22, 1.69, 0.71), (5.65, 5.03, 1.7, 0.44), (5.38, 4.75, 1.55, 0.6), (5.59, 5.01, 1.78, 0.58), (6.05, 4.85, 1.79, 1.2), (6.35, 5.13, 1.81, 1.37), (5.92, 5.52, 1.77, 1.54), (5.85, 5.39, 1.74, 1.7), (5.26, 5.74, 1.59, 1.32), (4.11, 5.89, 1.06, 1.82), (4.37, 5.13, 1.08, 1.06)]
Epoch 29 - Train Loss: 0.1795 - Train Acc: 0.9315 - Val Loss: 0.1691 - Val Acc: 0.9507 - Val MCC: 0.90
Epoch 30/30
Batch 421/2105 - Loss: 0.0412 - Accuracy: 1.0000
Batch 842/2105 - Loss: 0.1628 - Accuracy: 0.9062
Batch 1263/2105 - Loss: 0.2526 - Accuracy: 0.9062
Batch 1684/2105 - Loss: 0.2671 - Accuracy: 0.9062
Batch 2105/2105 - Loss: 0.0582 - Accuracy: 1.0000
Scalings = [(3.95, 4.05, 1.07, 0.9), (4.18, 3.71, 1.01, 0.24), (4.1, 3.57, 0.93, 0.17), (4.21, 3.73, 0.94, 0.15), (4.12, 3.49, 0.91, 0.2), (4.45, 3.84, 1.15, 0.15), (4.58, 4.33, 1.09, 0.26), (4.33, 4.62, 0.9, 0.31), (4.86, 4.27, 1.3, 0.32), (5.01, 4.38, 1.33, 0.28), (5.02, 4.96, 1.29, 0.55), (4.86, 4.33, 1.42, 0.23), (4.93, 4.66, 1.39, 0.4), (5.44, 5.23, 1.69, 0.71), (5.64, 5.03, 1.7, 0.44), (5.38, 4.75, 1.55, 0.6), (5.59, 5.01, 1.78, 0.58), (6.05, 4.85, 1.79, 1.2), (6.35, 5.13, 1.81, 1.37), (5.92, 5.52, 1.77, 1.54), (5.85, 5.39, 1.74, 1.7), (5.26, 5.74, 1.58, 1.32), (4.1, 5.88, 1.06, 1.82), (4.37, 5.14, 1.08, 1.06)]
Epoch 30 - Train Loss: 0.1807 - Train Acc: 0.9324 - Val Loss: 0.1683 - Val Acc: 0.9530 - Val MCC: 0.91
Average Time per Epoch: 1483.79s
