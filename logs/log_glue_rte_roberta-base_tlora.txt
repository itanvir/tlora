Running experiment with data: glue_rte, model: FacebookAI/roberta-base, LoRA type: tlora
RobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-11): 12 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=768, out_features=2, bias=True)
  )
)
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        38,603,520
│    │    └─Embedding: 3-2                                        394,752
│    │    └─Embedding: 3-3                                        768
│    │    └─LayerNorm: 3-4                                        1,536
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,054,464
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                590,592
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                1,538
==========================================================================================
Total params: 124,647,170
Trainable params: 124,647,170
Non-trainable params: 0
==========================================================================================
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        38,603,520
│    │    └─Embedding: 3-2                                        394,752
│    │    └─Embedding: 3-3                                        768
│    │    └─LayerNorm: 3-4                                        1,536
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,054,464
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                590,592
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                1,538
==========================================================================================
Total params: 124,647,170
Trainable params: 124,647,170
Non-trainable params: 0
==========================================================================================
RobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-11): 12 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSdpaSelfAttention(
              (query): TLoRALayer(
                (linear): Linear(in_features=768, out_features=768, bias=True)
              )
              (key): TLoRALayer(
                (linear): Linear(in_features=768, out_features=768, bias=True)
              )
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=768, out_features=2, bias=True)
  )
)
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        (38,603,520)
│    │    └─Embedding: 3-2                                        (394,752)
│    │    └─Embedding: 3-3                                        (768)
│    │    └─LayerNorm: 3-4                                        (1,536)
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,313,688
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                (590,592)
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                (1,538)
==========================================================================================
Total params: 124,906,394
Trainable params: 259,224
Non-trainable params: 124,647,170
==========================================================================================
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        (38,603,520)
│    │    └─Embedding: 3-2                                        (394,752)
│    │    └─Embedding: 3-3                                        (768)
│    │    └─LayerNorm: 3-4                                        (1,536)
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,313,688
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                (590,592)
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                (1,538)
==========================================================================================
Total params: 124,906,394
Trainable params: 259,224
Non-trainable params: 124,647,170
==========================================================================================
Epoch 1/20
Batch 15/78 - Loss: 0.6744 - Accuracy: 0.5625
Batch 30/78 - Loss: 0.7311 - Accuracy: 0.3750
Batch 45/78 - Loss: 0.7143 - Accuracy: 0.4062
Batch 60/78 - Loss: 0.6701 - Accuracy: 0.6250
Batch 75/78 - Loss: 0.6909 - Accuracy: 0.5938
Epoch 1 - Train Loss: 0.6990 - Train Acc: 0.4929 - Val Loss: 0.7010 - Val Acc: 0.4712 - Time: 71.23s
Epoch 2/20
Batch 15/78 - Loss: 0.7038 - Accuracy: 0.4688
Batch 30/78 - Loss: 0.7031 - Accuracy: 0.5312
Batch 45/78 - Loss: 0.6737 - Accuracy: 0.6250
Batch 60/78 - Loss: 0.6817 - Accuracy: 0.5000
Batch 75/78 - Loss: 0.6710 - Accuracy: 0.5625
Epoch 2 - Train Loss: 0.6975 - Train Acc: 0.4975 - Val Loss: 0.7002 - Val Acc: 0.4712 - Time: 54.81s
Epoch 3/20
Batch 15/78 - Loss: 0.6958 - Accuracy: 0.5625
Batch 30/78 - Loss: 0.7046 - Accuracy: 0.4688
Batch 45/78 - Loss: 0.6919 - Accuracy: 0.5000
Batch 60/78 - Loss: 0.7193 - Accuracy: 0.4062
Batch 75/78 - Loss: 0.7206 - Accuracy: 0.4375
Epoch 3 - Train Loss: 0.6990 - Train Acc: 0.4933 - Val Loss: 0.6994 - Val Acc: 0.4712 - Time: 53.94s
Epoch 4/20
Batch 15/78 - Loss: 0.7047 - Accuracy: 0.4062
Batch 30/78 - Loss: 0.7197 - Accuracy: 0.3750
Batch 45/78 - Loss: 0.6925 - Accuracy: 0.5000
Batch 60/78 - Loss: 0.6766 - Accuracy: 0.5938
Batch 75/78 - Loss: 0.7025 - Accuracy: 0.4375
Epoch 4 - Train Loss: 0.6977 - Train Acc: 0.5009 - Val Loss: 0.6983 - Val Acc: 0.4712 - Time: 51.40s
Epoch 5/20
Batch 15/78 - Loss: 0.7020 - Accuracy: 0.5000
Batch 30/78 - Loss: 0.6893 - Accuracy: 0.5000
Batch 45/78 - Loss: 0.6813 - Accuracy: 0.6562
Batch 60/78 - Loss: 0.6983 - Accuracy: 0.4688
Batch 75/78 - Loss: 0.6841 - Accuracy: 0.5625
Epoch 5 - Train Loss: 0.6952 - Train Acc: 0.5032 - Val Loss: 0.6962 - Val Acc: 0.4712 - Time: 52.73s
Epoch 6/20
Batch 15/78 - Loss: 0.6737 - Accuracy: 0.6875
Batch 30/78 - Loss: 0.6933 - Accuracy: 0.5000
Batch 45/78 - Loss: 0.7028 - Accuracy: 0.4062
Batch 60/78 - Loss: 0.7251 - Accuracy: 0.2500
Batch 75/78 - Loss: 0.6859 - Accuracy: 0.5000
Epoch 6 - Train Loss: 0.6934 - Train Acc: 0.4993 - Val Loss: 0.6932 - Val Acc: 0.4712 - Time: 51.36s
Epoch 7/20
Batch 15/78 - Loss: 0.6731 - Accuracy: 0.6875
Batch 30/78 - Loss: 0.6991 - Accuracy: 0.4688
Batch 45/78 - Loss: 0.7007 - Accuracy: 0.5312
Batch 60/78 - Loss: 0.6789 - Accuracy: 0.6250
Batch 75/78 - Loss: 0.6775 - Accuracy: 0.6562
Epoch 7 - Train Loss: 0.6901 - Train Acc: 0.5351 - Val Loss: 0.6875 - Val Acc: 0.6038 - Time: 51.29s
Epoch 8/20
Batch 15/78 - Loss: 0.6859 - Accuracy: 0.5625
Batch 30/78 - Loss: 0.6962 - Accuracy: 0.4375
Batch 45/78 - Loss: 0.6856 - Accuracy: 0.6250
Batch 60/78 - Loss: 0.6915 - Accuracy: 0.5312
Batch 75/78 - Loss: 0.6950 - Accuracy: 0.5625
Epoch 8 - Train Loss: 0.6861 - Train Acc: 0.5558 - Val Loss: 0.6828 - Val Acc: 0.6263 - Time: 49.75s
Epoch 9/20
Batch 15/78 - Loss: 0.6862 - Accuracy: 0.5938
Batch 30/78 - Loss: 0.6937 - Accuracy: 0.4688
Batch 45/78 - Loss: 0.6819 - Accuracy: 0.5312
Batch 60/78 - Loss: 0.6913 - Accuracy: 0.5312
Batch 75/78 - Loss: 0.6873 - Accuracy: 0.5938
Epoch 9 - Train Loss: 0.6842 - Train Acc: 0.5661 - Val Loss: 0.6784 - Val Acc: 0.6367 - Time: 50.18s
Epoch 10/20
Batch 15/78 - Loss: 0.6737 - Accuracy: 0.5938
Batch 30/78 - Loss: 0.6915 - Accuracy: 0.5938
Batch 45/78 - Loss: 0.6815 - Accuracy: 0.5625
Batch 60/78 - Loss: 0.6877 - Accuracy: 0.5000
Batch 75/78 - Loss: 0.6791 - Accuracy: 0.6562
Epoch 10 - Train Loss: 0.6800 - Train Acc: 0.5963 - Val Loss: 0.6736 - Val Acc: 0.6420 - Time: 51.88s
Epoch 11/20
Batch 15/78 - Loss: 0.6900 - Accuracy: 0.5625
Batch 30/78 - Loss: 0.6858 - Accuracy: 0.5938
Batch 45/78 - Loss: 0.6927 - Accuracy: 0.5000
Batch 60/78 - Loss: 0.7022 - Accuracy: 0.5000
Batch 75/78 - Loss: 0.6965 - Accuracy: 0.5625
Epoch 11 - Train Loss: 0.6783 - Train Acc: 0.6050 - Val Loss: 0.6674 - Val Acc: 0.6490 - Time: 48.56s
Epoch 12/20
Batch 15/78 - Loss: 0.6606 - Accuracy: 0.6875
Batch 30/78 - Loss: 0.7083 - Accuracy: 0.4375
Batch 45/78 - Loss: 0.6723 - Accuracy: 0.5938
Batch 60/78 - Loss: 0.6710 - Accuracy: 0.6875
Batch 75/78 - Loss: 0.6735 - Accuracy: 0.6562
Epoch 12 - Train Loss: 0.6746 - Train Acc: 0.6211 - Val Loss: 0.6613 - Val Acc: 0.6786 - Time: 51.32s
Epoch 13/20
Batch 15/78 - Loss: 0.6469 - Accuracy: 0.7188
Batch 30/78 - Loss: 0.6812 - Accuracy: 0.5625
Batch 45/78 - Loss: 0.6886 - Accuracy: 0.5312
Batch 60/78 - Loss: 0.6843 - Accuracy: 0.5000
Batch 75/78 - Loss: 0.6810 - Accuracy: 0.6250
Epoch 13 - Train Loss: 0.6681 - Train Acc: 0.6391 - Val Loss: 0.6548 - Val Acc: 0.6996 - Time: 49.18s
Epoch 14/20
Batch 15/78 - Loss: 0.6752 - Accuracy: 0.5938
Batch 30/78 - Loss: 0.6566 - Accuracy: 0.7500
Batch 45/78 - Loss: 0.6556 - Accuracy: 0.6562
Batch 60/78 - Loss: 0.6164 - Accuracy: 0.7812
Batch 75/78 - Loss: 0.6796 - Accuracy: 0.5938
Epoch 14 - Train Loss: 0.6611 - Train Acc: 0.6634 - Val Loss: 0.6496 - Val Acc: 0.6906 - Time: 50.14s
Epoch 15/20
Batch 15/78 - Loss: 0.6383 - Accuracy: 0.7500
Batch 30/78 - Loss: 0.6845 - Accuracy: 0.5938
Batch 45/78 - Loss: 0.6452 - Accuracy: 0.6875
Batch 60/78 - Loss: 0.6905 - Accuracy: 0.5312
Batch 75/78 - Loss: 0.6714 - Accuracy: 0.6875
Epoch 15 - Train Loss: 0.6600 - Train Acc: 0.6714 - Val Loss: 0.6447 - Val Acc: 0.6890 - Time: 49.23s
Epoch 16/20
Batch 15/78 - Loss: 0.6230 - Accuracy: 0.7812
Batch 30/78 - Loss: 0.6612 - Accuracy: 0.5938
Batch 45/78 - Loss: 0.6671 - Accuracy: 0.5625
Batch 60/78 - Loss: 0.6338 - Accuracy: 0.7812
Batch 75/78 - Loss: 0.6277 - Accuracy: 0.7500
Epoch 16 - Train Loss: 0.6533 - Train Acc: 0.6776 - Val Loss: 0.6381 - Val Acc: 0.7012 - Time: 49.92s
Epoch 17/20
Batch 15/78 - Loss: 0.6315 - Accuracy: 0.7812
Batch 30/78 - Loss: 0.6391 - Accuracy: 0.7500
Batch 45/78 - Loss: 0.6628 - Accuracy: 0.6562
Batch 60/78 - Loss: 0.6688 - Accuracy: 0.6250
Batch 75/78 - Loss: 0.6230 - Accuracy: 0.7812
Epoch 17 - Train Loss: 0.6477 - Train Acc: 0.6890 - Val Loss: 0.6340 - Val Acc: 0.6925 - Time: 49.75s
Epoch 18/20
Batch 15/78 - Loss: 0.6590 - Accuracy: 0.7188
Batch 30/78 - Loss: 0.6334 - Accuracy: 0.6875
Batch 45/78 - Loss: 0.6311 - Accuracy: 0.7188
Batch 60/78 - Loss: 0.6847 - Accuracy: 0.5938
Batch 75/78 - Loss: 0.6227 - Accuracy: 0.7500
Epoch 18 - Train Loss: 0.6434 - Train Acc: 0.6981 - Val Loss: 0.6311 - Val Acc: 0.6872 - Time: 48.17s
Epoch 19/20
Batch 15/78 - Loss: 0.6351 - Accuracy: 0.7500
Batch 30/78 - Loss: 0.5963 - Accuracy: 0.7812
Batch 45/78 - Loss: 0.6571 - Accuracy: 0.6562
Batch 60/78 - Loss: 0.6867 - Accuracy: 0.5625
Batch 75/78 - Loss: 0.5798 - Accuracy: 0.7500
Epoch 19 - Train Loss: 0.6337 - Train Acc: 0.7144 - Val Loss: 0.6281 - Val Acc: 0.6872 - Time: 52.39s
Epoch 20/20
Batch 15/78 - Loss: 0.6300 - Accuracy: 0.7812
Batch 30/78 - Loss: 0.6179 - Accuracy: 0.6875
Batch 45/78 - Loss: 0.6021 - Accuracy: 0.8125
Batch 60/78 - Loss: 0.5956 - Accuracy: 0.7812
Batch 75/78 - Loss: 0.6374 - Accuracy: 0.7500
Epoch 20 - Train Loss: 0.6293 - Train Acc: 0.7203 - Val Loss: 0.6289 - Val Acc: 0.6872 - Time: 51.21s
Average Time per Epoch: 51.92s
Validation Loss: 0.6289, Validation Accuracy: 0.6872
