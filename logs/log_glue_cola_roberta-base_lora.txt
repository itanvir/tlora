Running experiment with data: glue_cola, model: FacebookAI/roberta-base, LoRA type: lora
RobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-11): 12 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=768, out_features=2, bias=True)
  )
)
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        38,603,520
│    │    └─Embedding: 3-2                                        394,752
│    │    └─Embedding: 3-3                                        768
│    │    └─LayerNorm: 3-4                                        1,536
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,054,464
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                590,592
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                1,538
==========================================================================================
Total params: 124,647,170
Trainable params: 124,647,170
Non-trainable params: 0
==========================================================================================
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        38,603,520
│    │    └─Embedding: 3-2                                        394,752
│    │    └─Embedding: 3-3                                        768
│    │    └─LayerNorm: 3-4                                        1,536
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,054,464
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                590,592
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                1,538
==========================================================================================
Total params: 124,647,170
Trainable params: 124,647,170
Non-trainable params: 0
==========================================================================================
RobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-11): 12 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSdpaSelfAttention(
              (query): LoRALayer(
                (linear): Linear(in_features=768, out_features=768, bias=True)
              )
              (key): LoRALayer(
                (linear): Linear(in_features=768, out_features=768, bias=True)
              )
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=768, out_features=2, bias=True)
  )
)
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        (38,603,520)
│    │    └─Embedding: 3-2                                        (394,752)
│    │    └─Embedding: 3-3                                        (768)
│    │    └─LayerNorm: 3-4                                        (1,536)
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,349,376
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                (590,592)
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                (1,538)
==========================================================================================
Total params: 124,942,082
Trainable params: 294,912
Non-trainable params: 124,647,170
==========================================================================================
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        (38,603,520)
│    │    └─Embedding: 3-2                                        (394,752)
│    │    └─Embedding: 3-3                                        (768)
│    │    └─LayerNorm: 3-4                                        (1,536)
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,349,376
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                (590,592)
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                (1,538)
==========================================================================================
Total params: 124,942,082
Trainable params: 294,912
Non-trainable params: 124,647,170
==========================================================================================
Epoch 1/40
Batch 53/268 - Loss: 0.6891 - Accuracy: 0.5312
Batch 106/268 - Loss: 0.6790 - Accuracy: 0.6875
Batch 159/268 - Loss: 0.6783 - Accuracy: 0.7188
Batch 212/268 - Loss: 0.6936 - Accuracy: 0.5625
Batch 265/268 - Loss: 0.6604 - Accuracy: 0.7500
Epoch 1 - Train Loss: 0.6851 - Train Acc: 0.5987 - Val Loss: 0.6840 - Val Acc: 0.6931 - Time: 27.87s
Epoch 2/40
Batch 53/268 - Loss: 0.6924 - Accuracy: 0.5312
Batch 106/268 - Loss: 0.6921 - Accuracy: 0.4375
Batch 159/268 - Loss: 0.6819 - Accuracy: 0.5938
Batch 212/268 - Loss: 0.6899 - Accuracy: 0.4688
Batch 265/268 - Loss: 0.6774 - Accuracy: 0.7188
Epoch 2 - Train Loss: 0.6810 - Train Acc: 0.6338 - Val Loss: 0.6814 - Val Acc: 0.6931 - Time: 28.64s
Epoch 3/40
Batch 53/268 - Loss: 0.6771 - Accuracy: 0.5938
Batch 106/268 - Loss: 0.6682 - Accuracy: 0.6875
Batch 159/268 - Loss: 0.7011 - Accuracy: 0.5625
Batch 212/268 - Loss: 0.6743 - Accuracy: 0.6250
Batch 265/268 - Loss: 0.6810 - Accuracy: 0.6250
Epoch 3 - Train Loss: 0.6794 - Train Acc: 0.6468 - Val Loss: 0.6791 - Val Acc: 0.6931 - Time: 29.30s
Epoch 4/40
Batch 53/268 - Loss: 0.6857 - Accuracy: 0.6562
Batch 106/268 - Loss: 0.6806 - Accuracy: 0.5938
Batch 159/268 - Loss: 0.6691 - Accuracy: 0.7812
Batch 212/268 - Loss: 0.6790 - Accuracy: 0.7812
Batch 265/268 - Loss: 0.6673 - Accuracy: 0.6875
Epoch 4 - Train Loss: 0.6766 - Train Acc: 0.6644 - Val Loss: 0.6761 - Val Acc: 0.6931 - Time: 29.75s
Epoch 5/40
Batch 53/268 - Loss: 0.6978 - Accuracy: 0.5312
Batch 106/268 - Loss: 0.6671 - Accuracy: 0.7812
Batch 159/268 - Loss: 0.6787 - Accuracy: 0.6875
Batch 212/268 - Loss: 0.6682 - Accuracy: 0.7812
Batch 265/268 - Loss: 0.6575 - Accuracy: 0.7188
Epoch 5 - Train Loss: 0.6724 - Train Acc: 0.6784 - Val Loss: 0.6703 - Val Acc: 0.6931 - Time: 30.92s
Epoch 6/40
Batch 53/268 - Loss: 0.6521 - Accuracy: 0.7188
Batch 106/268 - Loss: 0.6467 - Accuracy: 0.7500
Batch 159/268 - Loss: 0.6539 - Accuracy: 0.7500
Batch 212/268 - Loss: 0.6703 - Accuracy: 0.7188
Batch 265/268 - Loss: 0.6540 - Accuracy: 0.7188
Epoch 6 - Train Loss: 0.6653 - Train Acc: 0.6937 - Val Loss: 0.6620 - Val Acc: 0.6931 - Time: 30.49s
Epoch 7/40
Batch 53/268 - Loss: 0.6696 - Accuracy: 0.6250
Batch 106/268 - Loss: 0.6625 - Accuracy: 0.7500
Batch 159/268 - Loss: 0.6648 - Accuracy: 0.7188
Batch 212/268 - Loss: 0.6776 - Accuracy: 0.6250
Batch 265/268 - Loss: 0.6673 - Accuracy: 0.6250
Epoch 7 - Train Loss: 0.6582 - Train Acc: 0.7008 - Val Loss: 0.6535 - Val Acc: 0.6931 - Time: 29.95s
Epoch 8/40
Batch 53/268 - Loss: 0.6274 - Accuracy: 0.7500
Batch 106/268 - Loss: 0.6297 - Accuracy: 0.8125
Batch 159/268 - Loss: 0.6252 - Accuracy: 0.7812
Batch 212/268 - Loss: 0.6509 - Accuracy: 0.6875
Batch 265/268 - Loss: 0.6490 - Accuracy: 0.6875
Epoch 8 - Train Loss: 0.6502 - Train Acc: 0.7020 - Val Loss: 0.6421 - Val Acc: 0.6931 - Time: 29.46s
Epoch 9/40
Batch 53/268 - Loss: 0.6603 - Accuracy: 0.5938
Batch 106/268 - Loss: 0.6443 - Accuracy: 0.7188
Batch 159/268 - Loss: 0.6378 - Accuracy: 0.6875
Batch 212/268 - Loss: 0.6213 - Accuracy: 0.7188
Batch 265/268 - Loss: 0.6137 - Accuracy: 0.7188
Epoch 9 - Train Loss: 0.6411 - Train Acc: 0.7028 - Val Loss: 0.6334 - Val Acc: 0.6931 - Time: 29.01s
Epoch 10/40
Batch 53/268 - Loss: 0.6448 - Accuracy: 0.6250
Batch 106/268 - Loss: 0.6528 - Accuracy: 0.6250
Batch 159/268 - Loss: 0.5620 - Accuracy: 0.8750
Batch 212/268 - Loss: 0.6135 - Accuracy: 0.7500
Batch 265/268 - Loss: 0.6717 - Accuracy: 0.6250
Epoch 10 - Train Loss: 0.6332 - Train Acc: 0.7037 - Val Loss: 0.6267 - Val Acc: 0.6931 - Time: 28.84s
Epoch 11/40
Batch 53/268 - Loss: 0.6588 - Accuracy: 0.6250
Batch 106/268 - Loss: 0.6721 - Accuracy: 0.6562
Batch 159/268 - Loss: 0.6686 - Accuracy: 0.5938
Batch 212/268 - Loss: 0.6484 - Accuracy: 0.6562
Batch 265/268 - Loss: 0.7071 - Accuracy: 0.5625
Epoch 11 - Train Loss: 0.6293 - Train Acc: 0.7043 - Val Loss: 0.6205 - Val Acc: 0.6931 - Time: 29.18s
Epoch 12/40
Batch 53/268 - Loss: 0.6644 - Accuracy: 0.5938
Batch 106/268 - Loss: 0.5837 - Accuracy: 0.7812
Batch 159/268 - Loss: 0.6083 - Accuracy: 0.7812
Batch 212/268 - Loss: 0.6505 - Accuracy: 0.6250
Batch 265/268 - Loss: 0.5913 - Accuracy: 0.7188
Epoch 12 - Train Loss: 0.6246 - Train Acc: 0.7032 - Val Loss: 0.6157 - Val Acc: 0.6931 - Time: 28.95s
Epoch 13/40
Batch 53/268 - Loss: 0.5903 - Accuracy: 0.7812
Batch 106/268 - Loss: 0.6307 - Accuracy: 0.6875
Batch 159/268 - Loss: 0.5401 - Accuracy: 0.8438
Batch 212/268 - Loss: 0.6698 - Accuracy: 0.6250
Batch 265/268 - Loss: 0.5882 - Accuracy: 0.8125
Epoch 13 - Train Loss: 0.6205 - Train Acc: 0.7035 - Val Loss: 0.6136 - Val Acc: 0.6931 - Time: 28.75s
Epoch 14/40
Batch 53/268 - Loss: 0.5675 - Accuracy: 0.7812
Batch 106/268 - Loss: 0.6622 - Accuracy: 0.5938
Batch 159/268 - Loss: 0.5299 - Accuracy: 0.8125
Batch 212/268 - Loss: 0.6900 - Accuracy: 0.5625
Batch 265/268 - Loss: 0.5941 - Accuracy: 0.7500
Epoch 14 - Train Loss: 0.6171 - Train Acc: 0.7050 - Val Loss: 0.6125 - Val Acc: 0.6931 - Time: 28.94s
Epoch 15/40
Batch 53/268 - Loss: 0.5809 - Accuracy: 0.7500
Batch 106/268 - Loss: 0.6625 - Accuracy: 0.5938
Batch 159/268 - Loss: 0.5426 - Accuracy: 0.8438
Batch 212/268 - Loss: 0.5168 - Accuracy: 0.8438
Batch 265/268 - Loss: 0.6351 - Accuracy: 0.6875
Epoch 15 - Train Loss: 0.6164 - Train Acc: 0.7042 - Val Loss: 0.6122 - Val Acc: 0.6931 - Time: 29.08s
Epoch 16/40
Batch 53/268 - Loss: 0.6083 - Accuracy: 0.6875
Batch 106/268 - Loss: 0.6537 - Accuracy: 0.7188
Batch 159/268 - Loss: 0.6061 - Accuracy: 0.7188
Batch 212/268 - Loss: 0.5712 - Accuracy: 0.7812
Batch 265/268 - Loss: 0.5813 - Accuracy: 0.6875
Epoch 16 - Train Loss: 0.6176 - Train Acc: 0.7040 - Val Loss: 0.6113 - Val Acc: 0.6931 - Time: 29.46s
Epoch 17/40
Batch 53/268 - Loss: 0.6622 - Accuracy: 0.6250
Batch 106/268 - Loss: 0.5813 - Accuracy: 0.7500
Batch 159/268 - Loss: 0.5686 - Accuracy: 0.7812
Batch 212/268 - Loss: 0.6062 - Accuracy: 0.7188
Batch 265/268 - Loss: 0.5962 - Accuracy: 0.6875
Epoch 17 - Train Loss: 0.6133 - Train Acc: 0.7044 - Val Loss: 0.6114 - Val Acc: 0.6931 - Time: 29.41s
Epoch 18/40
Batch 53/268 - Loss: 0.5701 - Accuracy: 0.7500
Batch 106/268 - Loss: 0.6128 - Accuracy: 0.7188
Batch 159/268 - Loss: 0.6619 - Accuracy: 0.6250
Batch 212/268 - Loss: 0.6519 - Accuracy: 0.5938
Batch 265/268 - Loss: 0.5565 - Accuracy: 0.8438
Epoch 18 - Train Loss: 0.6115 - Train Acc: 0.7048 - Val Loss: 0.6102 - Val Acc: 0.6931 - Time: 29.36s
Epoch 19/40
Batch 53/268 - Loss: 0.5557 - Accuracy: 0.7812
Batch 106/268 - Loss: 0.5947 - Accuracy: 0.7188
Batch 159/268 - Loss: 0.5237 - Accuracy: 0.8750
Batch 212/268 - Loss: 0.5583 - Accuracy: 0.7812
Batch 265/268 - Loss: 0.6721 - Accuracy: 0.6250
Epoch 19 - Train Loss: 0.6119 - Train Acc: 0.7030 - Val Loss: 0.6100 - Val Acc: 0.6931 - Time: 29.43s
Epoch 20/40
Batch 53/268 - Loss: 0.6181 - Accuracy: 0.6250
Batch 106/268 - Loss: 0.6102 - Accuracy: 0.7500
Batch 159/268 - Loss: 0.6012 - Accuracy: 0.6875
Batch 212/268 - Loss: 0.5357 - Accuracy: 0.8125
Batch 265/268 - Loss: 0.6996 - Accuracy: 0.6250
Epoch 20 - Train Loss: 0.6115 - Train Acc: 0.7033 - Val Loss: 0.6085 - Val Acc: 0.6931 - Time: 30.05s
Epoch 21/40
Batch 53/268 - Loss: 0.6660 - Accuracy: 0.5938
Batch 106/268 - Loss: 0.5370 - Accuracy: 0.8125
Batch 159/268 - Loss: 0.5033 - Accuracy: 0.8750
Batch 212/268 - Loss: 0.6349 - Accuracy: 0.7188
Batch 265/268 - Loss: 0.5308 - Accuracy: 0.8125
Epoch 21 - Train Loss: 0.6071 - Train Acc: 0.7040 - Val Loss: 0.6088 - Val Acc: 0.6931 - Time: 29.60s
Epoch 22/40
Batch 53/268 - Loss: 0.5257 - Accuracy: 0.8125
Batch 106/268 - Loss: 0.6190 - Accuracy: 0.6875
Batch 159/268 - Loss: 0.6930 - Accuracy: 0.5938
Batch 212/268 - Loss: 0.6206 - Accuracy: 0.7188
Batch 265/268 - Loss: 0.6880 - Accuracy: 0.5938
Epoch 22 - Train Loss: 0.6053 - Train Acc: 0.7050 - Val Loss: 0.6081 - Val Acc: 0.6931 - Time: 29.49s
Epoch 23/40
Batch 53/268 - Loss: 0.6328 - Accuracy: 0.6562
Batch 106/268 - Loss: 0.5469 - Accuracy: 0.7500
Batch 159/268 - Loss: 0.7164 - Accuracy: 0.5625
Batch 212/268 - Loss: 0.6437 - Accuracy: 0.6875
Batch 265/268 - Loss: 0.6155 - Accuracy: 0.7188
Epoch 23 - Train Loss: 0.6040 - Train Acc: 0.7045 - Val Loss: 0.6071 - Val Acc: 0.6931 - Time: 29.52s
Epoch 24/40
Batch 53/268 - Loss: 0.6051 - Accuracy: 0.6562
Batch 106/268 - Loss: 0.6080 - Accuracy: 0.7188
Batch 159/268 - Loss: 0.5917 - Accuracy: 0.7500
Batch 212/268 - Loss: 0.5359 - Accuracy: 0.7500
Batch 265/268 - Loss: 0.6486 - Accuracy: 0.5938
Epoch 24 - Train Loss: 0.6050 - Train Acc: 0.7034 - Val Loss: 0.6046 - Val Acc: 0.6931 - Time: 29.68s
Epoch 25/40
Batch 53/268 - Loss: 0.6949 - Accuracy: 0.5938
Batch 106/268 - Loss: 0.6317 - Accuracy: 0.6562
Batch 159/268 - Loss: 0.6576 - Accuracy: 0.6562
Batch 212/268 - Loss: 0.5166 - Accuracy: 0.8438
Batch 265/268 - Loss: 0.6020 - Accuracy: 0.7500
Epoch 25 - Train Loss: 0.6022 - Train Acc: 0.7031 - Val Loss: 0.6031 - Val Acc: 0.6931 - Time: 29.89s
Epoch 26/40
Batch 53/268 - Loss: 0.6725 - Accuracy: 0.6250
Batch 106/268 - Loss: 0.5704 - Accuracy: 0.8125
Batch 159/268 - Loss: 0.5438 - Accuracy: 0.7812
Batch 212/268 - Loss: 0.6727 - Accuracy: 0.6250
Batch 265/268 - Loss: 0.6618 - Accuracy: 0.5938
Epoch 26 - Train Loss: 0.6022 - Train Acc: 0.7039 - Val Loss: 0.5996 - Val Acc: 0.6931 - Time: 30.00s
Epoch 27/40
Batch 53/268 - Loss: 0.5978 - Accuracy: 0.6875
Batch 106/268 - Loss: 0.5767 - Accuracy: 0.7812
Batch 159/268 - Loss: 0.6537 - Accuracy: 0.5938
Batch 212/268 - Loss: 0.6127 - Accuracy: 0.6875
Batch 265/268 - Loss: 0.6318 - Accuracy: 0.6250
Epoch 27 - Train Loss: 0.5978 - Train Acc: 0.7047 - Val Loss: 0.5994 - Val Acc: 0.6931 - Time: 29.69s
Epoch 28/40
Batch 53/268 - Loss: 0.5318 - Accuracy: 0.7812
Batch 106/268 - Loss: 0.5228 - Accuracy: 0.7812
Batch 159/268 - Loss: 0.4836 - Accuracy: 0.8750
Batch 212/268 - Loss: 0.5300 - Accuracy: 0.7812
Batch 265/268 - Loss: 0.5927 - Accuracy: 0.7188
Epoch 28 - Train Loss: 0.5977 - Train Acc: 0.7043 - Val Loss: 0.5970 - Val Acc: 0.6931 - Time: 29.73s
Epoch 29/40
Batch 53/268 - Loss: 0.5180 - Accuracy: 0.8750
Batch 106/268 - Loss: 0.5272 - Accuracy: 0.8125
Batch 159/268 - Loss: 0.5927 - Accuracy: 0.6562
Batch 212/268 - Loss: 0.5956 - Accuracy: 0.7500
Batch 265/268 - Loss: 0.6943 - Accuracy: 0.5625
Epoch 29 - Train Loss: 0.5976 - Train Acc: 0.7036 - Val Loss: 0.5931 - Val Acc: 0.6931 - Time: 29.52s
Epoch 30/40
Batch 53/268 - Loss: 0.5764 - Accuracy: 0.7500
Batch 106/268 - Loss: 0.5446 - Accuracy: 0.7500
Batch 159/268 - Loss: 0.6406 - Accuracy: 0.6562
Batch 212/268 - Loss: 0.6716 - Accuracy: 0.6250
Batch 265/268 - Loss: 0.6655 - Accuracy: 0.5938
Epoch 30 - Train Loss: 0.5953 - Train Acc: 0.7044 - Val Loss: 0.5944 - Val Acc: 0.6931 - Time: 29.98s
Epoch 31/40
Batch 53/268 - Loss: 0.5561 - Accuracy: 0.8125
Batch 106/268 - Loss: 0.6857 - Accuracy: 0.5625
Batch 159/268 - Loss: 0.5618 - Accuracy: 0.7188
Batch 212/268 - Loss: 0.6591 - Accuracy: 0.6250
Batch 265/268 - Loss: 0.5484 - Accuracy: 0.7812
Epoch 31 - Train Loss: 0.5936 - Train Acc: 0.7040 - Val Loss: 0.5919 - Val Acc: 0.6931 - Time: 29.35s
Epoch 32/40
Batch 53/268 - Loss: 0.5754 - Accuracy: 0.7188
Batch 106/268 - Loss: 0.6718 - Accuracy: 0.5938
Batch 159/268 - Loss: 0.6352 - Accuracy: 0.6562
Batch 212/268 - Loss: 0.4849 - Accuracy: 0.8750
Batch 265/268 - Loss: 0.6636 - Accuracy: 0.5000
Epoch 32 - Train Loss: 0.5919 - Train Acc: 0.7039 - Val Loss: 0.5897 - Val Acc: 0.6931 - Time: 29.40s
Epoch 33/40
Batch 53/268 - Loss: 0.5740 - Accuracy: 0.7812
Batch 106/268 - Loss: 0.5435 - Accuracy: 0.7812
Batch 159/268 - Loss: 0.6002 - Accuracy: 0.6875
Batch 212/268 - Loss: 0.5593 - Accuracy: 0.7188
Batch 265/268 - Loss: 0.5994 - Accuracy: 0.6875
Epoch 33 - Train Loss: 0.5924 - Train Acc: 0.7043 - Val Loss: 0.5897 - Val Acc: 0.6931 - Time: 29.33s
Epoch 34/40
Batch 53/268 - Loss: 0.5568 - Accuracy: 0.7500
Batch 106/268 - Loss: 0.6691 - Accuracy: 0.6250
Batch 159/268 - Loss: 0.5280 - Accuracy: 0.7812
Batch 212/268 - Loss: 0.5827 - Accuracy: 0.7188
Batch 265/268 - Loss: 0.5770 - Accuracy: 0.6875
Epoch 34 - Train Loss: 0.5899 - Train Acc: 0.7056 - Val Loss: 0.5859 - Val Acc: 0.6931 - Time: 28.91s
Epoch 35/40
Batch 53/268 - Loss: 0.5938 - Accuracy: 0.6250
Batch 106/268 - Loss: 0.5720 - Accuracy: 0.6875
Batch 159/268 - Loss: 0.5991 - Accuracy: 0.6875
Batch 212/268 - Loss: 0.5877 - Accuracy: 0.6875
Batch 265/268 - Loss: 0.5669 - Accuracy: 0.6875
Epoch 35 - Train Loss: 0.5878 - Train Acc: 0.7040 - Val Loss: 0.5878 - Val Acc: 0.6931 - Time: 28.96s
Epoch 36/40
Batch 53/268 - Loss: 0.4810 - Accuracy: 0.8438
Batch 106/268 - Loss: 0.4993 - Accuracy: 0.8438
Batch 159/268 - Loss: 0.5067 - Accuracy: 0.9062
Batch 212/268 - Loss: 0.5314 - Accuracy: 0.7500
Batch 265/268 - Loss: 0.5700 - Accuracy: 0.7500
Epoch 36 - Train Loss: 0.5878 - Train Acc: 0.7040 - Val Loss: 0.5892 - Val Acc: 0.6931 - Time: 29.05s
Epoch 37/40
Batch 53/268 - Loss: 0.5792 - Accuracy: 0.7188
Batch 106/268 - Loss: 0.5920 - Accuracy: 0.7188
Batch 159/268 - Loss: 0.5261 - Accuracy: 0.8125
Batch 212/268 - Loss: 0.5224 - Accuracy: 0.8125
Batch 265/268 - Loss: 0.5217 - Accuracy: 0.7500
Epoch 37 - Train Loss: 0.5840 - Train Acc: 0.7062 - Val Loss: 0.5858 - Val Acc: 0.6931 - Time: 28.94s
Epoch 38/40
Batch 53/268 - Loss: 0.5692 - Accuracy: 0.7188
Batch 106/268 - Loss: 0.6372 - Accuracy: 0.6562
Batch 159/268 - Loss: 0.5085 - Accuracy: 0.8125
Batch 212/268 - Loss: 0.5170 - Accuracy: 0.7812
Batch 265/268 - Loss: 0.4757 - Accuracy: 0.8750
Epoch 38 - Train Loss: 0.5838 - Train Acc: 0.7051 - Val Loss: 0.5857 - Val Acc: 0.6931 - Time: 28.99s
Epoch 39/40
Batch 53/268 - Loss: 0.6102 - Accuracy: 0.6562
Batch 106/268 - Loss: 0.4777 - Accuracy: 0.8125
Batch 159/268 - Loss: 0.6067 - Accuracy: 0.5938
Batch 212/268 - Loss: 0.6192 - Accuracy: 0.6250
Batch 265/268 - Loss: 0.6557 - Accuracy: 0.5938
Epoch 39 - Train Loss: 0.5830 - Train Acc: 0.7042 - Val Loss: 0.5821 - Val Acc: 0.6931 - Time: 28.97s
Epoch 40/40
Batch 53/268 - Loss: 0.5767 - Accuracy: 0.7500
Batch 106/268 - Loss: 0.6371 - Accuracy: 0.6250
Batch 159/268 - Loss: 0.6213 - Accuracy: 0.6250
Batch 212/268 - Loss: 0.6280 - Accuracy: 0.6562
Batch 265/268 - Loss: 0.5661 - Accuracy: 0.7188
Epoch 40 - Train Loss: 0.5825 - Train Acc: 0.7051 - Val Loss: 0.5790 - Val Acc: 0.6931 - Time: 29.31s
Average Time per Epoch: 29.38s
Validation Loss: 0.5790, Validation Accuracy: 0.6931
