Running experiment with data: glue_cola, model: FacebookAI/roberta-large-mnli, LoRA type: tlora
RobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 1024, padding_idx=1)
      (position_embeddings): Embedding(514, 1024, padding_idx=1)
      (token_type_embeddings): Embedding(1, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-23): 24 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSdpaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHead(
    (dense): Linear(in_features=1024, out_features=1024, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=1024, out_features=3, bias=True)
  )
)
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        51,471,360
│    │    └─Embedding: 3-2                                        526,336
│    │    └─Embedding: 3-3                                        1,024
│    │    └─LayerNorm: 3-4                                        2,048
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       302,309,376
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                1,049,600
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                3,075
==========================================================================================
Total params: 355,362,819
Trainable params: 355,362,819
Non-trainable params: 0
==========================================================================================
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        51,471,360
│    │    └─Embedding: 3-2                                        526,336
│    │    └─Embedding: 3-3                                        1,024
│    │    └─LayerNorm: 3-4                                        2,048
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       302,309,376
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                1,049,600
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                3,075
==========================================================================================
Total params: 355,362,819
Trainable params: 355,362,819
Non-trainable params: 0
==========================================================================================
RobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 1024, padding_idx=1)
      (position_embeddings): Embedding(514, 1024, padding_idx=1)
      (token_type_embeddings): Embedding(1, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-23): 24 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSdpaSelfAttention(
              (query): TLoRALayer(
                (linear): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.5, inplace=False)
              )
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): TLoRALayer(
                (linear): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.5, inplace=False)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHead(
    (dense): Linear(in_features=1024, out_features=1024, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=1024, out_features=3, bias=True)
  )
)
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        (51,471,360)
│    │    └─Embedding: 3-2                                        (526,336)
│    │    └─Embedding: 3-3                                        (1,024)
│    │    └─LayerNorm: 3-4                                        (2,048)
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       305,504,304
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                (1,049,600)
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                (3,075)
==========================================================================================
Total params: 358,557,747
Trainable params: 49,200
Non-trainable params: 358,508,547
==========================================================================================
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        (51,471,360)
│    │    └─Embedding: 3-2                                        (526,336)
│    │    └─Embedding: 3-3                                        (1,024)
│    │    └─LayerNorm: 3-4                                        (2,048)
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       305,504,304
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                (1,049,600)
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                (3,075)
==========================================================================================
Total params: 358,557,747
Trainable params: 49,200
Non-trainable params: 358,508,547
==========================================================================================
Epoch 1/30
Batch 53/268 - Loss: 0.6940 - Accuracy: 0.7500
Batch 106/268 - Loss: 0.6705 - Accuracy: 0.7188
Batch 159/268 - Loss: 0.8683 - Accuracy: 0.5000
Batch 212/268 - Loss: 0.5794 - Accuracy: 0.7812
Batch 265/268 - Loss: 0.5605 - Accuracy: 0.6875
Scalings = [(0.13, 0.16, 1.01, 1.03), (0.13, 0.15, 1.0, 1.02), (0.13, 0.15, 1.01, 1.01), (0.14, 0.17, 1.02, 1.02), (0.13, 0.15, 1.02, 1.02), (0.11, 0.16, 1.0, 1.02), (0.12, 0.2, 1.01, 1.03), (0.14, 0.16, 1.01, 1.01), (0.18, 0.24, 1.02, 1.02), (0.15, 0.21, 1.01, 1.02), (0.15, 0.2, 1.02, 1.02), (0.19, 0.27, 1.03, 1.04), (0.18, 0.25, 1.02, 1.03), (0.27, 0.24, 1.05, 1.02), (0.23, 0.34, 1.03, 1.06), (0.28, 0.34, 1.05, 1.04), (0.28, 0.39, 1.05, 1.04), (0.27, 0.41, 1.04, 1.05), (0.29, 0.37, 1.03, 1.04), (0.28, 0.43, 1.04, 1.05), (0.33, 0.46, 1.05, 1.06), (0.24, 0.46, 1.04, 1.06), (0.27, 0.54, 1.05, 1.06), (0.16, 0.64, 1.03, 1.08)]
Epoch 1 - Train Loss: 0.7720 - Train Acc: 0.6694 - Val Loss: 0.6264 - Val Acc: 0.7066 - Val MCC: 0.17
Epoch 2/30
Batch 53/268 - Loss: 0.5982 - Accuracy: 0.7500
Batch 106/268 - Loss: 0.6455 - Accuracy: 0.6250
Batch 159/268 - Loss: 0.6921 - Accuracy: 0.6250
Batch 212/268 - Loss: 0.5424 - Accuracy: 0.7188
Batch 265/268 - Loss: 0.6025 - Accuracy: 0.8125
Scalings = [(0.28, 0.34, 1.01, 1.06), (0.28, 0.33, 1.0, 1.04), (0.29, 0.31, 1.01, 1.02), (0.31, 0.34, 1.05, 1.04), (0.29, 0.28, 1.02, 1.02), (0.23, 0.31, 1.0, 1.05), (0.25, 0.39, 1.02, 1.07), (0.23, 0.35, 1.0, 1.05), (0.33, 0.41, 1.03, 1.04), (0.29, 0.38, 1.02, 1.04), (0.29, 0.41, 1.01, 1.04), (0.41, 0.49, 1.06, 1.08), (0.34, 0.39, 1.03, 1.03), (0.56, 0.42, 1.11, 1.02), (0.43, 0.54, 1.06, 1.08), (0.54, 0.59, 1.09, 1.07), (0.54, 0.63, 1.11, 1.07), (0.51, 0.73, 1.08, 1.1), (0.49, 0.67, 1.05, 1.08), (0.67, 0.71, 1.14, 1.09), (0.66, 0.72, 1.12, 1.08), (0.52, 0.74, 1.11, 1.11), (0.41, 0.84, 1.06, 1.1), (0.4, 0.96, 1.05, 1.13)]
Epoch 2 - Train Loss: 0.5889 - Train Acc: 0.7093 - Val Loss: 0.5923 - Val Acc: 0.7210 - Val MCC: 0.25
Epoch 3/30
Batch 53/268 - Loss: 0.5546 - Accuracy: 0.6875
Batch 106/268 - Loss: 0.6756 - Accuracy: 0.5938
Batch 159/268 - Loss: 0.4990 - Accuracy: 0.7812
Batch 212/268 - Loss: 0.5418 - Accuracy: 0.7812
Batch 265/268 - Loss: 0.3942 - Accuracy: 0.8125
Scalings = [(0.52, 0.59, 1.04, 1.09), (0.53, 0.51, 1.02, 1.02), (0.58, 0.49, 1.06, 1.01), (0.52, 0.42, 1.04, 0.99), (0.56, 0.47, 1.06, 1.03), (0.43, 0.5, 1.02, 1.06), (0.51, 0.54, 1.05, 1.07), (0.53, 0.59, 1.07, 1.07), (0.55, 0.67, 1.03, 1.06), (0.61, 0.58, 1.09, 1.04), (0.57, 0.57, 1.04, 1.03), (0.9, 0.59, 1.19, 1.05), (0.65, 0.57, 1.1, 1.02), (0.92, 0.68, 1.18, 1.02), (0.75, 0.65, 1.12, 1.05), (0.9, 0.88, 1.15, 1.09), (0.85, 0.81, 1.18, 1.06), (0.77, 1.03, 1.13, 1.14), (1.14, 1.03, 1.23, 1.13), (1.11, 0.97, 1.26, 1.12), (1.09, 1.05, 1.23, 1.11), (0.99, 1.03, 1.26, 1.16), (0.66, 1.09, 1.1, 1.12), (0.58, 1.07, 1.04, 1.13)]
Epoch 3 - Train Loss: 0.5507 - Train Acc: 0.7246 - Val Loss: 0.6638 - Val Acc: 0.7191 - Val MCC: 0.24
Epoch 4/30
Batch 53/268 - Loss: 0.4919 - Accuracy: 0.8750
Batch 106/268 - Loss: 0.7061 - Accuracy: 0.6250
Batch 159/268 - Loss: 0.4941 - Accuracy: 0.7812
Batch 212/268 - Loss: 0.5428 - Accuracy: 0.7188
Batch 265/268 - Loss: 0.4143 - Accuracy: 0.8125
Scalings = [(0.77, 0.86, 1.03, 1.11), (0.86, 0.7, 1.07, 0.99), (0.87, 0.66, 1.09, 0.97), (0.77, 0.55, 1.04, 0.93), (0.9, 0.58, 1.11, 0.98), (0.73, 0.73, 1.07, 1.06), (0.75, 0.72, 1.05, 1.06), (0.86, 0.77, 1.13, 1.07), (0.79, 0.76, 1.02, 1.04), (0.78, 0.78, 1.07, 1.03), (0.87, 0.69, 1.07, 1.0), (1.16, 0.79, 1.19, 1.04), (0.98, 0.78, 1.16, 1.01), (1.23, 0.8, 1.23, 0.98), (0.94, 0.81, 1.12, 1.04), (1.19, 1.12, 1.2, 1.11), (1.1, 0.93, 1.21, 1.05), (1.09, 1.26, 1.2, 1.15), (1.49, 1.31, 1.31, 1.16), (1.57, 1.28, 1.37, 1.17), (1.57, 1.35, 1.33, 1.15), (1.47, 1.37, 1.38, 1.24), (1.0, 1.38, 1.15, 1.15), (0.84, 1.29, 1.08, 1.15)]
Epoch 4 - Train Loss: 0.5176 - Train Acc: 0.7527 - Val Loss: 0.6490 - Val Acc: 0.7450 - Val MCC: 0.34
Epoch 5/30
Batch 53/268 - Loss: 0.4829 - Accuracy: 0.7188
Batch 106/268 - Loss: 0.5679 - Accuracy: 0.7500
Batch 159/268 - Loss: 0.5903 - Accuracy: 0.7188
Batch 212/268 - Loss: 0.4173 - Accuracy: 0.8438
Batch 265/268 - Loss: 0.4884 - Accuracy: 0.7500
Scalings = [(1.02, 1.04, 1.11, 1.11), (1.09, 0.86, 1.1, 0.97), (1.14, 0.77, 1.14, 0.93), (0.97, 0.72, 1.05, 0.9), (1.12, 0.72, 1.13, 0.96), (0.95, 0.8, 1.09, 1.03), (0.93, 0.76, 1.05, 1.02), (1.05, 0.79, 1.13, 1.04), (1.05, 0.77, 1.05, 1.0), (0.96, 0.83, 1.06, 0.98), (1.06, 0.77, 1.07, 0.97), (1.48, 0.82, 1.22, 1.0), (1.17, 0.9, 1.16, 0.99), (1.47, 0.9, 1.25, 0.93), (1.15, 0.87, 1.14, 1.01), (1.44, 1.25, 1.24, 1.11), (1.31, 1.04, 1.24, 1.04), (1.23, 1.48, 1.2, 1.17), (1.71, 1.48, 1.34, 1.17), (1.86, 1.53, 1.42, 1.21), (1.75, 1.63, 1.35, 1.2), (1.88, 1.71, 1.46, 1.31), (1.3, 1.7, 1.19, 1.22), (1.15, 1.51, 1.14, 1.19)]
Epoch 5 - Train Loss: 0.4954 - Train Acc: 0.7662 - Val Loss: 0.6254 - Val Acc: 0.7536 - Val MCC: 0.36
Epoch 6/30
Batch 53/268 - Loss: 0.3483 - Accuracy: 0.8438
Batch 106/268 - Loss: 0.5790 - Accuracy: 0.7500
Batch 159/268 - Loss: 0.4935 - Accuracy: 0.8438
Batch 212/268 - Loss: 0.4067 - Accuracy: 0.8438
Batch 265/268 - Loss: 0.6005 - Accuracy: 0.6875
Scalings = [(1.22, 1.21, 1.16, 1.12), (1.27, 0.95, 1.12, 0.94), (1.29, 0.87, 1.13, 0.9), (1.11, 0.78, 1.05, 0.85), (1.21, 0.73, 1.11, 0.91), (1.2, 0.87, 1.15, 1.0), (1.11, 0.8, 1.07, 0.98), (1.19, 0.8, 1.13, 1.01), (1.22, 0.86, 1.06, 0.97), (1.2, 0.98, 1.1, 0.97), (1.24, 0.89, 1.08, 0.94), (1.65, 0.94, 1.23, 0.98), (1.41, 1.0, 1.19, 0.97), (1.68, 1.02, 1.28, 0.91), (1.39, 0.98, 1.17, 0.99), (1.72, 1.29, 1.3, 1.09), (1.39, 1.09, 1.23, 1.03), (1.45, 1.53, 1.24, 1.15), (1.88, 1.54, 1.36, 1.16), (2.06, 1.7, 1.45, 1.25), (1.99, 1.83, 1.38, 1.23), (2.2, 2.01, 1.53, 1.36), (1.6, 1.97, 1.24, 1.28), (1.42, 1.74, 1.19, 1.24)]
Epoch 6 - Train Loss: 0.4865 - Train Acc: 0.7751 - Val Loss: 0.6121 - Val Acc: 0.7622 - Val MCC: 0.39
Epoch 7/30
Batch 53/268 - Loss: 0.5082 - Accuracy: 0.7500
Batch 106/268 - Loss: 0.5893 - Accuracy: 0.7188
Batch 159/268 - Loss: 0.5837 - Accuracy: 0.7188
Batch 212/268 - Loss: 0.6621 - Accuracy: 0.7188
Batch 265/268 - Loss: 0.3361 - Accuracy: 0.8750
Scalings = [(1.38, 1.33, 1.19, 1.12), (1.33, 1.02, 1.09, 0.9), (1.4, 0.88, 1.13, 0.86), (1.33, 0.87, 1.06, 0.82), (1.33, 0.81, 1.11, 0.88), (1.38, 0.94, 1.17, 0.98), (1.2, 0.88, 1.04, 0.95), (1.31, 0.89, 1.1, 1.0), (1.28, 0.9, 1.04, 0.95), (1.32, 1.0, 1.09, 0.93), (1.43, 0.95, 1.1, 0.91), (1.8, 0.97, 1.23, 0.96), (1.52, 1.09, 1.19, 0.96), (1.75, 1.1, 1.27, 0.89), (1.51, 1.07, 1.18, 0.98), (1.99, 1.39, 1.36, 1.08), (1.51, 1.21, 1.24, 1.03), (1.58, 1.63, 1.26, 1.16), (2.0, 1.68, 1.37, 1.18), (2.18, 1.88, 1.46, 1.28), (2.19, 1.99, 1.42, 1.26), (2.37, 2.19, 1.55, 1.38), (1.82, 2.14, 1.27, 1.31), (1.47, 1.85, 1.17, 1.26)]
Epoch 7 - Train Loss: 0.4727 - Train Acc: 0.7817 - Val Loss: 0.6356 - Val Acc: 0.7661 - Val MCC: 0.40
Epoch 8/30
Batch 53/268 - Loss: 0.3800 - Accuracy: 0.8438
Batch 106/268 - Loss: 0.5715 - Accuracy: 0.7188
Batch 159/268 - Loss: 0.4444 - Accuracy: 0.8438
Batch 212/268 - Loss: 0.4226 - Accuracy: 0.7812
Batch 265/268 - Loss: 0.4886 - Accuracy: 0.6875
Scalings = [(1.5, 1.47, 1.22, 1.15), (1.46, 1.13, 1.12, 0.9), (1.5, 0.98, 1.14, 0.86), (1.45, 0.89, 1.09, 0.79), (1.47, 0.93, 1.14, 0.88), (1.48, 1.0, 1.18, 0.97), (1.29, 0.9, 1.03, 0.93), (1.39, 0.94, 1.09, 0.98), (1.32, 0.99, 1.02, 0.94), (1.44, 1.09, 1.1, 0.92), (1.57, 1.04, 1.11, 0.91), (1.88, 1.08, 1.23, 0.96), (1.71, 1.27, 1.23, 0.97), (1.87, 1.18, 1.27, 0.87), (1.63, 1.14, 1.19, 0.96), (2.19, 1.49, 1.4, 1.09), (1.65, 1.29, 1.26, 1.02), (1.76, 1.64, 1.29, 1.15), (2.04, 1.8, 1.36, 1.18), (2.34, 2.03, 1.49, 1.32), (2.37, 2.09, 1.44, 1.27), (2.45, 2.4, 1.56, 1.42), (1.91, 2.24, 1.28, 1.34), (1.64, 2.01, 1.19, 1.3)]
Epoch 8 - Train Loss: 0.4661 - Train Acc: 0.7927 - Val Loss: 0.5969 - Val Acc: 0.7891 - Val MCC: 0.47
Epoch 9/30
Batch 53/268 - Loss: 0.5528 - Accuracy: 0.7812
Batch 106/268 - Loss: 0.6937 - Accuracy: 0.6250
Batch 159/268 - Loss: 0.4155 - Accuracy: 0.8125
Batch 212/268 - Loss: 0.3289 - Accuracy: 0.8750
Batch 265/268 - Loss: 0.4864 - Accuracy: 0.8125
Scalings = [(1.62, 1.55, 1.25, 1.15), (1.53, 1.25, 1.11, 0.91), (1.63, 1.06, 1.17, 0.85), (1.6, 0.94, 1.12, 0.77), (1.57, 0.96, 1.14, 0.85), (1.69, 1.03, 1.22, 0.95), (1.35, 0.96, 1.03, 0.91), (1.44, 0.94, 1.09, 0.96), (1.4, 1.0, 1.03, 0.92), (1.54, 1.15, 1.11, 0.9), (1.7, 1.1, 1.12, 0.89), (1.99, 1.16, 1.24, 0.95), (1.83, 1.33, 1.25, 0.95), (2.01, 1.27, 1.29, 0.87), (1.69, 1.19, 1.18, 0.95), (2.28, 1.57, 1.4, 1.09), (1.71, 1.36, 1.26, 1.02), (1.8, 1.75, 1.29, 1.15), (2.08, 1.88, 1.36, 1.18), (2.49, 2.13, 1.51, 1.33), (2.47, 2.19, 1.45, 1.28), (2.55, 2.52, 1.57, 1.44), (2.0, 2.34, 1.28, 1.36), (1.65, 2.11, 1.17, 1.32)]
Epoch 9 - Train Loss: 0.4600 - Train Acc: 0.7922 - Val Loss: 0.6338 - Val Acc: 0.7804 - Val MCC: 0.45
Epoch 10/30
Batch 53/268 - Loss: 0.4538 - Accuracy: 0.8438
Batch 106/268 - Loss: 0.5764 - Accuracy: 0.7188
Batch 159/268 - Loss: 0.4771 - Accuracy: 0.7500
Batch 212/268 - Loss: 0.7747 - Accuracy: 0.5938
Batch 265/268 - Loss: 0.4235 - Accuracy: 0.7812
Scalings = [(1.71, 1.67, 1.27, 1.19), (1.64, 1.29, 1.13, 0.89), (1.73, 1.12, 1.18, 0.84), (1.67, 1.04, 1.12, 0.77), (1.73, 0.98, 1.17, 0.83), (1.75, 1.07, 1.23, 0.94), (1.46, 1.03, 1.05, 0.91), (1.52, 1.0, 1.09, 0.95), (1.48, 1.06, 1.02, 0.91), (1.62, 1.23, 1.11, 0.89), (1.75, 1.17, 1.11, 0.88), (2.08, 1.21, 1.24, 0.93), (1.91, 1.41, 1.25, 0.95), (2.03, 1.36, 1.28, 0.87), (1.82, 1.26, 1.21, 0.94), (2.34, 1.64, 1.4, 1.09), (1.83, 1.35, 1.28, 0.99), (1.86, 1.79, 1.29, 1.15), (2.15, 1.92, 1.37, 1.18), (2.55, 2.25, 1.51, 1.37), (2.56, 2.3, 1.47, 1.3), (2.58, 2.64, 1.57, 1.46), (2.19, 2.48, 1.31, 1.4), (1.74, 2.19, 1.18, 1.34)]
Epoch 10 - Train Loss: 0.4564 - Train Acc: 0.7953 - Val Loss: 0.5813 - Val Acc: 0.7862 - Val MCC: 0.46
Epoch 11/30
Batch 53/268 - Loss: 0.2640 - Accuracy: 0.8750
Batch 106/268 - Loss: 0.3249 - Accuracy: 0.7812
Batch 159/268 - Loss: 0.5027 - Accuracy: 0.7500
Batch 212/268 - Loss: 0.3514 - Accuracy: 0.8438
Batch 265/268 - Loss: 0.4632 - Accuracy: 0.8125
Scalings = [(1.77, 1.72, 1.28, 1.19), (1.69, 1.36, 1.14, 0.89), (1.77, 1.12, 1.18, 0.81), (1.72, 1.11, 1.12, 0.78), (1.8, 1.01, 1.18, 0.81), (1.76, 1.11, 1.22, 0.93), (1.51, 1.05, 1.05, 0.89), (1.57, 1.04, 1.09, 0.94), (1.52, 1.11, 1.02, 0.9), (1.7, 1.26, 1.12, 0.87), (1.87, 1.2, 1.13, 0.87), (2.15, 1.24, 1.24, 0.92), (1.99, 1.45, 1.25, 0.94), (2.11, 1.41, 1.29, 0.86), (1.84, 1.31, 1.2, 0.93), (2.42, 1.68, 1.41, 1.09), (1.92, 1.4, 1.29, 0.99), (1.94, 1.87, 1.3, 1.15), (2.22, 1.95, 1.37, 1.18), (2.65, 2.34, 1.52, 1.4), (2.69, 2.39, 1.49, 1.31), (2.66, 2.79, 1.58, 1.48), (2.31, 2.53, 1.33, 1.41), (1.77, 2.3, 1.18, 1.37)]
Epoch 11 - Train Loss: 0.4504 - Train Acc: 0.7959 - Val Loss: 0.5675 - Val Acc: 0.7862 - Val MCC: 0.46
Epoch 12/30
Batch 53/268 - Loss: 0.4772 - Accuracy: 0.7812
Batch 106/268 - Loss: 0.3526 - Accuracy: 0.7812
Batch 159/268 - Loss: 0.4843 - Accuracy: 0.8125
Batch 212/268 - Loss: 0.3103 - Accuracy: 0.8750
Batch 265/268 - Loss: 0.3326 - Accuracy: 0.8438
Scalings = [(1.84, 1.81, 1.31, 1.21), (1.77, 1.4, 1.15, 0.89), (1.81, 1.15, 1.17, 0.8), (1.8, 1.15, 1.13, 0.76), (1.88, 1.03, 1.19, 0.79), (1.9, 1.15, 1.24, 0.93), (1.56, 1.11, 1.05, 0.88), (1.61, 1.03, 1.08, 0.92), (1.6, 1.15, 1.03, 0.89), (1.79, 1.3, 1.14, 0.86), (1.97, 1.26, 1.15, 0.86), (2.24, 1.33, 1.25, 0.92), (2.1, 1.51, 1.27, 0.94), (2.16, 1.49, 1.28, 0.86), (1.95, 1.35, 1.22, 0.92), (2.48, 1.71, 1.42, 1.08), (2.01, 1.43, 1.3, 0.98), (2.0, 1.91, 1.31, 1.15), (2.31, 1.97, 1.39, 1.17), (2.72, 2.39, 1.53, 1.4), (2.77, 2.44, 1.5, 1.31), (2.77, 2.93, 1.6, 1.5), (2.38, 2.65, 1.34, 1.44), (1.82, 2.35, 1.18, 1.38)]
Epoch 12 - Train Loss: 0.4476 - Train Acc: 0.7969 - Val Loss: 0.5977 - Val Acc: 0.7929 - Val MCC: 0.48
Epoch 13/30
Batch 53/268 - Loss: 0.4086 - Accuracy: 0.8438
Batch 106/268 - Loss: 0.4612 - Accuracy: 0.7500
Batch 159/268 - Loss: 0.4212 - Accuracy: 0.8438
Batch 212/268 - Loss: 0.4530 - Accuracy: 0.7500
Batch 265/268 - Loss: 0.3318 - Accuracy: 0.8750
Scalings = [(1.89, 1.89, 1.32, 1.22), (1.8, 1.45, 1.14, 0.88), (1.85, 1.21, 1.17, 0.79), (1.83, 1.18, 1.12, 0.74), (1.95, 1.09, 1.18, 0.78), (2.04, 1.2, 1.26, 0.91), (1.59, 1.16, 1.04, 0.88), (1.7, 1.03, 1.1, 0.91), (1.63, 1.16, 1.02, 0.87), (1.83, 1.31, 1.14, 0.84), (2.08, 1.32, 1.17, 0.85), (2.3, 1.39, 1.26, 0.92), (2.15, 1.56, 1.27, 0.94), (2.21, 1.52, 1.28, 0.85), (2.01, 1.37, 1.22, 0.91), (2.59, 1.76, 1.44, 1.08), (2.05, 1.45, 1.3, 0.97), (2.07, 1.94, 1.32, 1.15), (2.36, 2.0, 1.4, 1.17), (2.8, 2.48, 1.54, 1.43), (2.85, 2.51, 1.51, 1.32), (2.84, 2.98, 1.61, 1.51), (2.43, 2.74, 1.35, 1.46), (1.84, 2.42, 1.18, 1.39)]
Epoch 13 - Train Loss: 0.4453 - Train Acc: 0.8029 - Val Loss: 0.6300 - Val Acc: 0.7824 - Val MCC: 0.45
Epoch 14/30
Batch 53/268 - Loss: 0.5404 - Accuracy: 0.6875
Batch 106/268 - Loss: 0.3334 - Accuracy: 0.8750
Batch 159/268 - Loss: 0.4075 - Accuracy: 0.8125
Batch 212/268 - Loss: 0.5099 - Accuracy: 0.6562
Batch 265/268 - Loss: 0.3602 - Accuracy: 0.8750
Scalings = [(1.94, 1.9, 1.33, 1.21), (1.85, 1.46, 1.14, 0.86), (1.87, 1.21, 1.16, 0.77), (1.85, 1.25, 1.12, 0.74), (1.96, 1.11, 1.17, 0.75), (2.03, 1.26, 1.24, 0.91), (1.65, 1.18, 1.03, 0.86), (1.77, 1.05, 1.11, 0.9), (1.71, 1.2, 1.03, 0.86), (1.88, 1.38, 1.14, 0.84), (2.15, 1.38, 1.18, 0.85), (2.36, 1.42, 1.26, 0.91), (2.2, 1.56, 1.27, 0.93), (2.28, 1.56, 1.29, 0.85), (2.06, 1.4, 1.23, 0.89), (2.65, 1.8, 1.45, 1.08), (2.11, 1.47, 1.31, 0.97), (2.15, 1.97, 1.34, 1.15), (2.39, 2.05, 1.39, 1.17), (2.9, 2.53, 1.57, 1.44), (2.92, 2.54, 1.52, 1.32), (2.87, 3.08, 1.61, 1.52), (2.49, 2.79, 1.36, 1.47), (1.88, 2.46, 1.19, 1.4)]
Epoch 14 - Train Loss: 0.4339 - Train Acc: 0.8053 - Val Loss: 0.6984 - Val Acc: 0.7747 - Val MCC: 0.43
Epoch 15/30
Batch 53/268 - Loss: 0.2821 - Accuracy: 0.9062
Batch 106/268 - Loss: 0.4286 - Accuracy: 0.7812
Batch 159/268 - Loss: 0.4590 - Accuracy: 0.7812
Batch 212/268 - Loss: 0.4296 - Accuracy: 0.8438
Batch 265/268 - Loss: 0.4202 - Accuracy: 0.8125
Scalings = [(2.0, 1.93, 1.35, 1.21), (1.89, 1.49, 1.16, 0.86), (1.9, 1.26, 1.17, 0.77), (1.88, 1.24, 1.12, 0.73), (1.98, 1.11, 1.17, 0.74), (2.09, 1.26, 1.25, 0.89), (1.67, 1.23, 1.03, 0.86), (1.84, 1.08, 1.12, 0.89), (1.72, 1.23, 1.02, 0.86), (1.95, 1.43, 1.15, 0.84), (2.18, 1.43, 1.17, 0.86), (2.4, 1.46, 1.26, 0.91), (2.27, 1.61, 1.29, 0.93), (2.37, 1.62, 1.3, 0.85), (2.12, 1.43, 1.24, 0.89), (2.68, 1.83, 1.45, 1.08), (2.17, 1.48, 1.32, 0.96), (2.21, 1.97, 1.34, 1.14), (2.44, 2.07, 1.4, 1.17), (2.93, 2.59, 1.56, 1.46), (2.96, 2.6, 1.53, 1.33), (2.95, 3.17, 1.63, 1.54), (2.59, 2.87, 1.37, 1.5), (1.99, 2.55, 1.21, 1.44)]
Epoch 15 - Train Loss: 0.4331 - Train Acc: 0.8107 - Val Loss: 0.6575 - Val Acc: 0.7747 - Val MCC: 0.43
Epoch 16/30
Batch 53/268 - Loss: 0.4054 - Accuracy: 0.8438
Batch 106/268 - Loss: 0.5676 - Accuracy: 0.6562
Batch 159/268 - Loss: 0.4429 - Accuracy: 0.7812
Batch 212/268 - Loss: 0.5807 - Accuracy: 0.7500
Batch 265/268 - Loss: 0.4437 - Accuracy: 0.7812
Scalings = [(2.04, 1.97, 1.37, 1.22), (1.94, 1.52, 1.18, 0.85), (1.97, 1.3, 1.18, 0.77), (1.93, 1.29, 1.12, 0.74), (2.0, 1.12, 1.17, 0.73), (2.12, 1.27, 1.25, 0.88), (1.7, 1.28, 1.03, 0.86), (1.91, 1.09, 1.14, 0.88), (1.77, 1.25, 1.02, 0.85), (1.97, 1.47, 1.15, 0.84), (2.21, 1.46, 1.17, 0.84), (2.45, 1.49, 1.27, 0.9), (2.33, 1.64, 1.3, 0.93), (2.39, 1.67, 1.3, 0.85), (2.18, 1.46, 1.25, 0.89), (2.74, 1.9, 1.46, 1.09), (2.25, 1.52, 1.33, 0.96), (2.24, 2.02, 1.34, 1.15), (2.46, 2.12, 1.4, 1.17), (2.96, 2.63, 1.56, 1.47), (3.01, 2.63, 1.53, 1.33), (2.98, 3.22, 1.63, 1.55), (2.58, 2.9, 1.36, 1.51), (1.99, 2.6, 1.21, 1.45)]
Epoch 16 - Train Loss: 0.4283 - Train Acc: 0.8090 - Val Loss: 0.6915 - Val Acc: 0.7718 - Val MCC: 0.42
Epoch 17/30
Batch 53/268 - Loss: 0.3835 - Accuracy: 0.7812
Batch 106/268 - Loss: 0.4450 - Accuracy: 0.7812
Batch 159/268 - Loss: 0.4225 - Accuracy: 0.8438
Batch 212/268 - Loss: 0.4064 - Accuracy: 0.8438
Batch 265/268 - Loss: 0.3613 - Accuracy: 0.8438
Scalings = [(2.07, 2.01, 1.38, 1.22), (1.99, 1.54, 1.19, 0.85), (2.0, 1.31, 1.19, 0.77), (1.95, 1.31, 1.12, 0.74), (2.01, 1.13, 1.17, 0.72), (2.15, 1.3, 1.25, 0.88), (1.73, 1.27, 1.04, 0.84), (1.94, 1.11, 1.15, 0.87), (1.81, 1.26, 1.03, 0.84), (1.99, 1.51, 1.15, 0.84), (2.27, 1.5, 1.18, 0.85), (2.48, 1.53, 1.27, 0.9), (2.39, 1.69, 1.31, 0.93), (2.45, 1.7, 1.31, 0.84), (2.19, 1.5, 1.24, 0.89), (2.81, 1.93, 1.47, 1.09), (2.26, 1.55, 1.33, 0.96), (2.28, 2.05, 1.35, 1.15), (2.51, 2.13, 1.41, 1.17), (2.99, 2.65, 1.57, 1.47), (3.04, 2.66, 1.54, 1.33), (3.02, 3.25, 1.63, 1.55), (2.59, 2.9, 1.36, 1.51), (2.05, 2.65, 1.22, 1.46)]
Epoch 17 - Train Loss: 0.4247 - Train Acc: 0.8128 - Val Loss: 0.6563 - Val Acc: 0.7795 - Val MCC: 0.44
Epoch 18/30
Batch 53/268 - Loss: 0.4711 - Accuracy: 0.7188
Batch 106/268 - Loss: 0.3953 - Accuracy: 0.8750
Batch 159/268 - Loss: 0.3144 - Accuracy: 0.8438
Batch 212/268 - Loss: 0.3455 - Accuracy: 0.8438
Batch 265/268 - Loss: 0.3116 - Accuracy: 0.9062
Scalings = [(2.11, 2.05, 1.4, 1.24), (2.01, 1.56, 1.19, 0.85), (2.01, 1.32, 1.18, 0.76), (1.98, 1.32, 1.13, 0.73), (2.03, 1.15, 1.17, 0.72), (2.16, 1.33, 1.25, 0.88), (1.76, 1.28, 1.04, 0.84), (1.99, 1.15, 1.16, 0.87), (1.84, 1.27, 1.03, 0.83), (2.04, 1.54, 1.15, 0.84), (2.33, 1.53, 1.19, 0.84), (2.55, 1.56, 1.28, 0.9), (2.45, 1.73, 1.32, 0.93), (2.49, 1.73, 1.31, 0.84), (2.23, 1.53, 1.25, 0.89), (2.86, 1.94, 1.49, 1.08), (2.28, 1.58, 1.33, 0.96), (2.32, 2.05, 1.36, 1.15), (2.55, 2.13, 1.42, 1.16), (3.04, 2.69, 1.58, 1.49), (3.08, 2.7, 1.54, 1.34), (3.07, 3.31, 1.64, 1.56), (2.61, 2.95, 1.36, 1.52), (2.06, 2.68, 1.22, 1.47)]
Epoch 18 - Train Loss: 0.4208 - Train Acc: 0.8145 - Val Loss: 0.6271 - Val Acc: 0.7843 - Val MCC: 0.46
Epoch 19/30
Batch 53/268 - Loss: 0.3587 - Accuracy: 0.8750
Batch 106/268 - Loss: 0.2402 - Accuracy: 0.9062
Batch 159/268 - Loss: 0.3923 - Accuracy: 0.8125
Batch 212/268 - Loss: 0.3008 - Accuracy: 0.8750
Batch 265/268 - Loss: 0.4055 - Accuracy: 0.8438
Scalings = [(2.15, 2.06, 1.41, 1.23), (2.06, 1.58, 1.22, 0.86), (1.99, 1.35, 1.17, 0.76), (2.02, 1.3, 1.14, 0.72), (2.05, 1.17, 1.17, 0.72), (2.18, 1.32, 1.25, 0.87), (1.78, 1.32, 1.04, 0.84), (2.02, 1.17, 1.16, 0.87), (1.86, 1.32, 1.03, 0.84), (2.08, 1.58, 1.16, 0.84), (2.38, 1.57, 1.19, 0.85), (2.59, 1.59, 1.29, 0.9), (2.5, 1.76, 1.33, 0.94), (2.52, 1.75, 1.31, 0.84), (2.28, 1.56, 1.25, 0.89), (2.88, 1.94, 1.49, 1.08), (2.28, 1.61, 1.33, 0.96), (2.33, 2.07, 1.36, 1.15), (2.56, 2.16, 1.42, 1.17), (3.07, 2.72, 1.59, 1.5), (3.09, 2.75, 1.54, 1.35), (3.07, 3.36, 1.64, 1.57), (2.62, 2.97, 1.36, 1.53), (2.11, 2.7, 1.23, 1.47)]
Epoch 19 - Train Loss: 0.4154 - Train Acc: 0.8169 - Val Loss: 0.6253 - Val Acc: 0.7910 - Val MCC: 0.48
Epoch 20/30
Batch 53/268 - Loss: 0.3530 - Accuracy: 0.7812
Batch 106/268 - Loss: 0.4498 - Accuracy: 0.7812
Batch 159/268 - Loss: 0.3899 - Accuracy: 0.8125
Batch 212/268 - Loss: 0.3885 - Accuracy: 0.8438
Batch 265/268 - Loss: 0.4676 - Accuracy: 0.6875
Scalings = [(2.17, 2.07, 1.43, 1.23), (2.1, 1.59, 1.23, 0.85), (2.03, 1.36, 1.18, 0.76), (2.04, 1.31, 1.14, 0.72), (2.06, 1.19, 1.17, 0.72), (2.2, 1.33, 1.25, 0.86), (1.8, 1.33, 1.05, 0.83), (2.05, 1.18, 1.16, 0.86), (1.9, 1.34, 1.04, 0.84), (2.12, 1.6, 1.17, 0.84), (2.42, 1.6, 1.2, 0.85), (2.61, 1.61, 1.29, 0.9), (2.52, 1.78, 1.34, 0.94), (2.54, 1.78, 1.31, 0.84), (2.31, 1.59, 1.26, 0.89), (2.91, 1.96, 1.49, 1.08), (2.31, 1.63, 1.33, 0.97), (2.35, 2.1, 1.36, 1.15), (2.59, 2.17, 1.42, 1.17), (3.1, 2.75, 1.59, 1.51), (3.12, 2.76, 1.55, 1.35), (3.11, 3.39, 1.65, 1.58), (2.65, 3.0, 1.36, 1.54), (2.14, 2.72, 1.23, 1.48)]
Epoch 20 - Train Loss: 0.4202 - Train Acc: 0.8155 - Val Loss: 0.6143 - Val Acc: 0.7939 - Val MCC: 0.49
Epoch 21/30
Batch 53/268 - Loss: 0.3559 - Accuracy: 0.8750
Batch 106/268 - Loss: 0.3276 - Accuracy: 0.8438
Batch 159/268 - Loss: 0.2764 - Accuracy: 0.8750
Batch 212/268 - Loss: 0.2873 - Accuracy: 0.8750
Batch 265/268 - Loss: 0.4835 - Accuracy: 0.7500
Scalings = [(2.2, 2.08, 1.45, 1.23), (2.12, 1.62, 1.24, 0.86), (2.02, 1.38, 1.18, 0.76), (2.06, 1.31, 1.14, 0.71), (2.07, 1.19, 1.17, 0.71), (2.24, 1.36, 1.26, 0.87), (1.82, 1.35, 1.05, 0.84), (2.05, 1.2, 1.16, 0.86), (1.91, 1.36, 1.04, 0.84), (2.14, 1.61, 1.18, 0.84), (2.44, 1.62, 1.2, 0.85), (2.63, 1.63, 1.3, 0.9), (2.55, 1.81, 1.34, 0.94), (2.57, 1.8, 1.32, 0.84), (2.32, 1.59, 1.26, 0.88), (2.92, 1.99, 1.49, 1.09), (2.36, 1.63, 1.34, 0.96), (2.36, 2.14, 1.36, 1.15), (2.59, 2.2, 1.42, 1.17), (3.11, 2.77, 1.59, 1.52), (3.12, 2.78, 1.55, 1.35), (3.1, 3.4, 1.64, 1.58), (2.67, 3.02, 1.36, 1.55), (2.17, 2.74, 1.24, 1.49)]
Epoch 21 - Train Loss: 0.4194 - Train Acc: 0.8165 - Val Loss: 0.5957 - Val Acc: 0.7910 - Val MCC: 0.48
Epoch 22/30
Batch 53/268 - Loss: 0.4014 - Accuracy: 0.8125
Batch 106/268 - Loss: 0.4200 - Accuracy: 0.7500
Batch 159/268 - Loss: 0.3920 - Accuracy: 0.8750
Batch 212/268 - Loss: 0.3278 - Accuracy: 0.8438
Batch 265/268 - Loss: 0.3071 - Accuracy: 0.9062
Scalings = [(2.23, 2.09, 1.46, 1.23), (2.14, 1.63, 1.25, 0.86), (2.04, 1.39, 1.18, 0.75), (2.07, 1.31, 1.14, 0.71), (2.08, 1.19, 1.17, 0.71), (2.26, 1.37, 1.26, 0.86), (1.84, 1.36, 1.06, 0.83), (2.07, 1.23, 1.16, 0.87), (1.92, 1.39, 1.04, 0.84), (2.18, 1.63, 1.18, 0.84), (2.47, 1.65, 1.21, 0.85), (2.66, 1.65, 1.3, 0.9), (2.59, 1.83, 1.35, 0.94), (2.59, 1.83, 1.32, 0.84), (2.33, 1.6, 1.25, 0.88), (2.94, 2.0, 1.49, 1.09), (2.38, 1.65, 1.35, 0.96), (2.39, 2.16, 1.36, 1.16), (2.6, 2.22, 1.42, 1.17), (3.12, 2.79, 1.6, 1.52), (3.15, 2.8, 1.55, 1.35), (3.11, 3.42, 1.65, 1.59), (2.68, 3.05, 1.37, 1.56), (2.21, 2.78, 1.25, 1.5)]
Epoch 22 - Train Loss: 0.4111 - Train Acc: 0.8170 - Val Loss: 0.6007 - Val Acc: 0.7900 - Val MCC: 0.47
Epoch 23/30
Batch 53/268 - Loss: 0.5574 - Accuracy: 0.7188
Batch 106/268 - Loss: 0.5057 - Accuracy: 0.7188
Batch 159/268 - Loss: 0.4245 - Accuracy: 0.7188
Batch 212/268 - Loss: 0.3617 - Accuracy: 0.8750
Batch 265/268 - Loss: 0.4703 - Accuracy: 0.8125
Scalings = [(2.25, 2.11, 1.47, 1.24), (2.14, 1.63, 1.25, 0.85), (2.07, 1.41, 1.19, 0.75), (2.08, 1.32, 1.14, 0.71), (2.11, 1.21, 1.18, 0.71), (2.27, 1.38, 1.26, 0.86), (1.86, 1.37, 1.06, 0.83), (2.06, 1.23, 1.15, 0.86), (1.95, 1.41, 1.04, 0.85), (2.18, 1.64, 1.18, 0.83), (2.49, 1.67, 1.21, 0.85), (2.68, 1.66, 1.31, 0.9), (2.61, 1.85, 1.35, 0.94), (2.6, 1.86, 1.32, 0.85), (2.37, 1.61, 1.26, 0.88), (2.95, 2.0, 1.49, 1.08), (2.4, 1.67, 1.35, 0.97), (2.4, 2.17, 1.36, 1.15), (2.62, 2.23, 1.42, 1.17), (3.14, 2.8, 1.6, 1.53), (3.17, 2.82, 1.56, 1.35), (3.12, 3.43, 1.65, 1.59), (2.69, 3.06, 1.37, 1.56), (2.25, 2.82, 1.26, 1.52)]
Epoch 23 - Train Loss: 0.4164 - Train Acc: 0.8187 - Val Loss: 0.6257 - Val Acc: 0.7900 - Val MCC: 0.47
Epoch 24/30
Batch 53/268 - Loss: 0.3718 - Accuracy: 0.7500
Batch 106/268 - Loss: 0.3009 - Accuracy: 0.8750
Batch 159/268 - Loss: 0.5265 - Accuracy: 0.7188
Batch 212/268 - Loss: 0.4592 - Accuracy: 0.8125
Batch 265/268 - Loss: 0.4096 - Accuracy: 0.8750
Scalings = [(2.26, 2.12, 1.48, 1.24), (2.16, 1.64, 1.26, 0.86), (2.07, 1.41, 1.19, 0.75), (2.09, 1.31, 1.14, 0.71), (2.12, 1.23, 1.18, 0.71), (2.27, 1.39, 1.26, 0.87), (1.88, 1.38, 1.07, 0.83), (2.08, 1.23, 1.16, 0.86), (1.97, 1.41, 1.05, 0.84), (2.21, 1.66, 1.19, 0.83), (2.51, 1.68, 1.21, 0.85), (2.7, 1.67, 1.31, 0.9), (2.64, 1.86, 1.36, 0.94), (2.62, 1.87, 1.32, 0.84), (2.38, 1.62, 1.27, 0.88), (2.97, 2.02, 1.5, 1.09), (2.43, 1.69, 1.36, 0.97), (2.4, 2.18, 1.36, 1.15), (2.65, 2.25, 1.43, 1.17), (3.14, 2.82, 1.59, 1.54), (3.19, 2.84, 1.56, 1.36), (3.12, 3.43, 1.65, 1.59), (2.73, 3.06, 1.37, 1.56), (2.29, 2.84, 1.27, 1.53)]
Epoch 24 - Train Loss: 0.4084 - Train Acc: 0.8244 - Val Loss: 0.6164 - Val Acc: 0.7900 - Val MCC: 0.47
Epoch 25/30
Batch 53/268 - Loss: 0.2862 - Accuracy: 0.9062
Batch 106/268 - Loss: 0.3835 - Accuracy: 0.8125
Batch 159/268 - Loss: 0.2532 - Accuracy: 0.9688
Batch 212/268 - Loss: 0.3893 - Accuracy: 0.8125
Batch 265/268 - Loss: 0.6878 - Accuracy: 0.7188
Scalings = [(2.28, 2.12, 1.49, 1.25), (2.17, 1.66, 1.26, 0.86), (2.07, 1.42, 1.19, 0.75), (2.09, 1.32, 1.14, 0.71), (2.13, 1.22, 1.18, 0.7), (2.26, 1.39, 1.25, 0.86), (1.89, 1.37, 1.07, 0.82), (2.1, 1.22, 1.16, 0.85), (1.98, 1.41, 1.05, 0.84), (2.22, 1.67, 1.19, 0.83), (2.52, 1.69, 1.21, 0.85), (2.71, 1.68, 1.31, 0.9), (2.67, 1.88, 1.36, 0.94), (2.63, 1.88, 1.32, 0.84), (2.39, 1.64, 1.27, 0.88), (2.97, 2.03, 1.5, 1.09), (2.44, 1.68, 1.36, 0.97), (2.43, 2.18, 1.36, 1.15), (2.66, 2.26, 1.43, 1.18), (3.15, 2.84, 1.6, 1.55), (3.21, 2.85, 1.57, 1.36), (3.11, 3.43, 1.64, 1.59), (2.73, 3.06, 1.37, 1.56), (2.29, 2.83, 1.27, 1.52)]
Epoch 25 - Train Loss: 0.4130 - Train Acc: 0.8156 - Val Loss: 0.6412 - Val Acc: 0.7900 - Val MCC: 0.48
Epoch 26/30
Batch 53/268 - Loss: 0.6620 - Accuracy: 0.6875
Batch 106/268 - Loss: 0.2911 - Accuracy: 0.8750
Batch 159/268 - Loss: 0.3485 - Accuracy: 0.9062
Batch 212/268 - Loss: 0.3391 - Accuracy: 0.8438
Batch 265/268 - Loss: 0.3122 - Accuracy: 0.8750
Scalings = [(2.28, 2.13, 1.49, 1.25), (2.18, 1.66, 1.26, 0.86), (2.07, 1.43, 1.19, 0.75), (2.1, 1.33, 1.14, 0.71), (2.13, 1.23, 1.18, 0.7), (2.27, 1.4, 1.25, 0.86), (1.89, 1.38, 1.07, 0.83), (2.1, 1.23, 1.16, 0.85), (1.99, 1.42, 1.05, 0.84), (2.23, 1.68, 1.19, 0.84), (2.54, 1.69, 1.21, 0.85), (2.73, 1.69, 1.32, 0.9), (2.67, 1.89, 1.36, 0.94), (2.64, 1.89, 1.32, 0.85), (2.39, 1.65, 1.27, 0.89), (2.98, 2.05, 1.5, 1.1), (2.44, 1.69, 1.36, 0.97), (2.44, 2.18, 1.36, 1.15), (2.66, 2.26, 1.43, 1.18), (3.16, 2.85, 1.6, 1.55), (3.2, 2.86, 1.57, 1.36), (3.12, 3.44, 1.64, 1.59), (2.75, 3.07, 1.38, 1.56), (2.3, 2.84, 1.27, 1.53)]
Epoch 26 - Train Loss: 0.4090 - Train Acc: 0.8246 - Val Loss: 0.6175 - Val Acc: 0.7891 - Val MCC: 0.47
Epoch 27/30
Batch 53/268 - Loss: 0.4027 - Accuracy: 0.8125
Batch 106/268 - Loss: 0.4522 - Accuracy: 0.7500
Batch 159/268 - Loss: 0.3517 - Accuracy: 0.8750
Batch 212/268 - Loss: 0.5152 - Accuracy: 0.7500
Batch 265/268 - Loss: 0.4813 - Accuracy: 0.7812
Scalings = [(2.28, 2.14, 1.49, 1.25), (2.18, 1.66, 1.27, 0.86), (2.08, 1.43, 1.19, 0.75), (2.11, 1.34, 1.14, 0.71), (2.14, 1.23, 1.18, 0.7), (2.27, 1.4, 1.25, 0.86), (1.9, 1.38, 1.07, 0.82), (2.11, 1.23, 1.16, 0.85), (2.0, 1.43, 1.06, 0.84), (2.24, 1.69, 1.19, 0.84), (2.55, 1.7, 1.22, 0.85), (2.74, 1.7, 1.32, 0.9), (2.68, 1.89, 1.36, 0.94), (2.65, 1.9, 1.32, 0.85), (2.39, 1.65, 1.26, 0.89), (2.99, 2.05, 1.5, 1.09), (2.45, 1.7, 1.36, 0.97), (2.45, 2.19, 1.37, 1.16), (2.66, 2.27, 1.43, 1.18), (3.16, 2.85, 1.6, 1.56), (3.2, 2.87, 1.57, 1.36), (3.12, 3.45, 1.64, 1.59), (2.76, 3.07, 1.38, 1.56), (2.3, 2.85, 1.27, 1.53)]
Epoch 27 - Train Loss: 0.4039 - Train Acc: 0.8215 - Val Loss: 0.6307 - Val Acc: 0.7910 - Val MCC: 0.48
Epoch 28/30
Batch 53/268 - Loss: 0.4421 - Accuracy: 0.7500
Batch 106/268 - Loss: 0.2723 - Accuracy: 0.9375
Batch 159/268 - Loss: 0.2129 - Accuracy: 0.9062
Batch 212/268 - Loss: 0.6006 - Accuracy: 0.7812
Batch 265/268 - Loss: 0.2475 - Accuracy: 0.9062
Scalings = [(2.29, 2.14, 1.5, 1.25), (2.19, 1.66, 1.27, 0.86), (2.09, 1.44, 1.19, 0.75), (2.12, 1.34, 1.15, 0.71), (2.15, 1.23, 1.18, 0.7), (2.29, 1.4, 1.26, 0.86), (1.9, 1.39, 1.07, 0.82), (2.12, 1.23, 1.16, 0.85), (2.0, 1.44, 1.06, 0.84), (2.25, 1.69, 1.2, 0.84), (2.56, 1.71, 1.22, 0.85), (2.75, 1.71, 1.32, 0.9), (2.69, 1.9, 1.37, 0.94), (2.66, 1.9, 1.32, 0.85), (2.4, 1.65, 1.27, 0.88), (3.0, 2.05, 1.5, 1.09), (2.45, 1.7, 1.36, 0.97), (2.45, 2.2, 1.37, 1.16), (2.66, 2.28, 1.43, 1.18), (3.16, 2.86, 1.6, 1.56), (3.21, 2.87, 1.57, 1.36), (3.12, 3.45, 1.64, 1.59), (2.76, 3.07, 1.38, 1.56), (2.3, 2.86, 1.27, 1.53)]
Epoch 28 - Train Loss: 0.3963 - Train Acc: 0.8298 - Val Loss: 0.6094 - Val Acc: 0.7910 - Val MCC: 0.48
Epoch 29/30
Batch 53/268 - Loss: 0.4609 - Accuracy: 0.7812
Batch 106/268 - Loss: 0.2568 - Accuracy: 0.9062
Batch 159/268 - Loss: 0.4176 - Accuracy: 0.7812
Batch 212/268 - Loss: 0.3004 - Accuracy: 0.9062
Batch 265/268 - Loss: 0.3111 - Accuracy: 0.8125
Scalings = [(2.29, 2.14, 1.5, 1.25), (2.19, 1.66, 1.27, 0.86), (2.1, 1.44, 1.19, 0.76), (2.12, 1.34, 1.15, 0.71), (2.15, 1.23, 1.18, 0.7), (2.29, 1.4, 1.26, 0.86), (1.91, 1.39, 1.07, 0.82), (2.12, 1.23, 1.16, 0.85), (2.01, 1.44, 1.06, 0.84), (2.25, 1.7, 1.2, 0.84), (2.56, 1.71, 1.22, 0.85), (2.75, 1.71, 1.32, 0.9), (2.69, 1.9, 1.37, 0.94), (2.66, 1.9, 1.32, 0.85), (2.4, 1.66, 1.27, 0.88), (3.0, 2.05, 1.5, 1.09), (2.45, 1.7, 1.36, 0.97), (2.46, 2.2, 1.37, 1.16), (2.66, 2.28, 1.43, 1.18), (3.17, 2.87, 1.6, 1.56), (3.21, 2.87, 1.57, 1.36), (3.12, 3.45, 1.64, 1.59), (2.76, 3.07, 1.38, 1.56), (2.29, 2.86, 1.27, 1.54)]
Epoch 29 - Train Loss: 0.4016 - Train Acc: 0.8237 - Val Loss: 0.6209 - Val Acc: 0.7929 - Val MCC: 0.48
Epoch 30/30
Batch 53/268 - Loss: 0.5195 - Accuracy: 0.7188
Batch 106/268 - Loss: 0.5551 - Accuracy: 0.7188
Batch 159/268 - Loss: 0.3339 - Accuracy: 0.8438
Batch 212/268 - Loss: 0.4736 - Accuracy: 0.7500
Batch 265/268 - Loss: 0.6143 - Accuracy: 0.7188
Scalings = [(2.29, 2.15, 1.5, 1.25), (2.19, 1.66, 1.27, 0.86), (2.1, 1.44, 1.19, 0.76), (2.12, 1.34, 1.15, 0.71), (2.15, 1.24, 1.18, 0.7), (2.29, 1.4, 1.26, 0.86), (1.91, 1.38, 1.07, 0.82), (2.12, 1.23, 1.17, 0.85), (2.01, 1.44, 1.06, 0.84), (2.25, 1.7, 1.2, 0.84), (2.56, 1.71, 1.22, 0.85), (2.75, 1.71, 1.32, 0.9), (2.69, 1.9, 1.37, 0.94), (2.66, 1.91, 1.32, 0.85), (2.4, 1.66, 1.27, 0.88), (3.0, 2.05, 1.5, 1.1), (2.45, 1.7, 1.36, 0.97), (2.46, 2.2, 1.37, 1.16), (2.66, 2.28, 1.43, 1.18), (3.17, 2.87, 1.6, 1.56), (3.21, 2.88, 1.57, 1.37), (3.12, 3.45, 1.64, 1.59), (2.76, 3.07, 1.38, 1.56), (2.3, 2.86, 1.27, 1.54)]
Epoch 30 - Train Loss: 0.4053 - Train Acc: 0.8239 - Val Loss: 0.6147 - Val Acc: 0.7929 - Val MCC: 0.48
Average Time per Epoch: 89.47s
