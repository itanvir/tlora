Running experiment with data: glue_mrpc, model: FacebookAI/roberta-base, LoRA type: tlora
RobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-11): 12 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=768, out_features=2, bias=True)
  )
)
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        38,603,520
│    │    └─Embedding: 3-2                                        394,752
│    │    └─Embedding: 3-3                                        768
│    │    └─LayerNorm: 3-4                                        1,536
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,054,464
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                590,592
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                1,538
==========================================================================================
Total params: 124,647,170
Trainable params: 124,647,170
Non-trainable params: 0
==========================================================================================
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        38,603,520
│    │    └─Embedding: 3-2                                        394,752
│    │    └─Embedding: 3-3                                        768
│    │    └─LayerNorm: 3-4                                        1,536
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,054,464
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                590,592
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                1,538
==========================================================================================
Total params: 124,647,170
Trainable params: 124,647,170
Non-trainable params: 0
==========================================================================================
RobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-11): 12 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSdpaSelfAttention(
              (query): TLoRALayer(
                (linear): Linear(in_features=768, out_features=768, bias=True)
              )
              (key): TLoRALayer(
                (linear): Linear(in_features=768, out_features=768, bias=True)
              )
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=768, out_features=2, bias=True)
  )
)
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        (38,603,520)
│    │    └─Embedding: 3-2                                        (394,752)
│    │    └─Embedding: 3-3                                        (768)
│    │    └─LayerNorm: 3-4                                        (1,536)
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,313,688
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                (590,592)
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                (1,538)
==========================================================================================
Total params: 124,906,394
Trainable params: 259,224
Non-trainable params: 124,647,170
==========================================================================================
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        (38,603,520)
│    │    └─Embedding: 3-2                                        (394,752)
│    │    └─Embedding: 3-3                                        (768)
│    │    └─LayerNorm: 3-4                                        (1,536)
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,313,688
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                (590,592)
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                (1,538)
==========================================================================================
Total params: 124,906,394
Trainable params: 259,224
Non-trainable params: 124,647,170
==========================================================================================
Epoch 1/20
Batch 23/115 - Loss: 0.7326 - Accuracy: 0.3750
Batch 46/115 - Loss: 0.7112 - Accuracy: 0.4375
Batch 69/115 - Loss: 0.7030 - Accuracy: 0.3125
Batch 92/115 - Loss: 0.7103 - Accuracy: 0.2500
Batch 115/115 - Loss: 0.6977 - Accuracy: 0.3000
Epoch 1 - Train Loss: 0.7115 - Train Acc: 0.3760 - Val Loss: 0.7035 - Val Acc: 0.3141 - Time: 30.40s
Epoch 2/20
Batch 23/115 - Loss: 0.6874 - Accuracy: 0.5625
Batch 46/115 - Loss: 0.6977 - Accuracy: 0.5000
Batch 69/115 - Loss: 0.6906 - Accuracy: 0.5000
Batch 92/115 - Loss: 0.6822 - Accuracy: 0.5938
Batch 115/115 - Loss: 0.6814 - Accuracy: 0.5500
Epoch 2 - Train Loss: 0.6911 - Train Acc: 0.5178 - Val Loss: 0.6627 - Val Acc: 0.6939 - Time: 26.26s
Epoch 3/20
Batch 23/115 - Loss: 0.6538 - Accuracy: 0.6875
Batch 46/115 - Loss: 0.7057 - Accuracy: 0.5625
Batch 69/115 - Loss: 0.6408 - Accuracy: 0.7188
Batch 92/115 - Loss: 0.6590 - Accuracy: 0.6562
Batch 115/115 - Loss: 0.5941 - Accuracy: 0.7500
Epoch 3 - Train Loss: 0.6509 - Train Acc: 0.6780 - Val Loss: 0.6173 - Val Acc: 0.6859 - Time: 26.67s
Epoch 4/20
Batch 23/115 - Loss: 0.5999 - Accuracy: 0.8125
Batch 46/115 - Loss: 0.6084 - Accuracy: 0.7188
Batch 69/115 - Loss: 0.5989 - Accuracy: 0.7188
Batch 92/115 - Loss: 0.6171 - Accuracy: 0.7188
Batch 115/115 - Loss: 0.5978 - Accuracy: 0.7500
Epoch 4 - Train Loss: 0.6268 - Train Acc: 0.6853 - Val Loss: 0.5913 - Val Acc: 0.6907 - Time: 27.64s
Epoch 5/20
Batch 23/115 - Loss: 0.6710 - Accuracy: 0.5312
Batch 46/115 - Loss: 0.6542 - Accuracy: 0.5625
Batch 69/115 - Loss: 0.6045 - Accuracy: 0.6562
Batch 92/115 - Loss: 0.5511 - Accuracy: 0.7812
Batch 115/115 - Loss: 0.7052 - Accuracy: 0.4500
Epoch 5 - Train Loss: 0.6068 - Train Acc: 0.6857 - Val Loss: 0.5654 - Val Acc: 0.6939 - Time: 27.09s
Epoch 6/20
Batch 23/115 - Loss: 0.5493 - Accuracy: 0.7500
Batch 46/115 - Loss: 0.5550 - Accuracy: 0.7500
Batch 69/115 - Loss: 0.5214 - Accuracy: 0.8125
Batch 92/115 - Loss: 0.6090 - Accuracy: 0.6875
Batch 115/115 - Loss: 0.6264 - Accuracy: 0.6500
Epoch 6 - Train Loss: 0.5958 - Train Acc: 0.6967 - Val Loss: 0.5491 - Val Acc: 0.6939 - Time: 27.17s
Epoch 7/20
Batch 23/115 - Loss: 0.6127 - Accuracy: 0.6875
Batch 46/115 - Loss: 0.6678 - Accuracy: 0.5000
Batch 69/115 - Loss: 0.5526 - Accuracy: 0.7500
Batch 92/115 - Loss: 0.5603 - Accuracy: 0.7188
Batch 115/115 - Loss: 0.4748 - Accuracy: 0.8500
Epoch 7 - Train Loss: 0.5803 - Train Acc: 0.7049 - Val Loss: 0.5342 - Val Acc: 0.7035 - Time: 26.96s
Epoch 8/20
Batch 23/115 - Loss: 0.5900 - Accuracy: 0.5938
Batch 46/115 - Loss: 0.5434 - Accuracy: 0.7188
Batch 69/115 - Loss: 0.5704 - Accuracy: 0.7188
Batch 92/115 - Loss: 0.6206 - Accuracy: 0.6250
Batch 115/115 - Loss: 0.5957 - Accuracy: 0.6000
Epoch 8 - Train Loss: 0.5733 - Train Acc: 0.7128 - Val Loss: 0.5228 - Val Acc: 0.7188 - Time: 27.10s
Epoch 9/20
Batch 23/115 - Loss: 0.6413 - Accuracy: 0.5625
Batch 46/115 - Loss: 0.4581 - Accuracy: 0.8438
Batch 69/115 - Loss: 0.5448 - Accuracy: 0.7500
Batch 92/115 - Loss: 0.6135 - Accuracy: 0.6562
Batch 115/115 - Loss: 0.4900 - Accuracy: 0.8000
Epoch 9 - Train Loss: 0.5652 - Train Acc: 0.7170 - Val Loss: 0.5287 - Val Acc: 0.7163 - Time: 27.23s
Epoch 10/20
Batch 23/115 - Loss: 0.5103 - Accuracy: 0.8125
Batch 46/115 - Loss: 0.5565 - Accuracy: 0.7188
Batch 69/115 - Loss: 0.5389 - Accuracy: 0.7500
Batch 92/115 - Loss: 0.4949 - Accuracy: 0.7812
Batch 115/115 - Loss: 0.5884 - Accuracy: 0.6500
Epoch 10 - Train Loss: 0.5611 - Train Acc: 0.7301 - Val Loss: 0.5194 - Val Acc: 0.7260 - Time: 27.17s
Epoch 11/20
Batch 23/115 - Loss: 0.4717 - Accuracy: 0.8438
Batch 46/115 - Loss: 0.4690 - Accuracy: 0.8438
Batch 69/115 - Loss: 0.4929 - Accuracy: 0.8438
Batch 92/115 - Loss: 0.5333 - Accuracy: 0.7500
Batch 115/115 - Loss: 0.5255 - Accuracy: 0.7500
Epoch 11 - Train Loss: 0.5522 - Train Acc: 0.7264 - Val Loss: 0.5169 - Val Acc: 0.7284 - Time: 26.88s
Epoch 12/20
Batch 23/115 - Loss: 0.5484 - Accuracy: 0.7500
Batch 46/115 - Loss: 0.5433 - Accuracy: 0.6875
Batch 69/115 - Loss: 0.6122 - Accuracy: 0.6875
Batch 92/115 - Loss: 0.5311 - Accuracy: 0.7188
Batch 115/115 - Loss: 0.5721 - Accuracy: 0.7000
Epoch 12 - Train Loss: 0.5478 - Train Acc: 0.7314 - Val Loss: 0.5084 - Val Acc: 0.7308 - Time: 26.83s
Epoch 13/20
Batch 23/115 - Loss: 0.5649 - Accuracy: 0.7188
Batch 46/115 - Loss: 0.5610 - Accuracy: 0.7188
Batch 69/115 - Loss: 0.5515 - Accuracy: 0.7500
Batch 92/115 - Loss: 0.4951 - Accuracy: 0.8125
Batch 115/115 - Loss: 0.6063 - Accuracy: 0.7000
Epoch 13 - Train Loss: 0.5403 - Train Acc: 0.7343 - Val Loss: 0.4907 - Val Acc: 0.7524 - Time: 27.22s
Epoch 14/20
Batch 23/115 - Loss: 0.5398 - Accuracy: 0.7188
Batch 46/115 - Loss: 0.4906 - Accuracy: 0.8438
Batch 69/115 - Loss: 0.4534 - Accuracy: 0.8750
Batch 92/115 - Loss: 0.5351 - Accuracy: 0.7500
Batch 115/115 - Loss: 0.5116 - Accuracy: 0.6500
Epoch 14 - Train Loss: 0.5360 - Train Acc: 0.7410 - Val Loss: 0.5055 - Val Acc: 0.7380 - Time: 26.29s
Epoch 15/20
Batch 23/115 - Loss: 0.4723 - Accuracy: 0.8438
Batch 46/115 - Loss: 0.5853 - Accuracy: 0.6562
Batch 69/115 - Loss: 0.5650 - Accuracy: 0.8125
Batch 92/115 - Loss: 0.5325 - Accuracy: 0.7812
Batch 115/115 - Loss: 0.6259 - Accuracy: 0.6500
Epoch 15 - Train Loss: 0.5317 - Train Acc: 0.7442 - Val Loss: 0.4823 - Val Acc: 0.7620 - Time: 26.69s
Epoch 16/20
Batch 23/115 - Loss: 0.5527 - Accuracy: 0.7188
Batch 46/115 - Loss: 0.5838 - Accuracy: 0.5938
Batch 69/115 - Loss: 0.5955 - Accuracy: 0.6250
Batch 92/115 - Loss: 0.5891 - Accuracy: 0.6562
Batch 115/115 - Loss: 0.4725 - Accuracy: 0.7500
Epoch 16 - Train Loss: 0.5245 - Train Acc: 0.7549 - Val Loss: 0.4806 - Val Acc: 0.7596 - Time: 26.61s
Epoch 17/20
Batch 23/115 - Loss: 0.6149 - Accuracy: 0.6875
Batch 46/115 - Loss: 0.4345 - Accuracy: 0.8438
Batch 69/115 - Loss: 0.4645 - Accuracy: 0.7812
Batch 92/115 - Loss: 0.6724 - Accuracy: 0.5312
Batch 115/115 - Loss: 0.6699 - Accuracy: 0.5500
Epoch 17 - Train Loss: 0.5232 - Train Acc: 0.7526 - Val Loss: 0.4791 - Val Acc: 0.7596 - Time: 26.43s
Epoch 18/20
Batch 23/115 - Loss: 0.5142 - Accuracy: 0.7500
Batch 46/115 - Loss: 0.5538 - Accuracy: 0.7812
Batch 69/115 - Loss: 0.4678 - Accuracy: 0.8438
Batch 92/115 - Loss: 0.4966 - Accuracy: 0.7500
Batch 115/115 - Loss: 0.6089 - Accuracy: 0.6000
Epoch 18 - Train Loss: 0.5151 - Train Acc: 0.7506 - Val Loss: 0.4738 - Val Acc: 0.7716 - Time: 26.41s
Epoch 19/20
Batch 23/115 - Loss: 0.4532 - Accuracy: 0.8438
Batch 46/115 - Loss: 0.4999 - Accuracy: 0.7812
Batch 69/115 - Loss: 0.4991 - Accuracy: 0.7812
Batch 92/115 - Loss: 0.4842 - Accuracy: 0.8125
Batch 115/115 - Loss: 0.6381 - Accuracy: 0.5500
Epoch 19 - Train Loss: 0.5065 - Train Acc: 0.7599 - Val Loss: 0.4786 - Val Acc: 0.7668 - Time: 27.06s
Epoch 20/20
Batch 23/115 - Loss: 0.3999 - Accuracy: 0.9062
Batch 46/115 - Loss: 0.4510 - Accuracy: 0.7812
Batch 69/115 - Loss: 0.4857 - Accuracy: 0.8125
Batch 92/115 - Loss: 0.4366 - Accuracy: 0.8438
Batch 115/115 - Loss: 0.5380 - Accuracy: 0.8000
Epoch 20 - Train Loss: 0.5079 - Train Acc: 0.7589 - Val Loss: 0.4742 - Val Acc: 0.7716 - Time: 26.60s
Average Time per Epoch: 27.04s
Validation Loss: 0.4742, Validation Accuracy: 0.7716
