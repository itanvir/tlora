Running experiment with data: glue_sst2, model: FacebookAI/roberta-base, LoRA type: lora
RobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-11): 12 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=768, out_features=2, bias=True)
  )
)
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        38,603,520
│    │    └─Embedding: 3-2                                        394,752
│    │    └─Embedding: 3-3                                        768
│    │    └─LayerNorm: 3-4                                        1,536
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,054,464
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                590,592
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                1,538
==========================================================================================
Total params: 124,647,170
Trainable params: 124,647,170
Non-trainable params: 0
==========================================================================================
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        38,603,520
│    │    └─Embedding: 3-2                                        394,752
│    │    └─Embedding: 3-3                                        768
│    │    └─LayerNorm: 3-4                                        1,536
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,054,464
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                590,592
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                1,538
==========================================================================================
Total params: 124,647,170
Trainable params: 124,647,170
Non-trainable params: 0
==========================================================================================
RobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-11): 12 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSdpaSelfAttention(
              (query): LoRALayer(
                (linear): Linear(in_features=768, out_features=768, bias=True)
              )
              (key): LoRALayer(
                (linear): Linear(in_features=768, out_features=768, bias=True)
              )
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=768, out_features=2, bias=True)
  )
)
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        (38,603,520)
│    │    └─Embedding: 3-2                                        (394,752)
│    │    └─Embedding: 3-3                                        (768)
│    │    └─LayerNorm: 3-4                                        (1,536)
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,349,376
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                (590,592)
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                (1,538)
==========================================================================================
Total params: 124,942,082
Trainable params: 294,912
Non-trainable params: 124,647,170
==========================================================================================
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        (38,603,520)
│    │    └─Embedding: 3-2                                        (394,752)
│    │    └─Embedding: 3-3                                        (768)
│    │    └─LayerNorm: 3-4                                        (1,536)
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,349,376
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                (590,592)
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                (1,538)
==========================================================================================
Total params: 124,942,082
Trainable params: 294,912
Non-trainable params: 124,647,170
==========================================================================================
Epoch 1/20
Batch 421/2105 - Loss: 0.6911 - Accuracy: 0.5312
Batch 842/2105 - Loss: 0.6982 - Accuracy: 0.5000
Batch 1263/2105 - Loss: 0.6910 - Accuracy: 0.5312
Batch 1684/2105 - Loss: 0.6867 - Accuracy: 0.5312
Batch 2105/2105 - Loss: 0.6909 - Accuracy: 0.5238
Epoch 1 - Train Loss: 0.6997 - Train Acc: 0.4788 - Val Loss: 0.6812 - Val Acc: 0.5681 - Time: 325.00s
Epoch 2/20
Batch 421/2105 - Loss: 0.6928 - Accuracy: 0.6875
Batch 842/2105 - Loss: 0.6775 - Accuracy: 0.5625
Batch 1263/2105 - Loss: 0.6838 - Accuracy: 0.5312
Batch 1684/2105 - Loss: 0.6682 - Accuracy: 0.6250
Batch 2105/2105 - Loss: 0.6191 - Accuracy: 0.7619
Epoch 2 - Train Loss: 0.6646 - Train Acc: 0.6605 - Val Loss: 0.6046 - Val Acc: 0.7902 - Time: 329.21s
Epoch 3/20
Batch 421/2105 - Loss: 0.6146 - Accuracy: 0.8438
Batch 842/2105 - Loss: 0.6108 - Accuracy: 0.7188
Batch 1263/2105 - Loss: 0.5440 - Accuracy: 0.9062
Batch 1684/2105 - Loss: 0.5485 - Accuracy: 0.8750
Batch 2105/2105 - Loss: 0.5492 - Accuracy: 0.9048
Epoch 3 - Train Loss: 0.5992 - Train Acc: 0.8019 - Val Loss: 0.4670 - Val Acc: 0.8750 - Time: 324.72s
Epoch 4/20
Batch 421/2105 - Loss: 0.5506 - Accuracy: 0.8125
Batch 842/2105 - Loss: 0.5116 - Accuracy: 0.8750
Batch 1263/2105 - Loss: 0.5291 - Accuracy: 0.8750
Batch 1684/2105 - Loss: 0.5242 - Accuracy: 0.9375
Batch 2105/2105 - Loss: 0.5386 - Accuracy: 0.7619
Epoch 4 - Train Loss: 0.5446 - Train Acc: 0.8297 - Val Loss: 0.4004 - Val Acc: 0.8862 - Time: 325.58s
Epoch 5/20
Batch 421/2105 - Loss: 0.5846 - Accuracy: 0.7188
Batch 842/2105 - Loss: 0.5099 - Accuracy: 0.8750
Batch 1263/2105 - Loss: 0.5301 - Accuracy: 0.8438
Batch 1684/2105 - Loss: 0.4590 - Accuracy: 0.8750
Batch 2105/2105 - Loss: 0.5700 - Accuracy: 0.8095
Epoch 5 - Train Loss: 0.5095 - Train Acc: 0.8416 - Val Loss: 0.3676 - Val Acc: 0.8951 - Time: 332.18s
Epoch 6/20
Batch 421/2105 - Loss: 0.4553 - Accuracy: 0.9062
Batch 842/2105 - Loss: 0.4101 - Accuracy: 0.9375
Batch 1263/2105 - Loss: 0.5019 - Accuracy: 0.8438
Batch 1684/2105 - Loss: 0.4703 - Accuracy: 0.9062
Batch 2105/2105 - Loss: 0.5174 - Accuracy: 0.7619
Epoch 6 - Train Loss: 0.4852 - Train Acc: 0.8490 - Val Loss: 0.3462 - Val Acc: 0.8962 - Time: 327.77s
Epoch 7/20
Batch 421/2105 - Loss: 0.4318 - Accuracy: 0.8750
Batch 842/2105 - Loss: 0.5107 - Accuracy: 0.8125
Batch 1263/2105 - Loss: 0.5582 - Accuracy: 0.7188
Batch 1684/2105 - Loss: 0.4385 - Accuracy: 0.8750
Batch 2105/2105 - Loss: 0.5059 - Accuracy: 0.7619
Epoch 7 - Train Loss: 0.4671 - Train Acc: 0.8565 - Val Loss: 0.3305 - Val Acc: 0.8996 - Time: 329.73s
Epoch 8/20
Batch 421/2105 - Loss: 0.5108 - Accuracy: 0.8125
Batch 842/2105 - Loss: 0.4404 - Accuracy: 0.9062
Batch 1263/2105 - Loss: 0.4105 - Accuracy: 0.9062
Batch 1684/2105 - Loss: 0.4689 - Accuracy: 0.8750
Batch 2105/2105 - Loss: 0.3959 - Accuracy: 1.0000
Epoch 8 - Train Loss: 0.4545 - Train Acc: 0.8596 - Val Loss: 0.3196 - Val Acc: 0.8962 - Time: 331.05s
Epoch 9/20
Batch 421/2105 - Loss: 0.4691 - Accuracy: 0.8125
Batch 842/2105 - Loss: 0.4471 - Accuracy: 0.8438
Batch 1263/2105 - Loss: 0.5034 - Accuracy: 0.7812
Batch 1684/2105 - Loss: 0.4272 - Accuracy: 0.8750
Batch 2105/2105 - Loss: 0.4289 - Accuracy: 0.8095
Epoch 9 - Train Loss: 0.4436 - Train Acc: 0.8633 - Val Loss: 0.3116 - Val Acc: 0.9018 - Time: 334.89s
Epoch 10/20
Batch 421/2105 - Loss: 0.4266 - Accuracy: 0.9062
Batch 842/2105 - Loss: 0.4371 - Accuracy: 0.9062
Batch 1263/2105 - Loss: 0.3506 - Accuracy: 0.9062
Batch 1684/2105 - Loss: 0.4089 - Accuracy: 0.9062
Batch 2105/2105 - Loss: 0.4386 - Accuracy: 0.8095
Epoch 10 - Train Loss: 0.4341 - Train Acc: 0.8669 - Val Loss: 0.3053 - Val Acc: 0.9040 - Time: 327.74s
Epoch 11/20
Batch 421/2105 - Loss: 0.4396 - Accuracy: 0.8438
Batch 842/2105 - Loss: 0.3989 - Accuracy: 0.9062
Batch 1263/2105 - Loss: 0.3424 - Accuracy: 0.9375
Batch 1684/2105 - Loss: 0.4497 - Accuracy: 0.8750
Batch 2105/2105 - Loss: 0.3714 - Accuracy: 0.9524
Epoch 11 - Train Loss: 0.4281 - Train Acc: 0.8689 - Val Loss: 0.3028 - Val Acc: 0.9040 - Time: 325.89s
Epoch 12/20
Batch 421/2105 - Loss: 0.3759 - Accuracy: 0.9062
Batch 842/2105 - Loss: 0.4469 - Accuracy: 0.8750
Batch 1263/2105 - Loss: 0.4020 - Accuracy: 0.9375
Batch 1684/2105 - Loss: 0.4697 - Accuracy: 0.7812
Batch 2105/2105 - Loss: 0.3812 - Accuracy: 0.9048
Epoch 12 - Train Loss: 0.4209 - Train Acc: 0.8729 - Val Loss: 0.2976 - Val Acc: 0.9085 - Time: 328.43s
Epoch 13/20
Batch 421/2105 - Loss: 0.4189 - Accuracy: 0.8750
Batch 842/2105 - Loss: 0.4196 - Accuracy: 0.8750
Batch 1263/2105 - Loss: 0.4326 - Accuracy: 0.8750
Batch 1684/2105 - Loss: 0.5627 - Accuracy: 0.7500
Batch 2105/2105 - Loss: 0.4961 - Accuracy: 0.8095
Epoch 13 - Train Loss: 0.4148 - Train Acc: 0.8743 - Val Loss: 0.2966 - Val Acc: 0.9051 - Time: 322.64s
Epoch 14/20
Batch 421/2105 - Loss: 0.4495 - Accuracy: 0.8125
Batch 842/2105 - Loss: 0.4099 - Accuracy: 0.8750
Batch 1263/2105 - Loss: 0.3887 - Accuracy: 0.9062
Batch 1684/2105 - Loss: 0.3141 - Accuracy: 0.9688
Batch 2105/2105 - Loss: 0.4724 - Accuracy: 0.8571
Epoch 14 - Train Loss: 0.4095 - Train Acc: 0.8753 - Val Loss: 0.2939 - Val Acc: 0.9051 - Time: 321.49s
Epoch 15/20
Batch 421/2105 - Loss: 0.3197 - Accuracy: 0.9688
Batch 842/2105 - Loss: 0.3632 - Accuracy: 0.9375
Batch 1263/2105 - Loss: 0.4845 - Accuracy: 0.8438
Batch 1684/2105 - Loss: 0.4138 - Accuracy: 0.8750
Batch 2105/2105 - Loss: 0.3527 - Accuracy: 0.9524
Epoch 15 - Train Loss: 0.4050 - Train Acc: 0.8776 - Val Loss: 0.2918 - Val Acc: 0.9074 - Time: 322.04s
Epoch 16/20
Batch 421/2105 - Loss: 0.3569 - Accuracy: 0.9375
Batch 842/2105 - Loss: 0.4032 - Accuracy: 0.8750
Batch 1263/2105 - Loss: 0.3642 - Accuracy: 0.9062
Batch 1684/2105 - Loss: 0.5207 - Accuracy: 0.7188
Batch 2105/2105 - Loss: 0.3472 - Accuracy: 0.9524
Epoch 16 - Train Loss: 0.4003 - Train Acc: 0.8791 - Val Loss: 0.2899 - Val Acc: 0.9062 - Time: 322.20s
Epoch 17/20
Batch 421/2105 - Loss: 0.3197 - Accuracy: 0.9375
Batch 842/2105 - Loss: 0.3266 - Accuracy: 0.9688
Batch 1263/2105 - Loss: 0.3697 - Accuracy: 0.9375
Batch 1684/2105 - Loss: 0.3918 - Accuracy: 0.9062
Batch 2105/2105 - Loss: 0.3887 - Accuracy: 0.9524
Epoch 17 - Train Loss: 0.3973 - Train Acc: 0.8792 - Val Loss: 0.2914 - Val Acc: 0.9018 - Time: 321.47s
Epoch 18/20
Batch 421/2105 - Loss: 0.3826 - Accuracy: 0.8750
Batch 842/2105 - Loss: 0.3705 - Accuracy: 0.9062
Batch 1263/2105 - Loss: 0.4275 - Accuracy: 0.8125
Batch 1684/2105 - Loss: 0.3359 - Accuracy: 0.9062
Batch 2105/2105 - Loss: 0.3748 - Accuracy: 0.9048
Epoch 18 - Train Loss: 0.3916 - Train Acc: 0.8811 - Val Loss: 0.2864 - Val Acc: 0.9051 - Time: 321.75s
Epoch 19/20
Batch 421/2105 - Loss: 0.2875 - Accuracy: 1.0000
Batch 842/2105 - Loss: 0.5282 - Accuracy: 0.7812
Batch 1263/2105 - Loss: 0.4374 - Accuracy: 0.8125
Batch 1684/2105 - Loss: 0.4124 - Accuracy: 0.8438
Batch 2105/2105 - Loss: 0.4163 - Accuracy: 0.8571
Epoch 19 - Train Loss: 0.3890 - Train Acc: 0.8825 - Val Loss: 0.2862 - Val Acc: 0.9040 - Time: 321.88s
Epoch 20/20
Batch 421/2105 - Loss: 0.5723 - Accuracy: 0.7500
Batch 842/2105 - Loss: 0.3740 - Accuracy: 0.9062
Batch 1263/2105 - Loss: 0.4159 - Accuracy: 0.8438
Batch 1684/2105 - Loss: 0.4213 - Accuracy: 0.8125
Batch 2105/2105 - Loss: 0.3516 - Accuracy: 0.9524
Epoch 20 - Train Loss: 0.3865 - Train Acc: 0.8832 - Val Loss: 0.2863 - Val Acc: 0.9029 - Time: 324.45s
Average Time per Epoch: 326.00s
Validation Loss: 0.2863, Validation Accuracy: 0.9029
