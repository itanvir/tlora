Running experiment with data: glue_cola, model: FacebookAI/roberta-base, LoRA type: tlora
RobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-11): 12 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=768, out_features=2, bias=True)
  )
)
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        38,603,520
│    │    └─Embedding: 3-2                                        394,752
│    │    └─Embedding: 3-3                                        768
│    │    └─LayerNorm: 3-4                                        1,536
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,054,464
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                590,592
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                1,538
==========================================================================================
Total params: 124,647,170
Trainable params: 124,647,170
Non-trainable params: 0
==========================================================================================
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        38,603,520
│    │    └─Embedding: 3-2                                        394,752
│    │    └─Embedding: 3-3                                        768
│    │    └─LayerNorm: 3-4                                        1,536
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,054,464
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                590,592
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                1,538
==========================================================================================
Total params: 124,647,170
Trainable params: 124,647,170
Non-trainable params: 0
==========================================================================================
RobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-11): 12 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSdpaSelfAttention(
              (query): TLoRALayer(
                (linear): Linear(in_features=768, out_features=768, bias=True)
              )
              (key): TLoRALayer(
                (linear): Linear(in_features=768, out_features=768, bias=True)
              )
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=768, out_features=2, bias=True)
  )
)
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        (38,603,520)
│    │    └─Embedding: 3-2                                        (394,752)
│    │    └─Embedding: 3-3                                        (768)
│    │    └─LayerNorm: 3-4                                        (1,536)
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,313,688
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                (590,592)
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                (1,538)
==========================================================================================
Total params: 124,906,394
Trainable params: 259,224
Non-trainable params: 124,647,170
==========================================================================================
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
RobertaForSequenceClassification                                  --
├─RobertaModel: 1-1                                               --
│    └─RobertaEmbeddings: 2-1                                     --
│    │    └─Embedding: 3-1                                        (38,603,520)
│    │    └─Embedding: 3-2                                        (394,752)
│    │    └─Embedding: 3-3                                        (768)
│    │    └─LayerNorm: 3-4                                        (1,536)
│    │    └─Dropout: 3-5                                          --
│    └─RobertaEncoder: 2-2                                        --
│    │    └─ModuleList: 3-6                                       85,313,688
├─RobertaClassificationHead: 1-2                                  --
│    └─Linear: 2-3                                                (590,592)
│    └─Dropout: 2-4                                               --
│    └─Linear: 2-5                                                (1,538)
==========================================================================================
Total params: 124,906,394
Trainable params: 259,224
Non-trainable params: 124,647,170
==========================================================================================
Epoch 1/40
Batch 53/268 - Loss: 0.6774 - Accuracy: 0.7188
Batch 106/268 - Loss: 0.6914 - Accuracy: 0.6562
Batch 159/268 - Loss: 0.6944 - Accuracy: 0.5625
Batch 212/268 - Loss: 0.6860 - Accuracy: 0.5312
Batch 265/268 - Loss: 0.6634 - Accuracy: 0.6875
Epoch 1 - Train Loss: 0.6777 - Train Acc: 0.6510 - Val Loss: 0.6746 - Val Acc: 0.6931 - Time: 32.06s
Epoch 2/40
Batch 53/268 - Loss: 0.6758 - Accuracy: 0.6250
Batch 106/268 - Loss: 0.6816 - Accuracy: 0.6875
Batch 159/268 - Loss: 0.6890 - Accuracy: 0.5625
Batch 212/268 - Loss: 0.6359 - Accuracy: 0.7500
Batch 265/268 - Loss: 0.6687 - Accuracy: 0.7188
Epoch 2 - Train Loss: 0.6611 - Train Acc: 0.6972 - Val Loss: 0.6487 - Val Acc: 0.6931 - Time: 32.02s
Epoch 3/40
Batch 53/268 - Loss: 0.6534 - Accuracy: 0.7188
Batch 106/268 - Loss: 0.6378 - Accuracy: 0.7188
Batch 159/268 - Loss: 0.6544 - Accuracy: 0.6562
Batch 212/268 - Loss: 0.5928 - Accuracy: 0.7812
Batch 265/268 - Loss: 0.5108 - Accuracy: 0.9062
Epoch 3 - Train Loss: 0.6356 - Train Acc: 0.7042 - Val Loss: 0.6154 - Val Acc: 0.6931 - Time: 32.18s
Epoch 4/40
Batch 53/268 - Loss: 0.6231 - Accuracy: 0.6250
Batch 106/268 - Loss: 0.6947 - Accuracy: 0.5938
Batch 159/268 - Loss: 0.6428 - Accuracy: 0.6562
Batch 212/268 - Loss: 0.6042 - Accuracy: 0.7500
Batch 265/268 - Loss: 0.6051 - Accuracy: 0.7188
Epoch 4 - Train Loss: 0.6130 - Train Acc: 0.7048 - Val Loss: 0.6062 - Val Acc: 0.6931 - Time: 32.04s
Epoch 5/40
Batch 53/268 - Loss: 0.6263 - Accuracy: 0.6562
Batch 106/268 - Loss: 0.6301 - Accuracy: 0.7188
Batch 159/268 - Loss: 0.4895 - Accuracy: 0.8438
Batch 212/268 - Loss: 0.6034 - Accuracy: 0.7188
Batch 265/268 - Loss: 0.6794 - Accuracy: 0.5938
Epoch 5 - Train Loss: 0.6043 - Train Acc: 0.7050 - Val Loss: 0.6018 - Val Acc: 0.6931 - Time: 32.29s
Epoch 6/40
Batch 53/268 - Loss: 0.5564 - Accuracy: 0.7812
Batch 106/268 - Loss: 0.6294 - Accuracy: 0.6875
Batch 159/268 - Loss: 0.6521 - Accuracy: 0.6562
Batch 212/268 - Loss: 0.5746 - Accuracy: 0.7188
Batch 265/268 - Loss: 0.6720 - Accuracy: 0.5938
Epoch 6 - Train Loss: 0.5988 - Train Acc: 0.7044 - Val Loss: 0.5932 - Val Acc: 0.6931 - Time: 32.10s
Epoch 7/40
Batch 53/268 - Loss: 0.5866 - Accuracy: 0.7188
Batch 106/268 - Loss: 0.5255 - Accuracy: 0.7812
Batch 159/268 - Loss: 0.5643 - Accuracy: 0.7500
Batch 212/268 - Loss: 0.5707 - Accuracy: 0.7188
Batch 265/268 - Loss: 0.5914 - Accuracy: 0.7188
Epoch 7 - Train Loss: 0.5883 - Train Acc: 0.7048 - Val Loss: 0.5899 - Val Acc: 0.6931 - Time: 31.77s
Epoch 8/40
Batch 53/268 - Loss: 0.5842 - Accuracy: 0.7812
Batch 106/268 - Loss: 0.5556 - Accuracy: 0.7500
Batch 159/268 - Loss: 0.6185 - Accuracy: 0.6250
Batch 212/268 - Loss: 0.5959 - Accuracy: 0.6875
Batch 265/268 - Loss: 0.5623 - Accuracy: 0.6875
Epoch 8 - Train Loss: 0.5859 - Train Acc: 0.7036 - Val Loss: 0.5894 - Val Acc: 0.6931 - Time: 31.89s
Epoch 9/40
Batch 53/268 - Loss: 0.6030 - Accuracy: 0.7188
Batch 106/268 - Loss: 0.5271 - Accuracy: 0.7188
Batch 159/268 - Loss: 0.5685 - Accuracy: 0.6875
Batch 212/268 - Loss: 0.5749 - Accuracy: 0.7188
Batch 265/268 - Loss: 0.6197 - Accuracy: 0.6875
Epoch 9 - Train Loss: 0.5791 - Train Acc: 0.7038 - Val Loss: 0.5725 - Val Acc: 0.6931 - Time: 31.83s
Epoch 10/40
Batch 53/268 - Loss: 0.6074 - Accuracy: 0.6562
Batch 106/268 - Loss: 0.5367 - Accuracy: 0.7500
Batch 159/268 - Loss: 0.5190 - Accuracy: 0.8125
Batch 212/268 - Loss: 0.5874 - Accuracy: 0.7500
Batch 265/268 - Loss: 0.5257 - Accuracy: 0.8125
Epoch 10 - Train Loss: 0.5713 - Train Acc: 0.7052 - Val Loss: 0.5791 - Val Acc: 0.6931 - Time: 31.75s
Epoch 11/40
Batch 53/268 - Loss: 0.4605 - Accuracy: 0.8438
Batch 106/268 - Loss: 0.6951 - Accuracy: 0.5312
Batch 159/268 - Loss: 0.6122 - Accuracy: 0.6562
Batch 212/268 - Loss: 0.6258 - Accuracy: 0.6250
Batch 265/268 - Loss: 0.6291 - Accuracy: 0.5938
Epoch 11 - Train Loss: 0.5642 - Train Acc: 0.7047 - Val Loss: 0.5666 - Val Acc: 0.6931 - Time: 31.83s
Epoch 12/40
Batch 53/268 - Loss: 0.5918 - Accuracy: 0.5938
Batch 106/268 - Loss: 0.6058 - Accuracy: 0.6562
Batch 159/268 - Loss: 0.5240 - Accuracy: 0.7812
Batch 212/268 - Loss: 0.5327 - Accuracy: 0.7812
Batch 265/268 - Loss: 0.5823 - Accuracy: 0.7188
Epoch 12 - Train Loss: 0.5587 - Train Acc: 0.7050 - Val Loss: 0.5711 - Val Acc: 0.6931 - Time: 31.86s
Epoch 13/40
Batch 53/268 - Loss: 0.6326 - Accuracy: 0.6562
Batch 106/268 - Loss: 0.5826 - Accuracy: 0.6875
Batch 159/268 - Loss: 0.4803 - Accuracy: 0.7812
Batch 212/268 - Loss: 0.4887 - Accuracy: 0.7812
Batch 265/268 - Loss: 0.6216 - Accuracy: 0.5625
Epoch 13 - Train Loss: 0.5522 - Train Acc: 0.7040 - Val Loss: 0.5533 - Val Acc: 0.6931 - Time: 31.69s
Epoch 14/40
Batch 53/268 - Loss: 0.4633 - Accuracy: 0.8125
Batch 106/268 - Loss: 0.5779 - Accuracy: 0.7188
Batch 159/268 - Loss: 0.5829 - Accuracy: 0.5625
Batch 212/268 - Loss: 0.5536 - Accuracy: 0.6875
Batch 265/268 - Loss: 0.5591 - Accuracy: 0.6875
Epoch 14 - Train Loss: 0.5476 - Train Acc: 0.7060 - Val Loss: 0.5588 - Val Acc: 0.6931 - Time: 31.78s
Epoch 15/40
Batch 53/268 - Loss: 0.4747 - Accuracy: 0.8125
Batch 106/268 - Loss: 0.5238 - Accuracy: 0.7500
Batch 159/268 - Loss: 0.5526 - Accuracy: 0.7188
Batch 212/268 - Loss: 0.6045 - Accuracy: 0.6562
Batch 265/268 - Loss: 0.4824 - Accuracy: 0.7812
Epoch 15 - Train Loss: 0.5420 - Train Acc: 0.7066 - Val Loss: 0.5475 - Val Acc: 0.6931 - Time: 31.62s
Epoch 16/40
Batch 53/268 - Loss: 0.4372 - Accuracy: 0.7812
Batch 106/268 - Loss: 0.4564 - Accuracy: 0.7500
Batch 159/268 - Loss: 0.4737 - Accuracy: 0.7500
Batch 212/268 - Loss: 0.4462 - Accuracy: 0.8750
Batch 265/268 - Loss: 0.5869 - Accuracy: 0.6562
Epoch 16 - Train Loss: 0.5361 - Train Acc: 0.7070 - Val Loss: 0.5667 - Val Acc: 0.6931 - Time: 31.76s
Epoch 17/40
Batch 53/268 - Loss: 0.5381 - Accuracy: 0.7500
Batch 106/268 - Loss: 0.5020 - Accuracy: 0.7500
Batch 159/268 - Loss: 0.4327 - Accuracy: 0.7188
Batch 212/268 - Loss: 0.6072 - Accuracy: 0.6250
Batch 265/268 - Loss: 0.5860 - Accuracy: 0.6875
Epoch 17 - Train Loss: 0.5334 - Train Acc: 0.7068 - Val Loss: 0.5431 - Val Acc: 0.6931 - Time: 31.81s
Epoch 18/40
Batch 53/268 - Loss: 0.4960 - Accuracy: 0.7188
Batch 106/268 - Loss: 0.5201 - Accuracy: 0.7500
Batch 159/268 - Loss: 0.4067 - Accuracy: 0.8438
Batch 212/268 - Loss: 0.5839 - Accuracy: 0.7188
Batch 265/268 - Loss: 0.5905 - Accuracy: 0.6562
Epoch 18 - Train Loss: 0.5267 - Train Acc: 0.7128 - Val Loss: 0.5640 - Val Acc: 0.6931 - Time: 31.67s
Epoch 19/40
Batch 53/268 - Loss: 0.4400 - Accuracy: 0.7812
Batch 106/268 - Loss: 0.4941 - Accuracy: 0.7500
Batch 159/268 - Loss: 0.4144 - Accuracy: 0.8438
Batch 212/268 - Loss: 0.5296 - Accuracy: 0.7188
Batch 265/268 - Loss: 0.5521 - Accuracy: 0.6875
Epoch 19 - Train Loss: 0.5270 - Train Acc: 0.7105 - Val Loss: 0.5541 - Val Acc: 0.6931 - Time: 31.72s
Epoch 20/40
Batch 53/268 - Loss: 0.4669 - Accuracy: 0.8438
Batch 106/268 - Loss: 0.5013 - Accuracy: 0.6875
Batch 159/268 - Loss: 0.5760 - Accuracy: 0.6250
Batch 212/268 - Loss: 0.5611 - Accuracy: 0.6562
Batch 265/268 - Loss: 0.5800 - Accuracy: 0.6250
Epoch 20 - Train Loss: 0.5236 - Train Acc: 0.7142 - Val Loss: 0.5274 - Val Acc: 0.6931 - Time: 31.79s
Epoch 21/40
Batch 53/268 - Loss: 0.4415 - Accuracy: 0.7500
Batch 106/268 - Loss: 0.4852 - Accuracy: 0.6875
Batch 159/268 - Loss: 0.4938 - Accuracy: 0.7188
Batch 212/268 - Loss: 0.5285 - Accuracy: 0.7500
Batch 265/268 - Loss: 0.4003 - Accuracy: 0.8438
Epoch 21 - Train Loss: 0.5203 - Train Acc: 0.7126 - Val Loss: 0.5499 - Val Acc: 0.6941 - Time: 31.55s
Epoch 22/40
Batch 53/268 - Loss: 0.4985 - Accuracy: 0.6875
Batch 106/268 - Loss: 0.4616 - Accuracy: 0.7500
Batch 159/268 - Loss: 0.5555 - Accuracy: 0.6250
Batch 212/268 - Loss: 0.5356 - Accuracy: 0.7500
Batch 265/268 - Loss: 0.5837 - Accuracy: 0.6562
Epoch 22 - Train Loss: 0.5167 - Train Acc: 0.7162 - Val Loss: 0.5472 - Val Acc: 0.6950 - Time: 31.68s
Epoch 23/40
Batch 53/268 - Loss: 0.6011 - Accuracy: 0.6875
Batch 106/268 - Loss: 0.5250 - Accuracy: 0.6250
Batch 159/268 - Loss: 0.4795 - Accuracy: 0.7500
Batch 212/268 - Loss: 0.4554 - Accuracy: 0.7812
Batch 265/268 - Loss: 0.6384 - Accuracy: 0.5938
Epoch 23 - Train Loss: 0.5146 - Train Acc: 0.7186 - Val Loss: 0.5422 - Val Acc: 0.6963 - Time: 31.69s
Epoch 24/40
Batch 53/268 - Loss: 0.4722 - Accuracy: 0.6562
Batch 106/268 - Loss: 0.4677 - Accuracy: 0.7188
Batch 159/268 - Loss: 0.5796 - Accuracy: 0.5938
Batch 212/268 - Loss: 0.4987 - Accuracy: 0.6875
Batch 265/268 - Loss: 0.6930 - Accuracy: 0.5312
Epoch 24 - Train Loss: 0.5115 - Train Acc: 0.7141 - Val Loss: 0.5499 - Val Acc: 0.6998 - Time: 31.75s
Epoch 25/40
Batch 53/268 - Loss: 0.4984 - Accuracy: 0.7812
Batch 106/268 - Loss: 0.4168 - Accuracy: 0.8125
Batch 159/268 - Loss: 0.5259 - Accuracy: 0.7188
Batch 212/268 - Loss: 0.5572 - Accuracy: 0.6562
Batch 265/268 - Loss: 0.4509 - Accuracy: 0.8750
Epoch 25 - Train Loss: 0.5086 - Train Acc: 0.7228 - Val Loss: 0.5609 - Val Acc: 0.7026 - Time: 31.77s
Epoch 26/40
Batch 53/268 - Loss: 0.5290 - Accuracy: 0.7188
Batch 106/268 - Loss: 0.4740 - Accuracy: 0.7500
Batch 159/268 - Loss: 0.4764 - Accuracy: 0.7500
Batch 212/268 - Loss: 0.5553 - Accuracy: 0.6562
Batch 265/268 - Loss: 0.5234 - Accuracy: 0.6875
Epoch 26 - Train Loss: 0.5072 - Train Acc: 0.7211 - Val Loss: 0.5499 - Val Acc: 0.7076 - Time: 31.63s
Epoch 27/40
Batch 53/268 - Loss: 0.5343 - Accuracy: 0.6875
Batch 106/268 - Loss: 0.4680 - Accuracy: 0.8125
Batch 159/268 - Loss: 0.5603 - Accuracy: 0.6250
Batch 212/268 - Loss: 0.4858 - Accuracy: 0.7812
Batch 265/268 - Loss: 0.4888 - Accuracy: 0.7188
Epoch 27 - Train Loss: 0.5056 - Train Acc: 0.7223 - Val Loss: 0.5443 - Val Acc: 0.7114 - Time: 31.62s
Epoch 28/40
Batch 53/268 - Loss: 0.4772 - Accuracy: 0.8438
Batch 106/268 - Loss: 0.5246 - Accuracy: 0.6875
Batch 159/268 - Loss: 0.4306 - Accuracy: 0.8125
Batch 212/268 - Loss: 0.5917 - Accuracy: 0.6875
Batch 265/268 - Loss: 0.4556 - Accuracy: 0.7188
Epoch 28 - Train Loss: 0.5027 - Train Acc: 0.7282 - Val Loss: 0.5697 - Val Acc: 0.7152 - Time: 31.78s
Epoch 29/40
Batch 53/268 - Loss: 0.6184 - Accuracy: 0.5625
Batch 106/268 - Loss: 0.4242 - Accuracy: 0.7812
Batch 159/268 - Loss: 0.4746 - Accuracy: 0.7500
Batch 212/268 - Loss: 0.4678 - Accuracy: 0.7188
Batch 265/268 - Loss: 0.5971 - Accuracy: 0.6250
Epoch 29 - Train Loss: 0.5025 - Train Acc: 0.7268 - Val Loss: 0.5181 - Val Acc: 0.7323 - Time: 31.72s
Epoch 30/40
Batch 53/268 - Loss: 0.4721 - Accuracy: 0.7500
Batch 106/268 - Loss: 0.6090 - Accuracy: 0.5625
Batch 159/268 - Loss: 0.5147 - Accuracy: 0.7188
Batch 212/268 - Loss: 0.6087 - Accuracy: 0.5938
Batch 265/268 - Loss: 0.5513 - Accuracy: 0.6562
Epoch 30 - Train Loss: 0.5023 - Train Acc: 0.7330 - Val Loss: 0.5380 - Val Acc: 0.7294 - Time: 31.74s
Epoch 31/40
Batch 53/268 - Loss: 0.4693 - Accuracy: 0.7500
Batch 106/268 - Loss: 0.4283 - Accuracy: 0.7812
Batch 159/268 - Loss: 0.4377 - Accuracy: 0.8125
Batch 212/268 - Loss: 0.5764 - Accuracy: 0.6250
Batch 265/268 - Loss: 0.6036 - Accuracy: 0.5625
Epoch 31 - Train Loss: 0.5011 - Train Acc: 0.7356 - Val Loss: 0.5545 - Val Acc: 0.7275 - Time: 31.67s
Epoch 32/40
Batch 53/268 - Loss: 0.5680 - Accuracy: 0.6562
Batch 106/268 - Loss: 0.5070 - Accuracy: 0.7500
Batch 159/268 - Loss: 0.4889 - Accuracy: 0.7812
Batch 212/268 - Loss: 0.4074 - Accuracy: 0.8438
Batch 265/268 - Loss: 0.5398 - Accuracy: 0.6562
Epoch 32 - Train Loss: 0.4961 - Train Acc: 0.7352 - Val Loss: 0.5322 - Val Acc: 0.7332 - Time: 31.68s
Epoch 33/40
Batch 53/268 - Loss: 0.5657 - Accuracy: 0.6875
Batch 106/268 - Loss: 0.4441 - Accuracy: 0.7500
Batch 159/268 - Loss: 0.5571 - Accuracy: 0.5938
Batch 212/268 - Loss: 0.4754 - Accuracy: 0.6875
Batch 265/268 - Loss: 0.3461 - Accuracy: 0.8125
Epoch 33 - Train Loss: 0.4947 - Train Acc: 0.7408 - Val Loss: 0.5484 - Val Acc: 0.7398 - Time: 31.80s
Epoch 34/40
Batch 53/268 - Loss: 0.5397 - Accuracy: 0.6875
Batch 106/268 - Loss: 0.5086 - Accuracy: 0.6875
Batch 159/268 - Loss: 0.4656 - Accuracy: 0.8125
Batch 212/268 - Loss: 0.4877 - Accuracy: 0.7188
Batch 265/268 - Loss: 0.4827 - Accuracy: 0.7812
Epoch 34 - Train Loss: 0.4920 - Train Acc: 0.7358 - Val Loss: 0.5495 - Val Acc: 0.7417 - Time: 31.67s
Epoch 35/40
Batch 53/268 - Loss: 0.4619 - Accuracy: 0.7188
Batch 106/268 - Loss: 0.4829 - Accuracy: 0.6875
Batch 159/268 - Loss: 0.5429 - Accuracy: 0.6875
Batch 212/268 - Loss: 0.3173 - Accuracy: 0.8750
Batch 265/268 - Loss: 0.3634 - Accuracy: 0.8750
Epoch 35 - Train Loss: 0.4921 - Train Acc: 0.7414 - Val Loss: 0.5576 - Val Acc: 0.7408 - Time: 31.81s
Epoch 36/40
Batch 53/268 - Loss: 0.4693 - Accuracy: 0.7500
Batch 106/268 - Loss: 0.4094 - Accuracy: 0.8125
Batch 159/268 - Loss: 0.5440 - Accuracy: 0.6250
Batch 212/268 - Loss: 0.3976 - Accuracy: 0.8750
Batch 265/268 - Loss: 0.5294 - Accuracy: 0.6875
Epoch 36 - Train Loss: 0.4909 - Train Acc: 0.7426 - Val Loss: 0.5502 - Val Acc: 0.7512 - Time: 31.67s
Epoch 37/40
Batch 53/268 - Loss: 0.5555 - Accuracy: 0.6250
Batch 106/268 - Loss: 0.5119 - Accuracy: 0.7188
Batch 159/268 - Loss: 0.5951 - Accuracy: 0.6562
Batch 212/268 - Loss: 0.5078 - Accuracy: 0.6875
Batch 265/268 - Loss: 0.5700 - Accuracy: 0.6250
Epoch 37 - Train Loss: 0.4856 - Train Acc: 0.7494 - Val Loss: 0.5411 - Val Acc: 0.7673 - Time: 31.80s
Epoch 38/40
Batch 53/268 - Loss: 0.4891 - Accuracy: 0.8750
Batch 106/268 - Loss: 0.4524 - Accuracy: 0.8438
Batch 159/268 - Loss: 0.5398 - Accuracy: 0.7500
Batch 212/268 - Loss: 0.5173 - Accuracy: 0.7500
Batch 265/268 - Loss: 0.4123 - Accuracy: 0.8125
Epoch 38 - Train Loss: 0.4861 - Train Acc: 0.7487 - Val Loss: 0.5455 - Val Acc: 0.7626 - Time: 31.69s
Epoch 39/40
Batch 53/268 - Loss: 0.5521 - Accuracy: 0.7188
Batch 106/268 - Loss: 0.4591 - Accuracy: 0.8125
Batch 159/268 - Loss: 0.5259 - Accuracy: 0.7188
Batch 212/268 - Loss: 0.4169 - Accuracy: 0.8125
Batch 265/268 - Loss: 0.4719 - Accuracy: 0.7500
Epoch 39 - Train Loss: 0.4846 - Train Acc: 0.7506 - Val Loss: 0.5600 - Val Acc: 0.7626 - Time: 31.68s
Epoch 40/40
Batch 53/268 - Loss: 0.4346 - Accuracy: 0.8438
Batch 106/268 - Loss: 0.6373 - Accuracy: 0.5312
Batch 159/268 - Loss: 0.4204 - Accuracy: 0.8438
Batch 212/268 - Loss: 0.4512 - Accuracy: 0.7812
Batch 265/268 - Loss: 0.4165 - Accuracy: 0.8750
Epoch 40 - Train Loss: 0.4807 - Train Acc: 0.7593 - Val Loss: 0.5444 - Val Acc: 0.7730 - Time: 31.60s
Average Time per Epoch: 31.79s
Validation Loss: 0.5444, Validation Accuracy: 0.7730
